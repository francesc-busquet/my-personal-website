[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge\nI am a technophile and data enthusiast, passionate about harnessing the potential of data to analyze competitive landscapes and derive strategic market insights, which provide actionable guidance, ultimately empowering organizations to gain a competitive advantage. Equally important to me is the art of effectively communicating these insights, no matter how intricate they may be.\nI possess 7 years of academic research experience, during which I explored how to leverage state-of-the-art data-driven methods to gain a deeper understanding of consumers and markets. Throughout this period, I have also assumed leadership roles and actively contributed to several multifaceted industry consultancy endeavors, primarily in the finance and tech sectors, in which I helped organizations enhance their understanding of consumer purchasing behavior and the complex dynamics that shape their markets."
  },
  {
    "objectID": "index.html#fran",
    "href": "index.html#fran",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge"
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "",
    "text": "In the previous post of the Statistics Foundations series, we explored the inherent errors associated with working with samples instead of the entire population. These errors stem from the limitations of samples in capturing the full spectrum of population nuances.\nWe delved into quantifying this error, commonly known as the standard error, by assessing the variability of a statistic derived from different samples of the same size drawn from the same population. To further explore this concept, we focused on the mean. We used the standard error definition to derive a formula that directly computes the standard error of the mean, dividing the population’s standard deviation by the square root of the sample size. However, we recognized that applying such a formula in practical scenarios can be unfeasible, as it hinges on possessing information about the entire population—information that is typically unavailable, underscoring the very reason why we employ samples in the first place.\nNevertheless, we uncovered a practical workaround by assuming that the standard deviation of our sample serves as an estimator for the population’s standard deviation. This substitution effectively transformed the formula into one that employs the sample’s standard deviation divided by the square root of its size, allowing us to estimate the standard error.\nSo far, our primary emphasis has been on understanding and quantifying the sampling error. But let’s take it a step further and connect this concept to something more practical and tangible. Imagine if we could use this knowledge to pinpoint a range within which our sample statistics are likely to fall and, by extension, where the true population parameters may lie. That’s where the notion of confidence intervals comes into the spotlight, a topic we’ll explore in this blog, with a primary focus on the mean.\nOnce more, we’ll be employing the same dataset from our previous posts, encompassing data from 2,000 supermarket customers, including details about their age, annual income, and educational level. As in our prior discussions, we’ll operate under the assumption that this dataset comprehensively captures information about all our customers, effectively representing our entire population."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#distributions",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#distributions",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Distributions",
    "text": "Distributions\nIn our previous discussions, we’ve touched upon the concept of distribution without providing a formal definition. In common parlance, this term is commonly used and readily understood, as demonstrated by phrases like “concentrated urban population distribution” or “disparities in wealth distribution within a country.” In these examples, “distribution” simply denotes how individuals or assets are spread across a specific area.\nIn statistics, the concept of distribution remains conceptually consistent. A variable’s distribution illustrates how the different values of that variable spread out and which are more prevalent. Our exploration of distributions has primarily involved the examination of a variable’s histogram—a visual representation that conveys the frequency of values within predefined intervals, commonly known as “bins”. This approach provides a direct and intuitive means of discerning the spread of values and identifying those that occur more and less frequently. For instance, let’s revisit the annual income distribution of our “population”.\n\n\n\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\nFigure 1 offers a visual representation of our population’s annual income distribution. This graphic, for instance, reveals that a substantial portion of our customers falls within the income range of $80,000 to $150,000, specifically encompassing 1,427 individuals, equivalent to 71.35% of our customer base.\nNow, consider a scenario where we randomly select a customer and, before checking any of their information, we make an educated guess about their income. In this situation, it’s reasonable to infer that their income is highly likely to fall within the range of $80,000 to $150,000, as the majority of our customers are concentrated in this income range.\nConsequently, distributions provide us with a framework to describe variables using the language of probability. This is why, in statistics, we often refer to them as probability distributions. To illustrate, in the previous example, we could state that the probability of a randomly chosen customer having an income between $80,000 and $150,000 is 71.35%. The connection between a variable’s values and their associated probabilities can be mathematically expressed through a function, which directly relates the variable’s values to their respective probabilities.\n\n\n\n\n\n\nDiscrete and continuous distributions\n\n\n\n\n\nWhen discussing probability distributions, it is crucial to distinguish between two fundamental types: continuous and discrete distributions.\nDiscrete distributions are characterized by elements that can only assume a finite number of values within a defined range. Examples of such distributions include the number of children in a family or the count of customers in a shop on a given day. These variables take on specific, countable values.\nOn the other hand, continuous distributions consist of elements that can take any value within a specified interval. While our everyday thinking and calculations often involve finite numbers, consider scenarios where precise measurements are vital, such as in pharmaceutical drug development, where even minuscule differences in weight can have significant implications. In this context, the weight of substances is treated as a continuous variable.\nIn continuous distributions, owing to their infinite range of potential values, it is not possible to precisely calculate the probability associated with a specific value. Conversely, in the case of discrete distributions, where a finite and countable set of values exists, we can accurately determine the probability associated with each individual value. In practical terms, this means that for continuous variables, we can only compute the probability of a variable falling within a certain range. While for discrete variables, we can calculate the probability of it assuming a specific value or falling within a particular range.\n\n\n\nMany probability distributions are frequently encountered and have earned distinctive names due to their importance. One such example is the uniform distribution, where every potential value of a variable is equally likely to occur. Another prominent distribution is the normal distribution, recognizable by its bell-shaped curve.\nWhile there are undeniably several other frequent probability distributions, for the purpose of this discussion, we will concentrate on the normal distribution. This emphasis is justified by its pivotal role in facilitating the translation of sample mean and sampling error, quantified by the standard error, into intervals within which we can confidently predict the likely range of the population mean. These intervals are commonly known as confidence intervals.\n\nThe normal distribution\nThe normal distribution, also known as the Gaussian distribution, is one of the most valuable continuous distributions, primarily because many statistics are normally distributed in their sampling distribution (as we saw in the previous post for the case of the mean).\nThe normal distribution is easily recognizable by its classic bell-shaped curve, as depicted in Figure 2. This curve resembles a perfectly symmetrical hill with a clear peak at its center, which represents the distribution’s mean. As you move away from this peak, the curve gradually slopes downward and then gently turns outward. This smooth descent and outward turn reveal a pattern of how data spreads—the likelihood of observing values becomes lower as you move further away from the mean, making values closer to the mean more probable.\nAdditionally, the symmetry of the normal distribution means that the probabilities of finding values above and below the mean are identical. In simpler terms, it implies that the chances of observing values on one side of the peak are the same as on the other side.\n\n\n\n\n\n\n\n\nFigure 2: Exemplary shape of a normal distribution\n\n\n\n\n\n\nMean and standard deviation: shaping the normal distribution\nThe entire shape of a normal distribution can be effectively described using just two key parameters: the mean and the standard deviation.\n\n\n\n\n\n\nNotation\n\n\n\n\n\nGiven that the mean and the standard deviation effectively describe the entire shape of a normal distribution. As such, we typically employ the following notation to succinctly represent a normal distribution: \\(N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the variable’s mean and \\(\\sigma\\) is the variable’s standard de\nTherefore, we can denote that a random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as:\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\n\n\nThe mean, as mentioned earlier, represents the distribution’s center and the location of its peak. On the other hand, the standard deviation characterizes the curve’s shape. It indicates whether the curve is relatively flat or sharply peaked.\nFigure 3 offers an interactive visual representation showcasing how tweaking the mean and the standard deviation influences a normal distribution. When we adjust the mean, while keeping the standard deviation constant, we how the entire distribution shifts. An increase in the mean nudges it to the right, while a decrease causes it to veer to the left. Conversely, changing the standard deviation while maintaining the mean constant is like stretching or compressing the data. A smaller standard deviation suggests that most data points group closer to the mean, yielding a tall, slender curve. On the contrary, a larger standard deviation indicates that data points are more dispersed, resulting in a shorter, broader curve.\n\nviewof current_sd = Inputs.range(\n  [1, 5],\n  {value: 1, step: 1, label: \"Standard Deviation\"}\n)\n\nviewof current_mean = Inputs.range(\n  [0, 20],\n  {value: 0, step: 5, label: \"Mean\"}\n)\n\nfiltered = transpose(data).filter(function(normal_distribution) {\n  return current_sd === normal_distribution.sd_value &&\n    current_mean === normal_distribution.mean_value;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered,\n  Plot.binX(\n    {y: \"proportion\"},\n    {x: \"values\", \n     curve: \"natural\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n     interval : 0.75\n    }\n  )\n  \n    \n).plot({x: {domain: [-20, 40], grid: true}, y: {domain: [0, 0.3]}})\n\n\n\n\n\n\n\n\nFigure 3: Exemplary normal distributions with varying mean and standard deviation\n\n\n\n\n\n\nThe normal probability density function\nAs we’ve just observed and articulated, the core characteristics of a normal distribution revolve around its mean and standard deviation. Mathematically, the shape of a normal distribution can be portrayed through a functional relationship between the values of a normally distributed variable \\(X\\), characterized by a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\), and their probability density, known as the probability density function:\n\\(f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\)\nThe pivotal element within this formula is the exponential term \\(\\left({ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\right)\\). This term effectively communicates the rate at which the probability density diminishes as we distance ourselves from the mean (\\(\\mu\\))—as \\(x\\) moves farther from \\(\\mu\\), the lower the probability density. It’s essential to note that a larger standard deviation (\\(\\sigma\\)) results in a reduction of the magnitude of this expression, which, in turn, moderates the pace of the probability density decay.\n\n\n\n\n\n\nProbability densities\n\n\n\n\n\nIt is essential to differentiate between probability densities and probabilities, as these two concepts fundamentally diverge. As we have underscored previously, computing the exact probability of a continuous variable taking a specific value is unfeasible, given that continuous variables can theoretically encompass an infinite range of values.\nTo illustrate this, consider measuring an individual’s height, which may be reported as 175cm. However, if we possessed an incredibly precise measuring instrument, it might record the height as 174.9999945 cm. In practice, we typically round such measurements to a more practical form, like 175 cm, instead of expressing them as infinite decimals.\nWhen we talk about probability densities, we essentially employ a similar principle — grouping values near one another. This grouping enables us to represent the likelihood of a value falling near a specific point, such as 175cm, without claiming it is precisely 175cm. It’s important to note that probability densities, in isolation, lack a direct interpretation as probabilities. However, they are ingeniously constructed to ensure that the area beneath the density curve always maintains its interpretability as genuine probabilities.\n\n\n\n\n\nThe 68-95-99.7 rule\nSince the “probability” of specific values in a normal distribution is dictated by the mean and the standard deviation, we can directly associate the likelihood of specific events with these two parameters, particularly concerning how many standard deviations we deviate from the mean. This relationship gives rise to a widely recognized rule, commonly known as the 68-95-99.7 rule. According to this rule, there’s an approximate probability of 68% that a particular observation falls within one standard deviation from the mean (i.e., between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\)), roughly 95% within two standard deviations from the mean (i.e., between \\(\\mu - 2\\sigma\\) and \\(\\mu + 2\\sigma\\)), and approximately 99.7% within three standard deviations from the mean (i.e., between \\(\\mu - 3\\sigma\\) and \\(\\mu + 3\\sigma\\)). In this context, the values 1, 2, and 3, representing the number of standard deviations from the mean, are often referred to as critical values. These values help to define specific regions in the distribution.\nIn simpler terms, this rule tells us that for a normally distributed variable, roughly 68% of observations are within one standard deviation of the mean, about 95% are within two standard deviations, and approximately 99.7% are within three standard deviations. Figure 4 provides a visual representation of this rule.\n\n\n\n\n\n\n\n\nFigure 4: Visual representation of the 68-95-99.7 rule"
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Sample means and the normal distribution",
    "text": "Sample means and the normal distribution\nNow that we have a better understanding of probability distributions and how they help us assess where most data points are likely to cluster, as well as to assess the probability of an unknown data point falling within a specific range, let’s revisit our example involving supermarket customers. In the previous post, we uncovered an essential concept: sample means drawn from the same population and of the same size conform to a normal distribution with its center—the distribution’s mean—aligning with the population’s mean.\n\nVisualizing the concept\nTo provide a visual representation of this concept, Figure 5 displays a histogram that provides an overview of the means obtained from 10,000 samples, with each sample containing 40 randomly selected customers from our population. In essence, this histogram visualizes the mean sampling distribution for samples of 40 observations drawn from our population.\n\n\n\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nAs evident from the figure, these sample means display a characteristic bell-shaped distribution, i.e., following a normal distribution, with the distribution mean precisely aligning with the population mean, which, in this case, is $120,950. Additionally, the standard deviation of this distribution, which is equivalent to the standard error of the mean, is $5,862.\n\n\nApplying the 68-95-99.7 rule\nBy applying the principles of the 68-95-99.7 rule, as explained earlier, we can infer that approximately 68% of samples drawn from the population will fall within one standard deviation from the mean, about 95% within two standard deviations, and nearly 99.7% within three. Notably, the standard deviation of sample means, computed from various samples of the same size and from the same population, corresponds to their standard error, whereas the mean aligns with the population mean. Consequently, we can rephrase this as follows: about 68% of sample means will be within one standard error of the population’s mean, approximately 95% within two standard errors, and nearly 99.7% within three standard errors of the population’s mean.\n\n\nPractical example and intuition\nNow, let’s apply this understanding in practice. Consider a scenario where we know the standard error of the mean (SEM = $5,862), but the population mean remains uncertain. However, we want to have an idea of the value the population mean could take. To do so, we select a sample of 40 customers from our population and calculate the mean of their annual income, which turns out to be $134,320.\nThis process of taking a sample and computing its mean is akin to randomly selecting a value from the distribution we discussed earlier, the mean sampling distribution. Therefore, it is highly likely that the value we obtain from this sample will be found within three standard errors of the population’s mean, as approximately 99.7% of sample means would be found within this range. This potential difference that we could have from the population mean is commonly known as the margin of error.\nConsequently, we can reverse the previous statement, affirming that the population mean is very likely to fall within three standard errors of the current sample mean. Therefore, we can say that our population’s annual income mean will be found with a 99.7% confidence within $116,734 (\\(134,320-3\\times5,862\\)) and $151,906 (\\(134,320+3\\times5,862\\)). We talk about 99.7% confidence because there’s a small chance (0.03%) that the population mean could be farther away than 3 standard errors. That’s why we refer to these intervals as confidence intervals, as they give us with some level of confidence, an interval in which the population mean will be found.\n\n\nVisualizing the intuition behind confidence intervals\n?@fig-exemplary-sample-means-distributions visually illustrates the intuition behind our reasoning. We assume that the mean obtained from our sample could belong to any mean sampling distribution, which center, i.e., distribution mean and, in turn, population mean, is found within this ±3SEM area, highlighted with a slightly grayer shade. Even the mean we obtained could be the center of the distribution, i.e., the population mean.\n\n\nNULL\n\n\n\n\nDirectly measuring errors\nIn our previous example, we gained an understanding of confidence intervals by examining the distribution of sample means. However, we can take this a step further by directly translating the distribution of sample means into an error distribution.\nError, in this context, refers to the difference from the population’s mean. To calculate it, we simply subtract the population mean from each individual sample. Non-zero values indicate a disparity between the sample mean and the population mean, with the magnitude of the value signifying the extent of this difference.\nWhen we subtract a consistent value from every observation of a variable, we effectively shift the variable’s mean by the same amount. Given that the mean of the sampling distribution aligns with the population mean, subtracting this value from the mean calculated for each sample effectively centers the distribution around 0.\nMoreover, as we’ve discussed before, altering the mean of a normal distribution corresponds to shifting the distribution horizontally, repositioning its central point where it is symmetrical. Therefore, this subtraction effectively repositions the distribution’s center to zero.\nLet’s revisit the previous scenario where we calculated the mean annual income from 10,000 different samples, each containing 40 customers drawn from our population. We visualized the distribution of the computed means in Figure 5. We now proceed to subtract the population mean from every computed individual sample mean, obtaining the errors incurred. We then plot the error distribution using a histogram, as depicted in Figure 6. As shown in the figure, it retains the same shape as Figure 5, yet it is now centered at zero, i.e. the point that signifies the absence of error.\n\n\n\n\n\n\n\n\nFigure 6: Error distribution of the means for 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nSo, rather than focusing on the distribution of various sample means, we are now examining how errors, the differences between sample means and population means, are distributed. Notably, this error is expressed in the same units as our variable, i.e., in thousands of dollars.\nIdeally, we aim to obtain errors that remain invariant across different measurement scales, enabling us to compare error distributions across different variables, even when they use distinct units of measurement. Achieving this requires us to utilize a common measurement unit, with the standard deviation commonly being the preferred metric for this purpose.\nConsequently, we proceed by dividing each value of our variable by its standard deviation, effectively transforming measurement units into standard deviation units. This process of dividing each observation by the standard deviation is essentially equivalent to dividing the standard deviation by itself, resulting in a standard deviation of 1.\nKeep in mind that the standard deviation of the sampling distribution of the mean is identical to the standard error of the mean. Therefore, our process involves dividing the diverse errors by the standard error, resulting in a error measured in standard errors of the mean distribution with a mean of 0 and a standard deviation of 1, as depicted in Figure 7. As we can see, the distribution is still normal, we only shifted its mean to 0 and converted its standard deviation to 1.\n\n\n\n\n\n\n\n\nFigure 7: Sampling error of the mean measured in standard errors distribution for 10,000 samples of 40 customers\n\n\n\n\n\nIn summary, our process involved subtracting the mean of the sampling distribution of the mean, i.e., the population mean from each computed sample mean, essentially transforming the sample means into errors—difference between the computed mean and the population mean. Following this, we divided these errors by the standard deviation of the sampling distribution of the mean, i.e. the standard error of the mean, thereby representing them in a consistent unit of measurement. The outcome is a distribution with a mean of 0 and a standard deviation of 1, allowing us to compare these standardized errors across various variables effectively. Hence the whole process can be represented through the following formula:\n\\[\n\\frac{\\bar{x}-\\mu}{SE} = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nThis is what we call the standardized version of the sample mean."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Dealing with uncertainty: When standard error information is lacking",
    "text": "Dealing with uncertainty: When standard error information is lacking\nOur idealized assumption of knowing the exact standard error of the mean is often impractical in real-world scenarios. As we discussed in the previous post, calculating the standard error usually necessitates either drawing numerous samples of the same size from the same population, calculating the mean for each sample, and then determining the standard deviation of those sample means, or dividing the population’s standard deviation by the square root of our sample size. Unfortunately, both of these methods are frequently unfeasible. To overcome this challenge, we resort to estimating the standard error by dividing the sample’s standard deviation by the square root of the sample size, thereby introducing an additional layer of uncertainty into our calculations.This can be easily visualized by replacing the population’s standard deviation to the sample’s standard deviation in the previous formula:\n\\[  \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}} \\]\nAs previously shown, when we have exact knowledge of the standard error, the distribution of the standardized version of the sample mean follows a normal distribution. However, when we lack precise knowledge of the standard error and instead use an estimate, does the distribution of the standardized version of the sample mean still follow a normal distribution? To verify this, we proceed to visualize the distribution of the standardized version of the sample mean when we don’t know the standard error precisely and estimate it by dividing the sample’s standard deviation by the square root of the sample size. Given that the standard error depends on the sample size, we repeat this process for various sample sizes. For each size, we extract 100,000 samples and compute the standardized version of the sample mean with the estimated standard error. These resulting distributions are illustrated in ?@fig-different-sizes-samples-error-div-sd-distr.\n\n\nNULL\n\n\nAs evident in ?@fig-different-sizes-samples-error-div-sd-distr, these errors exhibit a distribution that closely resembles the normal distribution, particularly when dealing with larger sample sizes. However, for smaller sample sizes, the distribution exhibits broader tails compared to the typical normal distribution. In reality, this altered distribution is known as the Student’s t-distribution, commonly referred to as the t-distribution for simplicity.\n\nEmbracing the t-distribution\nThe t-distribution is not a single function but rather a family of functions. Similar to the normal curve, each t-distribution is symmetric, with its mean positioned at the center. However, unlike the normal distribution, in the case of the t-distribution, the mean is always fixed at 0. This means that the t-distribution is centered around 0 by default, and its shape and spread are determined by a parameter known as the degrees of freedom. The concept of degrees of freedom, in a sense, varies from application to application, but in this domain we can understand it as the number of independent pieces of information to calculate a statistic, i.e. the mean for us. For the mean the degrees of freedom are equivalent to the number of observations minus one (\\(n-1\\)).\nThe intuition behind the appearance of t-distribution when using the estimated standard error arises from the added uncertainty because of the use of this estimation. It provides a more appropriate and conservative model for the variability of sample means when the standard error is unknown. Unlike the normal distribution, the t-distribution takes sample size into account, presenting wider tails that aptly accommodate the increased uncertainty and variability associated with estimating the standard error from a sample.\nFigure 8 illustrates how the t-distribution transforms with varying degrees of freedom.As degrees of freedom increase, the distribution’s tails gradually become narrower, nearing a state that closely resembles the normal distribution. This transformation is a consequence of larger sample sizes, which enable a more refined depiction of the underlying population. Consequently, it leads to reduced sampling errors and, consequently, enhances the precision of our estimations of the standard error, reducing uncertainty.\n\nviewof current_df = Inputs.range(\n  [1, 50],\n  {value: 1, step: 1, label: \"Degrees of freedom\"}\n)\n\n\n\nfiltered2 = transpose(data2).filter(function(df_distribution) {\n  return current_df === df_distribution.df;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered2,\n  { x: \"x\",\n    y: \"values\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n    }\n  ).plot({x: {domain: [-6, 6], grid: true}, y: {domain: [0, 0.45]}})\n\n\n\n\n\n\n\n\nFigure 8: Exemplary t distributions with varying degrees of freedom\n\n\n\n\nThe t-distribution, in contrast to the normal distribution, exhibits broader tails, making it unsuitable for applying the 68-95-99.7 rule we discussed earlier. With these wider tails, our expectations change: we can no longer anticipate approximately 68% of potential mean values falling within 1 standard error of the mean, 95% within 2, and 99.7% within 3; these proportions are now reduced.\nAs the characteristics of the t-distribution are contingent upon degrees of freedom, we must consider them when determining the number of standard errors from the mean required to encompass a specific proportion of values. These precise proportions will vary as degrees of freedom change. Nevertheless, as degrees of freedom increase, the distribution of standardized sample means approximates a normal distribution, allowing us to eventually employ the 68-95-99.7 rule.\nTable 1 provides the critical values for t-distributions across a range of degrees of freedom and confidence levels. Notably, as the sample size becomes sufficiently large, the critical values for the t-distribution closely mirror those of a (standardized) normal distribution. In fact, when dealing with an infinite number of degrees of freedom, the critical values for the t-distribution converge to those of a normal distribution. Consequently, for sufficiently large sample sizes, it’s entirely justified to work directly with a normal distribution due to its close approximation to the t-distribution in such cases.\n\n\nDisplay Table 1\n\n\n\n\nTable 1: T-distribution critical values\n\n\n\n\n\n\n\n\n\n\n\nDegrees of Freedom (df)\n68% Critical Value\n95% Critical Value\n99.7% Critical Value\n\n\n\n\n1\n1.819\n12.706\n212.205\n\n\n2\n1.312\n4.303\n18.216\n\n\n3\n1.189\n3.182\n8.891\n\n\n4\n1.134\n2.776\n6.435\n\n\n5\n1.104\n2.571\n5.376\n\n\n6\n1.084\n2.447\n4.800\n\n\n7\n1.070\n2.365\n4.442\n\n\n8\n1.060\n2.306\n4.199\n\n\n9\n1.053\n2.262\n4.024\n\n\n10\n1.046\n2.228\n3.892\n\n\n20\n1.020\n2.086\n3.376\n\n\n30\n1.011\n2.042\n3.230\n\n\n40\n1.007\n2.021\n3.160\n\n\n50\n1.004\n2.009\n3.120\n\n\n60\n1.003\n2.000\n3.094\n\n\n70\n1.002\n1.994\n3.075\n\n\n80\n1.001\n1.990\n3.061\n\n\n90\n1.000\n1.987\n3.051\n\n\n100\n0.999\n1.984\n3.042\n\n\n150\n0.998\n1.976\n3.017\n\n\nInfinity\n0.994\n1.960\n2.968\n\n\n\n\n\n\n\n\n\nEstimating confidence intervals for our exemplary sample of 40 Customers\nHaving established that estimating, rather than precisely knowing the standard error, introduces an additional layer of uncertainty, we must adapt our approach when calculating margins of error and confidence intervals. Instead of relying on the normal distribution, we turn to the t-distribution, a distribution whose shape varies with the sample size. For smaller samples, it features wider tails, transitioning towards a normal distribution as the sample size increases. These wider tails account for the added uncertainty, resulting in more conservative estimates.\nReturning to our example, where we examined a sample of 40 customers with a mean of $134,320 the first step is to estimate the standard deviation for this sample, which amounts to $70,439. Therefore, we estimate the standard error by dividing this standard deviation by the square root of 40, resulting in an estimated standard error of $11,137.38.\nFor a desired confidence level of 99.7%, we should use a critical value of 3.166. This value corresponds to the critical value for a confidence level of 99.7% and 39 degrees of freedom (40-1). Moreover, note that this critical value is larger than the value of 3, which we used when we knew the standard error and, thus, had a sampling distribution of the mean that followed a normal distribution. Consequently, we can calculate the margin of error by multiplying the critical value for the chosen confidence level by the estimated standard error, yielding a margin of error of $35,260.95 (\\(3.166 \\times \\$11,137.38\\) = \\(\\$35,260.95\\)). With 99.7% confidence, the population mean is estimated to fall between (\\(\\$134,320\\pm\\$35,260.95\\)) $99,059.05 and $169,581.\nThese confidence intervals, which are wider than the previous ones obtained when we knew the exact standard error, provide a range of $99,059.05 to $169,581, as opposed to the narrower intervals of $116,734 to $151,906. This increase in width reflects the additional caution necessitated by the t-distribution’s wider tails and the inherent uncertainty associated with estimating the standard error.\n\n\n\n\n\n\n95% confidence as a standard\n\n\n\n\n\nIn the examples provided thus far, we’ve been working with a relatively high confidence level of 99.7%. While such a level of confidence is valuable in certain contexts, it’s worth highlighting that in everyday practical applications, a 95% confidence level is the more prevalent choice.\nA 95% confidence level offers a balanced compromise between precision and practicality. This means that we are willing to tolerate a 5% chance of not capturing the population mean within the defined intervals, in return for the benefits of having narrower confidence intervals."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#a-final-note",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#a-final-note",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "A final note",
    "text": "A final note\nIn the preceding sections, we saw that we can define confidence intervals by leveraging the shape of the sampling distribution. This sampling distribution can be converted into an error distribution measured in standard errors (or estimated standard errors). And from such distribution, we can find critical values that define the points within which the majority of errors will be found, in a way that we can calculate confidence intervals. Thus, given that such a distribution for other statistics follows a symmetrical as previously seen, we can extract some simple formula for the computation of the confidence interval of such statistics.\nA confidence interval consists of two limits, defining the lower and upper bounds of the interval, where the point estimate lies at the center. The confidence interval (CI) for an estimator of a parameter \\(\\theta\\) can be expressed in the following way:\n\\[CI_{\\theta} = \\hat{\\theta}\\pm MOE\\]\nmeaning that the upper confidence interval for a parameter \\(\\theta\\) is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)), i.e., the value obtained in the sample for that parameter, plus the margin of error (MOE). While the lower confidence interval is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)) minus the margin of error.\nThe margin of error corresponds to half the width of the interval and is given by:\n\\[MOE = \\Phi^{-1}_{1-\\frac{\\alpha}{2}} \\times \\hat{\\sigma}_{\\theta} \\]\nHere, \\(\\hat{\\sigma}_{\\theta}\\) represents the estimated standard error of \\(\\theta\\) and \\(\\Phi^{-1}_{1-\\frac{\\alpha}{2}}\\) denotes the critical value. More specifically it refers to the inverse quantile function, which includes a confidence level equal to \\(1-\\alpha\\), i.e., a function that tells us the point in which that proportion of the distribution is found.\nWhile the exact sampling distribution for some statistics can be unknown, as well as their standard error, the central limit theorem often provides a justification for using a normal approximation. For this reason, for most statistics, we tend to assume that their sampling distribution follows a normal distribution (given that the sample is large enough. An example is the t-distribution which for large enough samples approximates the normal distribution). This approximation tends to be accurate, but even in cases in which it’s not that accurate, it’s better to have an approximate confidence interval than a solely-point estimate."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#summary",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#summary",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Summary",
    "text": "Summary\n\nSampling error arises from using samples rather than the entire population for analysis, as samples may not capture all population nuances.\nThe sampling error can be quantified using the standard error, which measures the variability of a statistic calculated from different samples.\nThe standard error helps define a range within which sample statistics are likely to fall, providing insights into potential population parameters.\nThe sampling distribution of the mean follows a normal distribution, with its mean equal to the population mean and standard deviation equal to the standard error.\nThe 68-95-99.7 rule provides a shorthand for understanding the distribution: approximately 68% of values are within 1 standard deviation from the mean, 95% within two standard deviations, and 99.7% within three standard deviations. These values defining specific regions are known as critical values.\nTranslating this to standard errors, 68% of values are about 1 standard error from the population mean, 95% about two standard errors, and 99.7% about three standard errors.\nA sample, with 95% confidence, is expected to be about 2 standard errors from the mean, and with 99.7% confidence, about 3 standard errors.\nThis argument can be reversed: with 95% confidence, a sample is about 2 standard errors from the population mean, and with 99.7% confidence, about 3 standard errors.\nThe margin of error, which creates a likely population range (confidence interval), is determined by multiplying the critical value by the standard error.\nThe confidence interval is obtained by adding the margin of error to the sample mean (upper interval) and subtracting it from the sample mean (lower interval).\nIn most cases, the standard error is unknown and needs to be estimated by dividing the sample’s standard deviation by the square root of its size.\nWhen estimating the standard error, the assumption that the sampling distribution of the mean follows a normal distribution no longer holds; it follows a Student t-distribution.\nThe Student t-distribution (t-distribution) varies based on degrees of freedom, resembling the normal distribution but having wider tails with smaller sample sizes (degrees of freedom).\nThe wider tails of the t-distribution result in larger magnitude critical values than the normal distribution, leading to wider confidence intervals.\n\n\n\nCode\n# Customer annual income distribution (thousands, $) ---------------\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\nstd_deviation_income &lt;- sd(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n    scale_x_continuous(breaks = round(seq(30, 320, by = 20))) +\n   annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, label = paste0(\"Mean: \", round(average_income, 2), \"\\nSD: \", round(std_deviation_income, 2))) \n\n# Create exemplary normality plot ---------------\n\nset.seed(150)\n\n#Create a normal distribution\nnormal_distribution &lt;- rnorm(30000)\nnormal_distribution &lt;- data.frame(values = normal_distribution)\n\n\n\n\nggplot(normal_distribution, aes(x = values)) +\n  geom_density(fill = \"steelblue\", color = \"steelblue\", alpha = 0.4, adjust = 2) + \n  labs(\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) \n    \n# Visual representation of the 68-95-99.7 rule ---------------\n\nmu = 0\nsigma = 1\nx &lt;- seq(-5*sigma, 5*sigma, length.out = 1000)\ny &lt;- dnorm(x, mean = mu, sd = sigma)\ndata &lt;- data.frame(x, y)\n\nggplot(data.frame(data), aes(x)) + \n    geom_ribbon(data = subset(data, x &gt;= mu - 3 * sigma & x &lt;= mu + 3 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#c1f5ef\") +\n    geom_ribbon(data = subset(data, x &gt;= mu - 2 * sigma & x &lt;= mu + 2 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#90ebe1\") +\n    geom_ribbon(data = subset(data, x &gt;= mu - 1 * sigma & x &lt;= mu + 1 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#34d1bf\") +\n  geom_ribbon(data = subset(data, x &gt;= -0.01 & x &lt;= 0.01),\n                aes(ymax = y), ymin = 0, fill = \"black\") +\n    theme_minimal() +\n    geom_vline(xintercept = c(mu - 3 * sigma, mu - 2 * sigma, mu - 1 * sigma, mu + 1 * sigma, mu + 2 * sigma, mu + 3 * sigma), \n               linetype = \"dashed\", color = \"black\", alpha = 0.5) +\n    geom_segment(x = 0 - 1 * sigma +0.05, xend = 0 + 1 * sigma -0.05, y = 0.45, yend = 0.45, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n  annotate(\"text\", x = 0, y = 0.46, label = \"68%\") +\n  geom_segment(x = 0 - 2 * sigma +0.05, xend = 0 + 2 * sigma -0.05, y = 0.5, yend = 0.5, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n   annotate(\"text\", x = 0, y = 0.51, label = \"95%\") +\n  geom_segment(x = 0 - 3 * sigma +0.05, xend = 0 + 3 * sigma -0.05, y = 0.55, yend = 0.55, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n   annotate(\"text\", x = 0, y = 0.56, label = \"99.7%\") +\n  ylim(c(0, 0.6)) +\n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), labels = c(expression(mu ~ \"- 3\" ~ sigma), expression(mu ~ \"- 2\" ~ sigma), expression(mu ~ \"-\" ~ sigma), expression(mu), expression(mu ~ \"+\" ~ sigma), expression(mu ~ \"+ 2\" ~ sigma),  expression(mu ~ \"+ 3\" ~ sigma))) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n        axis.title.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(),\n        plot.caption = element_text(hjust = 0)) \n        \n# Average income distribution of 10,000 samples of 40 customers (thousands, $) ---------------\n\nset.seed(0)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 10, label = paste0(\"Mean: \", round(average_income, 2))) \n\n# Exemplary possible sample means distributions ---------------\n\nlibrary(gganimate)\nsample_mean &lt;- 134.320\nstandard_error &lt;- sd(sample40_means)\n\nsample_means_modified &lt;- data.frame(Income = NA, possible_mean_sample = NA)\npossible_mean_sample &lt;- seq(sample_mean - 3 * standard_error, sample_mean + 3 * standard_error, length.out = 7)\npossible_mean_sample[2] &lt;- 120.950\n\nfor (possible_mean in possible_mean_sample) {\n  new_sample_mean &lt;- data.frame(Income = sample_means$Income - mean(sample_means$Income) + possible_mean)\n  new_sample_mean$possible_mean_sample &lt;- possible_mean\n  sample_means_modified &lt;- rbind(sample_means_modified, new_sample_mean)\n}\n\nsample_means_modified &lt;- sample_means_modified[-1,]\nsample_means_modified$possible_mean_sample &lt;- round(sample_means_modified$possible_mean_sample, 2)\nsample_means_modified$title &lt;- paste0(\"Assuming samples extracted from a population \\nwith M = \",sample_means_modified$possible_mean_sample, \" (SEM = \", round(standard_error,2), \")\")\nsample_means_modified$title &lt;- as.factor(sample_means_modified$title)\n\npossible_mean_sample &lt;- data.frame(Mean_value = possible_mean_sample, title = unique(sample_means_modified$title))\n\n\np &lt;- ggplot(sample_means_modified, aes(x = Income)) +\n  annotate(\"rect\", xmin = sample_mean - 3 * standard_error, xmax = sample_mean + 3 * standard_error,  ymin = 0, ymax = 0.075, alpha = .1) +\n  annotate(\"text\", x = sample_mean - 3 * standard_error * 1.05, y = 0.025, label = \"Sample Mean - 3SE\", alpha = 0.3, angle = 90)  +\n  annotate(\"text\", x = sample_mean + 3 * standard_error * 1.05, y = 0.025, label = \"Sample Mean + 3SE\", alpha = 0.3, angle = -90)  +\n  annotate(\"text\", x = sample_mean, y = 0.077, label = \"Sample Mean \", alpha = 0.3)  +\n  geom_vline(xintercept = sample_mean, size = 0.4, alpha = 0.4, linetype = \"dashed\") +\n  geom_density(fill = \"steelblue\", alpha = 0.4) +\n  geom_vline(data = possible_mean_sample, aes(xintercept = Mean_value), color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = possible_mean_sample, aes(x = Mean_value * 1.08, y = 0.05, label = paste0(\"Mean: \", round(Mean_value, 2)))) + \n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Probability Density\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0, size = 12),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    transition_states(title, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n# Error Distribution of the Means for 10,000 Samples of 40 Customers (thousands, $) ---------------\n\nsample_means_error &lt;- sample_means\nsample_means_error$error &lt;- sample_means$Income - mean(sample_means$Income)\n\nsd_sample_means_error &lt;- sd(sample_means_error$error)\n\n\nggplot(sample_means_error, aes(x = error)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Error (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = 3.2, y = 11, label = paste0(\"Mean: 0\\nSD: \", round(sd_sample_means_error, 2))) \n   \n# Sampling error of the mean measured in standard errors distribution for 10,000 Samples of 40 Customers ---------------\n\nsample_means_error$error_div_sd &lt;- sample_means_error$error / sd(sample_means$Income)\n\nsd_sample_means_error &lt;- sd(sample_means_error$error_div_sd)\n\n\n\nggplot(sample_means_error, aes(x = error_div_sd)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Error measured in standard errors\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = 1, y = 11, label = paste0(\"Mean: 0\\nSD: \", round(sd_sample_means_error, 2))) \n\n# Sampling error of the mean measured in estimated standard errors distribution for 100,000 Samples of varying number of Customers ---------------\n\nset.seed(34)\nsample_sizes &lt;- c(seq(2, 10, by = 1), seq(20, 60, by = 20))\nsamples_different_sizes &lt;- data.frame(values = NA, sizes = NA, sd = NA)\nfor(sample_size in sample_sizes){\n  \n  sample_means_diff_size &lt;- numeric(length = 100000)\n\n  for(i in seq(1, 100000)){\n     sample_means_diff_size[i] &lt;- mean(sample(customer_data$Income, sample_size))\n     \n  }\n  samples_different_sizes2 &lt;- data.frame(values = sample_means_diff_size, sizes = sample_size, sd = sd(sample_means_diff_size))\n  samples_different_sizes &lt;- rbind(samples_different_sizes, samples_different_sizes2)\n \n}\n\nsamples_different_sizes &lt;- samples_different_sizes[-1,]\n\n\nsamples_different_sizes$error_divided_by_sd &lt;- (samples_different_sizes$values - mean(customer_data$Income)) /(samples_different_sizes2$sd/sqrt(samples_different_sizes2$sizes))\n\n\nsamples_different_sizes$sizes &lt;- as.factor(samples_different_sizes$sizes)\nlevels(samples_different_sizes$sizes) &lt;- paste(\"Number of observations per sample:\", levels(samples_different_sizes$sizes))\ndata_error_div_sd_average &lt;- samples_different_sizes %&gt;%\n  summarise(mean_error_div_sd = mean(error_divided_by_sd), .by = sizes)\n\np &lt;- ggplot(samples_different_sizes, aes(x = error_divided_by_sd, fill = sizes)) +\n  geom_density() +\n  labs(\n    x = \"Error measured in estimated standard errors\",\n    y = \"Probability Density\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_manual(values = c(\n  \"#006699\", \"#FF9E00\", \"#B5113E\", \"#3B125F\", \"#007F7B\", \"#1A4C3C\",\n  \"#7C0A02\", \"#00567F\", \"#7500A4\", \"#B760DE\", \"#002E3E\", \"#7F7F7F\"\n)) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(sizes) +\n  enter_fade() +\n  view_zoom_manual(xmin = c(-120, -100,  -90,  -75,  -70,  -70,  -60, -60, -60,  -45,  -30,  -30), xmax = -1 * c(-120, -100,  -90,  -75,  -70,  -70,  -60, -60, -60,  -45,  -30,  -30), ymin = rep(0, 12), ymax = c(rep(0.02, 5), rep(0.025, 3), 0.03,  0.035, 0.05, 0.06)) +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 25)\nanim"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\n\n\n\nFigure 1: S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\n\n\n\nFigure 1: S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A time of low interest rates",
    "text": "A time of low interest rates\nMoreover, these economic disturbances led governments to act severely. Keynesian policies for the activation of the economy began to play a fundamental role. To encourage consumption, interest rates were lowered to near historic lows. Figure 2 displays the US 10 year note bond yield between 1920 and 2020. In this figure, we can see how the dotcom crash pushed down this yield. However, after that it started recovering until July 2007, when the financial crisis started, after that, that yield continued a downward trend.\n\n\n\n\n\n\nFigure 2: US 10 year note bond yield"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Stocks as the only attractive investment vehicle",
    "text": "Stocks as the only attractive investment vehicle\nSuch a long period of low interest rates inflated stock prices. Figure 3, shows how cyclically adjusted S&P 500 price-to-earnings ratio has been rising during the last decade, being quite high in comparison to other periods of time (specially before the dot-com bubble). Stock prices started trading at a premium, because there were no attractive alternative investments, this channelled much of the liquidity into equities (bonds with almost no interest or even negative interests were not an attractive investment anymore). Moreover, this low interest rate setting has prompted greater investor leverage, due to its low cost. Hence, low interest rates justify high stock prices, since stocks are highly attractive relative to bonds and debt is stimulated due to its reduced cost.\n\n\n\n\n\n\nFigure 3: Cyclically adjusted S&P 500 price-to-earnings rations"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "An unexpected event: A global pandemic",
    "text": "An unexpected event: A global pandemic\n2020 was not a good year, this year will always be remembered as the year of the COVID. COVID brought many changes in our lives, which undoubtedly had an impact on the economy. As a result, governments continued to pursue stimulative policies and interest rates remained at very low levels.\nOne of the best illustrations of those stimulative policies is the amount of dollars printed in 2020: 21% of the United States dollar was printed in 2020, as shown in Figure 4. This large injection was used for both direct and indirect assistance in the COVID situation. This, at the same time, indirectly channelled part of this aid to the financial markets, increasing their value.\n\n\n\n\n\n\nFigure 4: Annual money stock growth (trillions of USD)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A double-edged sword",
    "text": "A double-edged sword\nInjecting money to the economy is always something controversial. Through the law of supply and demand it is easy to infer that a considerable increase in supply (without a similar increase in demand) will reduce the price of a good. In money, when this happens, we say that the money loses value, i.e. one monetary unit can acquire fewer products. In other words, this is what we call inflation. Even Warren Buffet has shown concern for inflation in the past month:\n\n“We are seeing very substantial inflation […] We are raising prices. People are raising prices to us and it’s being accepted.”\n\nThis raise on prices can be already tracked on several indices such as Bloomberg’s agriculture index, shown in Figure 5 and also in the annual single-family home price, as shown in Figure 6.\n\n\n\n\n\n\nFigure 5: Bloomberg agriculture index\n\n\n\n\n\n\n\n\n\nFigure 6: Annual single-family home price\n\n\n\nThe appearance of inflation means that interest rates should rise. Something that was already pointed by Janet Yellen at the start of this month:\n\n“It may be that interest rates will have to rise somewhat to make sure that our economy doesn’t overheat”\n\nBut what would happen if inflation persists and interest rates have to be risen? Remember that I previously said that stocks are trading at a premium due to the lack of attractive alternative investments. This would no longer be true and this premium would no longer be a thing. In addition, an increase in interest rates would reduce the attractiveness of leverage, thereby encouraging investor’s deleverage. And, thus, we should expect a decrease in the stock price."
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "The market can not be timed",
    "text": "The market can not be timed\nEven with all these indicators, it is difficult to say whether this will happen in the short to medium term. There are many factors which could deter inflation away and interest rates low. As an example, Berkshire Hathaway has been stacking cash during the last years, as shown in Figure 7, playing a slightly more defensive position. This may be due the fact that they already saw that low interest rates during the last decade were driving the stock market at high prices. However, interest rates are still low and during that time the stock market has continued rising.\n\n\n\n\n\n\nFigure 7: Berkshire’s cash holdings"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Summary",
    "text": "Summary\n\nThe first decade of the 2000s was characterized by two economic crisis, which defined an economy with very low interest rates\nLow interest rates increased stock market prices, since there were no attractive alternative investments and leverage was cheap. Thus, stock markets traded at a premium.\nThe Covid crisis led to a massive injection of money into the economy.\nThis money injection is a double-edged sword which may have brought an exuberance illusion awakening a ghost that has been dormant in recent years, inflation.\nThe emergence of inflation would imply an increase in interest rates.\nAn increase in interest rates would mean that the premium paid for stocks would be lost.\nDespite all these facts, timing the market is no easy task. And thus, other factors could keep inflation away and interest rates away, prolonging this situation."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html",
    "href": "posts/2023/intro-to-voice-analytics/index.html",
    "title": "Introductory voice analytics with R",
    "section": "",
    "text": "Interpersonal communication transcends mere words, incorporating nuanced nonverbal signals where the voice plays a pivotal role. We dynamically adjust our voice to convey emotions, such as happiness or sadness, and intentions, including subtle nuances like sarcasm. We even form impressions from the way someone speaks. Therefore, analyzing not just the content but also the delivery—the voice—is essential for a more comprehensive understanding of communication."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#voice-analytics-decoding-the-vocal-spectrum",
    "href": "posts/2023/intro-to-voice-analytics/index.html#voice-analytics-decoding-the-vocal-spectrum",
    "title": "Introductory voice analytics with R",
    "section": "Voice analytics: Decoding the vocal spectrum",
    "text": "Voice analytics: Decoding the vocal spectrum\nVoice analytics precisely aims to achieve this by examining the voice beyond its linguistic content. Various methods exist for conducting voice analytics, with one of the most common involving the extraction of different characteristics from the voice, known as vocal features. These features include amplitude, correlated with the loudness of a sound, and fundamental frequency, associated with pitch—that is, how high or low we perceive a voice to be. For instance, amplitude provides insights into the volume or intensity of speech, while fundamental frequency helps discern the pitch variations in a speaker’s voice."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#the-analytical-process-from-acquisition-to-statistical-analysis",
    "href": "posts/2023/intro-to-voice-analytics/index.html#the-analytical-process-from-acquisition-to-statistical-analysis",
    "title": "Introductory voice analytics with R",
    "section": "The analytical process: From acquisition to statistical analysis",
    "text": "The analytical process: From acquisition to statistical analysis\nHowever, before delving into the extraction of vocal features, a series of pivotal steps forms an integral part of the analytical process. The initial phase entails the acquisition of voice recordings, achievable through direct recording or retrieval from publicly accessible sources. Once the files are obtained, meticulous processing becomes indispensable, involving the arrangement of metadata and validation of collected files to ensure precision and organizational coherence.\nFollowing this preparatory phase, the subsequent step involves reading and preprocessing the voice files. This encompasses data preprocessing and transformation by primarily eliminating extraneous elements, such as irrelevant utterances and background noise. These preprocessing steps are crucial for ensuring the quality of the data.\nAfter preprocessing the audio files, we can extract the vocal features of interest, such as amplitude and fundamental frequency. These features can subsequently be explored through visualization and the computation of summary statistics to gain a deeper understanding. This exploration may reveal further anomalies, or matters requiring additional processing may be detected. Consequently, we may proceed to further preprocess the audio files. Once the data attains sufficient quality, the process culminates in statistical analysis. In this phase, the extracted vocal features may be compared through statistical tests or used to train prediction models. This whole process is depicted in Figure 1.\nIt is imperative to recognize that the process just described here is a simplified abstraction of the voice analytics process. Practical voice analytics is characterized by flexibility and adaptability rather than a rigidly linear progression. This iterative nature accommodates refinements and adjustments, ultimately enhancing the robustness and accuracy of the analytical outcomes.\n\n\n\n\n\n\nFigure 1: Streamlined view of the voice analytics pipeline\n\n\n\nTo illustrate how voice analytics operates in a real-world context, we offer a simple, practical tutorial using R. This tutorial walks you through the fundamental steps of the voice analytics pipeline, from reading audio files to extracting vocal features and drawing basic inferences."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#understanding-user-frustration",
    "href": "posts/2023/intro-to-voice-analytics/index.html#understanding-user-frustration",
    "title": "Introductory voice analytics with R",
    "section": "Understanding User frustration",
    "text": "Understanding User frustration\nIn this tutorial, we will analyze a compelling video featuring a female Scottish user attempting, albeit humorously, to issue a command to Amazon Alexa to play a song on Spotify. This viral video, though amusing, highlights a common frustration many users encounter when trying to communicate effectively with voice-controlled interfaces.\nVideo"
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#data-acquisition-and-processing",
    "href": "posts/2023/intro-to-voice-analytics/index.html#data-acquisition-and-processing",
    "title": "Introductory voice analytics with R",
    "section": "Data acquisition and processing",
    "text": "Data acquisition and processing\n\nData acquisition\nFor our comprehensive analysis, we begin by extracting the audio from the previous video and converting it into the Waveform audio file format (WAV). In this scenario, we are interested in two pivotal aspects of this interaction:\n\nSpeech Formation of the wake word “Alexa”\nVocal Changes During the Issuance of a Command (“Alexa, play something is cooking in my kitchen on Spotify by Dana”)\n\nTo facilitate our analysis, we cropped the voice recordings, retaining only the segments containing the two initial commands, including the wake word “Alexa”. In the first command, the speaker calmly requests Alexa to play a song. However, it becomes apparent that Alexa doesn’t comprehend the given command. Consequently, the speaker repeats the same command with a noticeable tone of frustration.\nOur following sections will delve into a detailed examination of this particular case, untangling the distinctions between these two commands that lead us to perceive frustration from the user’s perspective. You can download the files for this example by clicking the following button.\n\n\n\nDownload\n\n\n\nOur analytical approach primarily leverages the seewave package, which has emerged as the gold standard in R-sound analysis. This versatile package encompasses an impressive array of 130 functions designed for the analysis, manipulation, representation, editing, and synthesis of time-based audio waveforms. While seewave serves as our cornerstone, we also make reference to other valuable packages, such as tuneR, soundgen, and phonTools, for their specialized functionalities as needed.\n\n\nReading sound files\nAs previously mentioned, the primary focus of this tutorial centers around the utilization of the seewave package. While it is important to note that seewave lacks native capabilities for sound file reading, we adeptly overcome this limitation by harnessing functions from complementary packages. It is important to emphasize that some packages may use distinct classes for sound objects. Consequently, when choosing an alternative package to load sound data, it becomes paramount to consider this inherent class compatibility.\nIn the context of seewave, its core functionalities are tailored to work with sound objects of the Wave class. These Wave class sound objects are conventionally created using the tuneR package. Hence, when working with seewave, it is strongly recommended to employ tuneR for sound data loading.\nTo load the two user commands including the wake word from the interaction with Amazon Alexa, we use the readWave() function from the tuneR package. This function loads or reads a sound file from a specified location, which we need to pass as its main argument. Additionally, we assign the resulting outputs from reading the two commands to two objects called cmd1 and cmd2, as shown below:\n\nlibrary(tuneR)\ncmd1 &lt;- readWave(\"alexa_cmd1.wav\")\ncmd2 &lt;- readWave(\"alexa_cmd2.wav\")\n\nAfter loading these two recordings into R, we can call them to obtain an informative output showing several basic characteristics of these recordings. These characteristics encompass:\n\nNumber of Samples: This indicates the total count of discrete data points in the audio waveform.\nDuration (in seconds): The elapsed time in seconds, capturing the length of the audio.\nSampling Rate (in Hertz): Denoting the rate at which individual samples are taken per second.\nNumber of Channels: It signifies whether the audio is mono (single channel) or stereo (two channels).\nBit Rate: Representing the number of bits processed per unit of time.\n\nBelow we can see the output for the cmd1 and cmd2 objects:\n\ncmd1\n\n\nWave Object\n    Number of Samples:      335789\n    Duration (seconds):     7.61\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\ncmd2\n\n\nWave Object\n    Number of Samples:      368128\n    Duration (seconds):     8.35\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nUpon inspecting this information, it becomes evident that both recordings share identical sampling rates, channel numbers, and bit rates. However, the second recording is 0.74 seconds longer than the first.\nMoreover, the readWave() function provides additional optional arguments to enhance control over file reading. Notably, the from and to arguments enable users to selectively read specific segments of the audio file. By default, these arguments operate in sample units, defining the segment based on sample counts. However, the readWave() function introduces the units argument, allowing users to customize the units of the from and to arguments to seconds, minutes, or hours.\nTo illustrate, suppose we aim to extract two segments from the first command, denoted as cmd1.s1 and cmd1.s2. The first segment covers the initial 0.5 seconds of the recording, while the second spans from that point to 2 seconds. This can be accomplished by directly using the readWave() function and specifying the from, to, and units arguments, as shown below:\n\n(cmd1.s1 &lt;- readWave(\"alexa_cmd1.wav\",from=0,to=0.5,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      22050\n    Duration (seconds):     0.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(cmd1.s2 &lt;- readWave(\"alexa_cmd1.wav\",from=0.5,to=2,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      66150\n    Duration (seconds):     1.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWrapping the code in parentheses triggers automatic printing.\n\n\n\n\nPlaying a sound file\nSomething that we may need at several points of the voice analytics pipeline is to play the recordings/processed recordings, as an additional way to inspect it. Although, R itself cannot play sound files the seewave’s listen() function allows us to call the default audio player of the user’s operating system from R to play the selected audio.\nTo do so, we first load the seewave package:\n\nlibrary(seewave)\n\nNow, you can employ the listen() function to play audio, for instance, to play the sound recorded in cmd1:\n\nlisten(cmd1)\n\n\n\n\nWe could do the same for the second command:\n\nlisten(cmd2)\n\n\n\n\nBoth commands convey identical content but with a slight variation in order. In the first command, the speaker instructs: “Alexa, play ‘Something Is Cooking in My Kitchen’ on Spotify by Dana”. In contrast, the second command the speaker says: “Alexa, play ‘Something Is Cooking in My Kitchen’ by Dana on my Spotify”.\nSimilar to the readWave() function, listen() supports the from and to arguments, enabling precise selection of sections for auditory playback. Additionally, it allows us to manipulate the sampling frequency rate through the f argument, altering the speaking rate. You can run the following code to hear the first command (cmd1) with a sampling rate 10% higher and with a sampling rate 10% lower, respectively:\n\nlisten(cmd1, f=cmd1@samp.rate*1.1)\n\n\n\n\n\nlisten(cmd1, f=cmd1@samp.rate/1.1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor convenience, subsequent sections directly include sound players after each processed or newly generated audio without explicitly calling the listen() function. Nevertheless, it is crucial to remember that for playing Wave objects through R, the listen() function must be utilized.\n\n\n\n\nPreprocessing sound files\nIn many instances, effective preprocessing of diverse voice files is crucial to optimize their overall quality. This preprocessing involves a variety of tasks, such as (1) extracting specific segments of interest from a sound wave, (2) removing selected utterances from a soundwave, (3) trimming periods of silence at the beginning or end of a sound file, (4) filtering out all unvoiced frames from a sound file, and (5) eliminating background noise.\nThe tuneR and seewave packages provide a comprehensive set of functions designed to address these diverse preprocessing procedures:\n\nextractWave(): This function facilitates the extraction of desired segments from a soundwave. Users can specify the segments using the from and to arguments, as discussed earlier. The default units for the extractWave() function are samples, but users can adjust this using the xunit argument. Specifically, setting ‘xunit’ to “time” enables the extraction of segments in seconds.\ndeletew(): This function removes specific portions from a soundwave. As in the case of extractWave(), users can specify segments using the from and to arguments. Notably, for this function, these values are directly specified in seconds. By default, this function returns a matrix, but we can change the output type to a Wave object by specifying the output argument to \"Wave\".\nnoSilence(): Particularly useful for removing periods of silence from the beginning and/or end of a sound file. By default, it removes silence periods from both the beginning and end. However, users can modify this behavior using the where argument, specifying \"start\" to remove only the beginning silent frames or \"end\" to remove only the end silent frames.\nzapsilw(): This function eliminates all unvoiced frames from a sound file. Users can customize this operation by setting the ‘threshold’ argument, which measures the amplitude threshold (in percent) distinguishing silence from signal. The zapsilw() function also, by default, plots oscillograms for both the original sound file and the modified version (after removing the silent voice frames), providing visual insight into the process. Automatic plotting of oscillograms can be deactivated by setting the plot argument to FALSE. Like other functions within the seewave package, this function returns a matrix by default. However, the output type can be changed to a Wave object specifying the output argument as \"Wave\".\nrmnoise(): The rmnoise() function effectively eliminates background noise from a sound file through smoothing. Like other functions within the seewave package, this function returns a matrix by default. However, the output type can be changed to a Wave object specifying the output argument as \"Wave\".\n\nThese functions allow us to easily manipulate sound files, ensuring they are tailored to meet the specific requirements of the analyses. To illustrate their practical utility, let’s delve into some illustrative examples.\n\nUsing the extractWave() function\nFor example, let’s employ the extractWave() function to isolate a specific segment from the first command which we assigned to the cmd1 object. Suppose our goal is to extract the initial 0.8 seconds of that file. To achieve this, we must set four arguments. Initially, the primary argument should be the object of the Wave class, representing the sound file from which we intend to extract a segment. Next, we need to specify the from and to arguments, indicating 0 and 0.8, respectively—indicating the segment we wish to extract spans from 0 to 0.8 seconds. It’s essential to note that, by default, these arguments are not expressed in seconds. Consequently, we need to explicitly set the xunit argument to \"time\" to ensure the units are interpreted as seconds. Otherwise, they would be interpreted as sample units. Therefore, we can extract the first 0.8 seconds from the first command stored in cmd1, which corresponds to the wake word “Alexa”, storing the resulting isolated segment in an object called cmd1.xtr, as demonstrated below:\n\n#Extract first 700ms\ncmd1.xtr &lt;- extractWave(cmd1, from = 0, to = 0.8, xunit = \"time\") \n\n\n\n\n\n\nUsing the deletew() function\nAlternatively, rather than extracting this segment, we can adopt the opposite strategy: removing this segment. This task is easily accomplished using the deletew() function. The arguments required for this operation are quite similar to the previous ones, with the distinction that there’s no need to specify an xunit argument, as the units are already in seconds (and cannot be changed). However, it is essential to specify an output argument to obtain an output of the Wave class. Consequently, we can create a new Wave object that excludes the first 0.8 seconds from the initial command, i.e., excluding the wake word “Alexa”, storing the output into cmd1.rem in the following manner:\n\n#Delete first 800ms\ncmd1.rem &lt;- deletew(cmd1, from=0, to=0.8, output=\"Wave\") \n\n\n\n\n\n\nUsing the noSilence() function\nThus far, we have delved into the extraction and deletion of specific audio segments defined by a time frame. However, there are scenarios where our interest lies in removing segments that meet specific conditions, such as unvoiced segments at the outset and conclusion of an audio file. This practice is frequently employed to standardize audio files, as variations in the length of unvoiced frames at the start and end may not necessarily be linked to speaker pauses but could be influenced by other factors. For instance, this variability could be attributed to the individual recording, taking additional time to instruct the speaker to commence or conclude their speech, or to manage the recording process after the speaker has concluded.\nTo accomplish this, we can use the noSilence() function. Therefore, if we wish to eliminate the initial and end unvoiced frames of the initial command, stored in cmd1, and store the output in a new object called cmd1.cut, we can achieve this with the following code:\n\n#Remove only unvoiced start and ending\ncmd1.cut &lt;- noSilence(cmd1, level = 350)\n\nIt is important to highlight that we define a argument called level with a value of 350. This argument determines the amplitude level below which samples are considered unvoiced and subsequently removed. BBy default, this value is initialized to zero, which proves overly restrictive in our context. This default setting would result in the detection of no unvoiced areas, given the presence of background noise at the end of the audio, despite these areas being unvoiced.\nTo address this limitation, we choose a much higher value, specifically 350. This value is carefully selected to be sufficiently elevated to avoid removing voiced areas while effectively identifying and removing unvoiced segments. After running the previous code, we proceed to compare the original and processed commands:\n\ncmd1\n\n\nWave Object\n    Number of Samples:      335789\n    Duration (seconds):     7.61\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\n\ncmd1.cut\n\n\nWave Object\n    Number of Samples:      305734\n    Duration (seconds):     6.93\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nUpon comparing both audio files, we can observe that the processed version is slightly shorter, specifically by 0.68 seconds, compared to the original command. Additionally, when listening to both audios, we can discern that the content of the audio has been effectively preserved in the processed version.\n\n\nUsing the zapsilw() function\nAlternatively, we could eliminate all the unvoiced frames—not only those at the start and end but across all segments—of the first command (cmd1) using the zapsilw() function, as shown below:\n\n#Remove all unvoiced frames of a soundwave\ncmd1.nosil &lt;- zapsilw(cmd1, output=\"Wave\")\n\n\n\n\n\n\n\nFigure 2: Oscillogram comparison: original (top) vs. processed first command recording with unvoiced frames removed using default zapsilw() parameters (bottom)\n\n\n\n\n\nBy default, the zapsilw() function generates an oscillogram that compares the original sound recording with its processed counterpart, exemplified in Figure 2. By comparing both oscillograms we can see how unvoiced frames have been removed, characterized by minimal or absent amplitude. Furthermore, for a comprehensive analysis, the characteristics of both audio files can be compared by calling cmd1 and cmd1.nosil:\n\ncmd1\n\n\nWave Object\n    Number of Samples:      335789\n    Duration (seconds):     7.61\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\n\ncmd1.nosil\n\n\nWave Object\n    Number of Samples:      101647\n    Duration (seconds):     2.3\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nWhen we look at the two files and compare their features, we can see that removing the unvoiced frames (cmd1.nosil) results in a reduction in both the number of samples and the recording duration. The original first command lasts for 7.61 seconds, whereas the processed command, with unvoiced frames removed, is much shorter, specifically having a duration of 2.30 seconds. In simpler terms, this means that 5.31 seconds, which corresponds to the unvoiced frames, have been effectively removed during the process. However, upon listening to cmd1.nosil, it becomes evident that the audio is now nearly incomprehensible. This is because the zapsilw() function removed frames that weren’t strictly unvoiced but had a significantly lower amplitude compared to the majority of voiced frames. The zapsilw() function includes an additional argument, the threshold, aimed at determining what qualifies as an unvoiced frame. The default threshold is 5% (5). Since the current threshold value resulted in the removal of an excessive number of frames for our specific case, let’s investigate the effects of adopting a much lower value, such as 0.3% (0.3):\n\ncmd1.nosil2 &lt;- zapsilw(cmd1, threshold=0.3, output=\"Wave\")\n\n\n\n\n\n\n\nFigure 3: Oscillogram comparison: original (top) vs. processed first command recording with unvoiced frames removed using threshold parameter equal to 0.3 (bottom)\n\n\n\n\n\n\ncmd1.nosil2\n\n\nWave Object\n    Number of Samples:      257922\n    Duration (seconds):     5.85\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nAs observed, the processed audio file is now longer than in the previous (5.85 vs 2.30 seconds), reflecting a decrease in the number of frames identified as unvoiced. Additionally, upon listening, the processed audio exhibits a more natural sound compared to the previous version. It’s worth noting that, while it may not replicate the exact naturalness of the original, this deviation is common. Humans naturally introduce pauses and breaks in speech, and the removal of unvoiced frames can contribute to this altered perception.\n\n\n\n\n\n\nDefining function arguments\n\n\n\nThe choice of preprocessing function arguments, which influence the behavior of the function, is a deliberate and thoughtful process. When establishing these parameters, we typically engage in an iterative approach. This involves listening to the resulting audio and visualizing it to ensure that the outcome aligns with our desired specifications. Although we refrain from explicitly illustrating this iterative process to maintain the clarity of this post, it’s essential to acknowledge its presence.\nIt’s important to recognize that determining the appropriate values for these arguments during file preprocessing is not a straightforward task. Instead, it demands careful consideration and may involve multiple iterations to arrive at the optimal values. This underscores the significance of a meticulous and thoughtful approach when fine-tuning these parameters to achieve the desired results.\n\n\nIt’s essential to consider that the decision to remove all unvoiced breaks should align with our analytical goals. If our aim is to analyze or extract information from the breaks and their duration, it might be preferable to solely eliminate unvoiced frames from the beginning and end of the recording using the noSilence() function instead of the zapsilw() function.\n\n\nUsing the rmnoise() function\nTo improve audio quality, we take additional steps beyond eliminating unvoiced frames. We enhance the quality further by utilizing the rmnoise() function to reduce background noise, as in this audio clip, you can clearly hear a disturbance that sounds like a metal object, most likely a teaspoon, hitting a glass or a cup.\nRecognizing that breaks in audio can offer insights into user frustration, we focus on using the processed version of the command—cmd1.cut—where we have only removed unvoiced frames from the start and the end, instead of using the version in which we removed all the unvoiced frames.\nIt’s important to highlight that in this specific scenario, we must explicitly set the output argument as \"Wave\" when using the rmnoise() function to obtain a Wave object. Otherwise, the output would be in the matrix format. Additionally, we fine-tuned the noise reduction process by adjusting the spar argument. This parameter essentially governs the extent of noise reduction—higher values lead to less audible background noise. However, it’s essential to be cautious since increasing the spar value not only diminishes background noise but also introduces a trade-off, potentially altering other parts of the audio. The spar argument typically takes values between 0 and 1, but it can also take other values. In this case, since the background noise is quite prominent, we proceed to set a spar value equal to 1.15:\n\n# Remove noise\ncmd1.nonoise &lt;- rmnoise(cmd1.cut, output = \"Wave\", spar = 1.15)\n\n\n\n\nUpon listening, it’s evident that the rmnoise() function effectively reduced the volume of the background noise— the metallic sound resembling an object striking a glass or cup—although traces of that sound persist. However, the heightened value of the spar argument in the rmnoise() function slightly impacted the overall audio quality. To address this, we employ additional functions to further minimize the noise and enhance audio quality:\n\nInitially, we apply the afilter() function, designed to eliminate signals with amplitudes below a specified threshold. The objective is to target the background noise, which now has a significantly lower amplitude compared to the rest of the audio. We control the threshold using the threshold argument, setting it to a low value, specifically 0.075.\nSubsequently, having applied the afilter() function, we revisit the rmnoise() function, this time with a reduced spar value of 0.75. With this step, we ensure thorough noise removal.\nFinally, we use the preemphasis() function, which amplifies high-frequency content in the sample. Given that we have either completely or nearly eliminated the background noise, we emphasize the high-frequency content that may have downplayed by earlier functions. This strategic emphasis aims to enhance the quality of the remaining sound.\n\nWe store the resulting processed audio in a new object called cmd1.filtered. The code for all the mentioned steps is provided below:\n\ncmd1.filtered &lt;- afilter(cmd1.nonoise, output = \"Wave\", threshold = 0.075, plot = FALSE)\n\ncmd1.filtered &lt;- rmnoise(cmd1.filtered, output = \"Wave\", spar = 0.75)\n\ncmd1.filtered &lt;- preemphasis(cmd1.filtered, alpha = 0.975, output = \"Wave\")\n\n\n\n\nAs we can tell, the voice quality has significantly improved now. While there’s still a bit of background noise, it’s notably reduced compared to the original audio.\n\n\nPreprocessing the second command\nAfter completing the preprocessing for the initial command, we proceed to apply the same preprocessing steps for the second command in this way to have fair comparisons of audio files (as with the preprocessing we manipulate some of the features of the audio file).\n\n#Remove only unvoiced start and ending. In this case, we use a parameter level equal to 800, as there's some background noise at the start and at the end. In this way, we can cut these areas.\ncmd2.cut &lt;- noSilence(cmd2, level = 800)\n#Remove noise\ncmd2.nonoise &lt;- rmnoise(cmd2.cut, output = \"Wave\", spar = 1.15)\n\ncmd2.filtered &lt;- afilter(cmd2.nonoise, output = \"Wave\", threshold = 0.075, plot = FALSE)\n\ncmd2.filtered &lt;- rmnoise(cmd2.filtered, output = \"Wave\", spar = 0.75)\n\ncmd2.filtered &lt;- preemphasis(cmd2.filtered, alpha = 0.975, output = \"Wave\")\n\n\n\n\n\n\n\nWriting sound files\nOnce you’ve made adjustments or enhancements to a sound file, preserving the edited version for future use is essential. The seewave package makes this process easy through savewav() function, specifically designed for saving R sound objects as .wav files. To utilize this function effectively, you’ll need to specify three crucial arguments:\n\nR Sound Object (wave): R object you want to save as a .wav file.\nSampling Frequency (f): Sampling frequency for the saved .wav file. If the R object you want to save is of the Wave type, there’s no need to specify such argument.\nFilename (filename): Name under which the edited sound object will be saved.\n\nAs a practical example, let’s save the background noise-free version of cmd1 and cmd2 (cmd1.filtered, and cmd2.filtered) as .wav files named cmd1_filtered.wav and cmd2_filtered.wav, respectively, within our system.\n\nsavewav(cmd1.filtered, filename = \"cmd1_filtered.wav\")\nsavewav(cmd2.filtered, filename = \"cmd2_filtered.wav\")"
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#visualizing-sound",
    "href": "posts/2023/intro-to-voice-analytics/index.html#visualizing-sound",
    "title": "Introductory voice analytics with R",
    "section": "Visualizing sound",
    "text": "Visualizing sound\nAfter addressing the stages of reading, editing, and saving sound objects, our next step involves visualizing the characteristics of a sound wave. Visualization serves as the process of translating a sound wave into a graphical representation. The key aspects typically depicted in a sound wave visualization are its (1) amplitude, (2) frequency, and (3) a combination of the previous two. All of them are usually illustrated against time.\nThe primary visualizations employed for this purpose are:\n\nOscillograms: These representations focus on capturing amplitude variations, providing a visual insight into the intensity or strength of the sound wave at different points in time.\nSpectrograms: This type of visualization offers a comprehensive view of both frequency and the dynamic relationship between frequency and amplitude over time.\n\nIt is important to highlight that, in this context, our progression directly shifts to visualization without the intermediary step of feature extraction, as depicted in Figure 1. This decision is motivated, in part, by the inherent capability of the visualization functions to directly extract the pertinent features prior to their visualization.\n\nVisualizing amplitude\n\n\n\n\n\n\nAmplitude\n\n\n\nAmplitude quantifies the extent of a wave’s displacement from its average value. In the context of sound, it specifically denotes the degree to which air particles deviate from their equilibrium position. Amplitude serves as a key factor in determining the strength or loudness of a sound and is expressed in decibels (dB).\n\n\nOscillograms offer a visual representation of the amplitude of a soundwave plotted against time. They are often referred to as waveforms, as they graphically depict the variations within the sound wave itself. Oscillograms serve as valuable tools for discerning potential changes in loudness over time within a soundwave. In R, you can create oscillograms using the oscillo() function from the seewave package. This function requires just one argument, the sound object. Moreover, oscillo() provides the flexibility to customize various visual aspects, such as the title (using the title argument), label color (via the collab argument), and wave color (by setting the colwave argument). Additionally, you can specify the from and to arguments, similar to what we did during data processing, to generate an oscillogram for a specific time interval in seconds.\nTo gain insights from the oscillograms of the two Alexa commands, we aim to first visualize the entire soundwave including the wake word (cmd1 and cmd2) and then zoom in to focus solely on the articulation of the wake word.\nTo consolidate all four graphs within a unified plotting area, we employ the standard par and mfrow arguments in R, partitioning the plot into four distinct sections. For an exclusive display of the wake word, the from and to arguments within the oscillo() function are utilized. Additionally, we utilize the colwave argument to distinguish the entire command plots in black and the isolated wake words in blue. We set automatic titles for the oscillograms by enabling the title argument to be TRUE for the complete commands, providing information on total time and sampling rate. For the wake words, we actively set a custom title by specifying the desired text in this argument.\nTo enhance differentiation between whole command and isolated wake word plots, we go a step further and adjust their axis label colors to red, setting the collab argument to red. The complete code is provided below:\n\nGeneral viewDetailed view of commands 1 and 2\n\n\n\npar(mfrow=c(2,2))\n\noscillo(cmd1.filtered, colwave=\"black\", collab=\"black\", title = TRUE)\n\noscillo(cmd2.filtered, colwave=\"black\", collab=\"black\", title = TRUE)\n\noscillo(cmd1.filtered, from=0, to=.7, colwave=\"blue\", collab=\"red\", \n             title = \"First     Command - wake word Only\")\n\noscillo(cmd2.filtered, from=0, to=.7, colwave=\"blue\", collab=\"red\", \n             title = \"Second Command - wake word Only\") \n\n\n\n\n\n\n\nFigure 4: Oscillograms for the first and second command, including the wake word (upper panel, colored in black) and isolated wake words (lower panel, colored in blue)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Detailed view of the oscillogram for the first command, including the wake word\n\n\n\n\n\n\n\n\n\nFigure 6: Detailed view of the oscillogram for the second command, including the wake word\n\n\n\n\n\n\nIn addition to the oscillograms we just created, which are presented in Figure 4. We have included two additional and separate oscillograms, presenting a detailed view for the initial and second commands, including the wake word. Notably, we adjusted the oscillogram of the initial command to align with the temporal segment of the second command. This adjustment facilitates a direct comparison between the two. Moreover, we have accompanied these two additional oscillograns by textual transcriptions for each utterance. The visual representations of these detailed oscillograms are illustrated in Figure 5 and Figure 6, respectively, and can be viewed by clicking the “Detailed view of commands 1 and 2” tab.\nUpon closer examination of these oscillograms, a few notable observations come to light at first glance:\n\nDifference in voice breaks: The first command exhibits a broader voice break between the wake word “Alexa” and the subsequent portion compared to the second command. Following this, the majority of voice breaks are relatively brief. However, in the second command, there is another extended voice break between “by Dana” and “on my Spotify”.\nEmphasis on Individual Words: The second command exhibits a somewhat clearer distinction between utterances in comparison to the first command. Moreover, discernible variations in the articulation of specific words are observable, as indicated by the distinct shapes of the utterances. For instance, in the first command, the term “on” is accentuated with a higher amplitude, signifying a louder pronunciation in contrast to the second command. Moreover, the wake word “Alexa” reveals disparities between the two commands, with more pronounced “irregularities” in the second command. These irregularities entail fluctuations in amplitude, particularly noticeable when enunciating the initial part of the word.\n\n\n\nVisualizing fundamental frequency\n\n\n\n\n\n\nFundamental frequency\n\n\n\nThe fundamental frequency (F0) is the lowest frequency present in a waveform, and it determines the perceived pitch of the voice, influencing how sounds are interpreted as high or low. At higher F0 values, the associated sounds are perceived as higher in pitch.\nTherefore, it plays a critical role in conveying the tonal and rhythmic properties of speech, being instrumental in transmitting linguistic objectives in speech communication. Additionally, it is intimately tied to gender perception—adult men generally exhibit F0 values ranging from 80 to 175Hz. While adult women typically fall between 160 and 270Hz.\n\n\nIn addition to examining amplitude, a crucial aspect involves visualizing the fundamental frequency. This is often represented as a plot of fundamental frequency against time, referred to as an “F0 contour” or “pitch track.”\nThis visualization yields valuable insights into the linguistic aspect of tone, contributing supplementary information to enrich our comprehension of message delivery. For the sake of simplicity, we will narrow our focus to the wake word for this visualization and subsequent visualizations. This decision is based on our previous observations in the oscillograms, where we discerned subtle variations in the shapes of the wake word “Alexa” between the two commands. Consequently, our objective is to delve deeper into understanding the delivery of this specific word, particularly in terms of its tone.\nTo proceed with this exploration, we employ the extractWave() function to extract the wake words from the first and second commands. Specifically, we focus on the initial 0.7 seconds of both commands, the duration during which (approximately) the wake word is situated. In addition, we use the noSilence() function, to make sure that we did not extract some unvoiced section at the start or at the end. The extracted wake word from the first command is saved as an object named w1, while the wake word from the second command is stored as w2:\n\nw1 &lt;- extractWave(cmd1.filtered, from = 0, to = .7, xunit = \"time\")\nw1 &lt;- noSilence(w1, level = 5)\n\n\n\n\n\nw2 &lt;- extractWave(cmd2.filtered, from = 0, to = .7, xunit = \"time\")\nw2 &lt;- noSilence(w2, level = 5)\n\n\n\n\nHaving isolated the two wake words, the next step involves concatenating them into a single audio file, with the second wake word playing immediately after the first one. This concatenation is achieved using the bind() function from the tuneR package, which seamlessly combines the provided Wave objects:\n\n(wake_all &lt;- bind(w1,w2))\n\n\nWave Object\n    Number of Samples:      55317\n    Duration (seconds):     1.25\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nAfter combining the two wake words, we utilize the autoc() function of the seewave package on the concatenated sound object, wake_all. This function produces a plot that visualizes the fundamental frequency of the audio object across different time points. Additionally, we specify the ylim parameter to limit the y-axis between 0 and 600 Hz (0.6 KHz). Finally, we add a vertical line using the abline() function, separating the wake words from the first and second commands. The resulting plot from this process is depicted in Figure 7.\n\n#Note: ylim units need to be in KHz\nF0 &lt;- autoc(wake_all, ylim = c(0, 0.6)) \n#We add a separation line, between the wake word 1 and wake word 2\nabline(v = 0.635,col=\"red\",lwd=2,lty=2)\n\n\n\n\n\n\n\nFigure 7: Pitch track for the two wake words\n\n\n\n\n\nThis plot reveals distinct pitch contours for both wake words. In the wake word of the first command, the fundamental frequency remains relatively constant, hovering around 200 Hz (0.2 KHz) for the initial 0.2 seconds. Subsequently, there is some variability for a few seconds before a noticeable increase during the last 0.15 seconds, reaching approximately 300 Hz. Conversely, in the wake word of the second command, the fundamental frequency ascends from 200 Hz (0.2 KHz) to around 300 Hz in the first 0.2 seconds, followed by a subsequent decrease.\n\nVisualizing formants\nTo enrich our analysis, we proceed to visualize key formants, which are the frequencies that resonate most prominently in human speech. The first three formants, denoted as F1, F2, and F3, are particularly informative. Formants play a pivotal role in discerning distinct vowels, contributing to the nuances of speech sounds.\nFor formant visualization, we employ the formanttrack() function from the phonTools package. It’s essential to note that the phonTools package exclusively supports mono audio. In our case, since we are dealing with stereo files, we specify only one channel. The channel of a wave object can be accessed using the @ operator along with the desired channel name (left or right). Additionally, since phonTools is not specifically tailored for Wave objects, the sampling frequency of the sound file must be manually set using the fs argument. Thus, we proceed to load the phonTools package to plot the first three formants and, then, use the formanttrack() function to the two wake words:\n\nlibrary(phonTools)\npar(mfrow=c(1,2))\nformanttrack(w1@left, fs=w1@samp.rate, formants=3, periodicity=.5)\nformanttrack(w2@left, fs=w2@samp.rate, formants=3, periodicity=.5)\n\n\n\n\n\n\n\nFigure 8: First, second ,and third formant tracks for the two wake words associated with the first (left panel) and second (right panel) commands\n\n\n\n\n\nIn Figure 8, we can see the first three formants for each wake word represented by different colors. By scrutinizing the formant tracks of the wake words in both commands, discernible distinctions emerge, offering insights into the speaker’s tone and emotional disposition.\nIn dissecting the first command’s wake word, we observe a more tightly spaced and evenly distributed set of formant frequencies compared to the second command’s wake word. This discrepancy implies a composed and relaxed speech pattern for the first command’s wake word than for the second, which exhibits greater variability, hinting at heightened tension and force in the speaker’s voice—reflecting the speaker frustration.\nDigging deeper into the analysis, the higher first formant frequency (F1) in the second command’s wake word suggests a wider mouth opening or an elevated tongue position, contributing to a more resonant and forceful vocal delivery. Similarly, the elevated second formant frequency (F2) in the second command’s formant track points to lip rounding or vocal tract narrowing, characteristics associated with increased vocal strength.\nOf particular note is the less pronounced third formant frequency (F3) in the second command’s wake word, indicating a degree of vocal tract constriction. While this intensifies the voice, it may also impart a muffled or harsh quality.\nIn summary, the formant tracks strongly imply that the speaker imparts greater force and tension in the second command, highlighting the undercurrent of frustration. Recognizing these discernible patterns in vocal intensity and tension enriches our understanding of the speaker’s emotional state and demeanor.\n\n\n\nSpectrograms\nSpectrograms offer a detailed, multidimensional representation of a soundwave, representing time along the x-axis, frequency along the y-axis, and amplitude levels (loudness) through varying color codes. They are specially useful for detecting audio problems by sight.\nThe seewave package provides the spectro() function, allowing us to easily create spectrograms. When using this function, you only need a Wave object as input. However, there are optional parameters that allow you to customize the appearance of the spectrogram. For instance, the flim argument allows the specification of minimum and maximum frequencies displayed, andthe osc parameter introduces an oscillogram at the bottom of the spectrogram plot.\nThe default color scheme in the spectro() function is relative, utilizing cyanred for regions with the highest amplitude in comparison to the entire audio representation, with all other colors relative to that maximum value. As a result, for an accurate comparison between the two wake words, rather than creating separate spectrograms for each, we will generate a unified spectrogram for the binned wake words (wake_all). Furthermore, we add a vertical line separating both wake words by using the abline() function:\n\nspectro(wake_all, osc=TRUE, flim=c(0,1.5))\nabline(v = 0.62,col=\"red\",lwd=2,lty=2)\n\n\n\n\n\n\n\nFigure 9: Spectrogram for the wake words associated with the first and second commands\n\n\n\n\n\nBy checking Figure 9, we can observe how the second command’s wake word, shows a wider range of frequencies, especially at the beginning of the word “Alexa”. Moreover, the lower frequencies in this wake word are louder than any other part of both wake words. Importantly, the second wake word also seems to have higher variability in terms of intensity."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#acoustic-feature-extraction",
    "href": "posts/2023/intro-to-voice-analytics/index.html#acoustic-feature-extraction",
    "title": "Introductory voice analytics with R",
    "section": "Acoustic feature extraction",
    "text": "Acoustic feature extraction\nAfter visually inspecting the audio, we move on to acoustic feature extraction. Visual examination helps us understand vocal features like amplitude and fundamental frequency, but it’s more of a qualitative overview. This overview can guide us in preprocessing or identifying specific areas that need attention. Getting a greater understanding of the audio data requires extracting numerical information that we can obtain through acoustic feature extraction. This process converts auditory signals into measurable characteristics, allowing for a more detailed analysis. Hence, we proceed to extract some key vocal features across the time, amplitude, frequency, and spectral domains.\n\nTime associated characteristics\nThe primary measure in the time domain is duration, usually expressed in seconds or milliseconds. It indicates the temporal length of a soundwave. The duration() function in the seewave package facilitates the direct extraction of a sound object’s duration in seconds. By applying this function to two commands, we observe that the first command has a shorter duration (6.93 seconds), compared to the second command (7.68 seconds).\n\nduration(cmd1.filtered)\n\n[1] 6.932744\n\nduration(cmd2.filtered)\n\n[1] 7.677347\n\n\nThe soundgen package is a handy package for feature extraction, and more specifically its analyze(). This function enables the extraction of various features spanning time, amplitude, frequency, and spectral domains. Examples include the fundamental frequency, percentage of voiced frames, amplitude, Harmonics-to-Noise ratio, and more.\nWhen you use the analyze() function, it generates two data.frames. The first, a detailed data.frame ($detailed), breaks down each frame of the analyzed audio, with each column representing a vocal feature. The second, a summarized data.frame ($summary), condenses information to one row per file, summarizing vocal features with statistics like mean and standard deviation.\nWe can proceed to employ the analyze() function to extract multiple vocal features from both the first and second commands, storing its outcome to feat_cmd1 and feat_cmd2:\n\nlibrary(soundgen)\nfeat_cmd1 &lt;- analyze(cmd1.filtered, plot = F)\nfeat_cmd2 &lt;- analyze(cmd2.filtered, plot = F)\n\nNow that we possess two objects containing information on distinct vocal characteristics from the two commands, our next step is to complement and quantify some of the key insights derived from our previous visualizations. In Figure 4, we observed differences in voice breaks between both commands. Specifically, the first command exhibited a longer voice break between the wake word and the rest of the command, whereas the second command had an extended voice break between “by Dana” and “on my Spotify”.\nDespite these observations, visually determining which command has a lower proportion of voice breaks was challenging. To address this, we can observe the percentage of voiced frames extracted by the analyze() function. This can be achieved by extracting the voiced column from the summary data.frame for both feat_cmd1 and feat_cmd2:\n\n#Returns the proportion of voiced samples\nfeat_cmd1$summary$voiced\n\n[1] 0.5217391\n\nfeat_cmd2$summary$voiced\n\n[1] 0.5588235\n\n\nRevealing a greater percentage of voiced frames in the second (55.88%) compared to the first command (52.17%). In other words, the second command has a lower percentage of unvoiced frames (voice breaks) than the first command. This information can be easily translated into total seconds of voice breaks by subtracting the voiced value from 1 and multiplying the result by the duration of each command:\n\n(1 - feat_cmd1$summary$voiced) * duration(cmd1.filtered)\n\n[1] 3.31566\n\n(1 - feat_cmd2$summary$voiced) * duration(cmd2.filtered)\n\n[1] 3.387065\n\n\nWhen examining the duration of voice breaks in seconds, we can still see that the first command has longer duration voice breaks. However, this difference is more pronounced in relative terms, considering that the length of the first command is shorter than that of the second.\nIn addition, as we previously saw in Figure 4 is that the first command displays a lengthier voice break between the wake word and the subsequent part of the command compared to the second command. Thus, we may think that excluding that part, the second command has a greater amount of voice breaks, i.e. that the duration of the voice breaks in the rest of the command is lengthier.\nTherefore, it’s interesting to examine the percentage of voice breaks solely within the rest of the command, excluding the wake word and the break between it and the remainder of the command. To achieve this, we use the deletew() function to remove the portion of each command containing the wake word. Subsequently, we ensure the elimination of all unvoiced frames preceding the remaining command section using the noSilence() function:\n\ncmd1.without.wakeword &lt;- deletew(cmd1.filtered, from = 0, to = 1, output = \"Wave\")\ncmd1.without.wakeword &lt;- noSilence(cmd1.without.wakeword, level = 25)\nfeat_cmd1_no_wakeword &lt;- analyze(cmd1.without.wakeword, plot = F)\n\ncmd2.without.wakeword &lt;- deletew(cmd2.filtered, from = 0, to = 1, output = \"Wave\")\ncmd2.without.wakeword &lt;- noSilence(cmd2.without.wakeword, level = 25)\nfeat_cmd2_no_wakeword &lt;- analyze(cmd2.without.wakeword, plot = F)\n\nAfter doing that, we can now extract the proportion of voiced frames by referencing the voiced column in the summary data.frame of the generated output:\n\nfeat_cmd1_no_wakeword$summary$voiced\n\n[1] 0.7043011\n\nfeat_cmd2_no_wakeword$summary$voiced\n\n[1] 0.6300813\n\n\nThese values reveal that after removing the wake word and the voice break between it and the subsequent part of the command, the first command exhibits a higher proportion of voiced frames (70.43%) compared to the second command (63.01%). Consequently, when excluding the wake word, the voice breaks in the second command are longer than those in the first command.\nTherefore, the first command has an overall greater proportion of voice breaks than the second command. However, this increased proportion of voice breaks is largely attributed to the extended break following the wake word. Once we eliminate the wake word and the subsequent break from both commands, it becomes evident that the second command actually has a higher proportion of voice breaks.\n\n\nIntensity associated characteristics\nPreviously, we delved into this domain by inspecting the oscillograms for the two commands. In doing so, we discerned subtle distinctions in the shapes of the individual words within each command. Expanding on this observation, our subsequent action entails quantifying the central point around which the amplitude tends to fluctuate within each command. This quantification is achieved by calculating the mean of the amplitude across each command. Additionally, we aim to measure the extent of deviation from this central point, which we will quantify using the standard deviation of the amplitude.\nAs previously said, the amplitude of a soundwave indicates its power or loudness, where smaller amplitudes represent softer sounds, and larger amplitudes denote louder ones. It essentially measures how far air particles deviate from their usual position. It’s important to note that these deviations can be both positive and negative. To tackle this, a common method for measuring amplitude is using the root mean square.\nThe analyze() function extracts the root mean square amplitude (ampl), which calculates the root mean square of the amplitude, excluding unvoiced frames. However, when summarizing this value over a time range, it might be lower than its actual value because unvoiced segments are considered. To address this, the analyze() function also extracts the root mean square amplitude for only the voiced areas (ampl_noSilence). Therefore, we proceed to compare the average root mean square amplitude, excluding unvoiced areas, between the first and the second command:\n\nfeat_cmd1$summary$ampl_noSilence_mean\n\n[1] 0.00150876\n\nfeat_cmd2$summary$ampl_noSilence_mean\n\n[1] 0.001395994\n\n\nExamining these values, we notice that the first command, on average, has a slightly higher amplitude than the second command. Moving forward, our next step involves determining the extent to which the amplitude deviates from this average within each command. This variability is quantified through the calculation of the standard deviation, which can also be obtained from the summary data.frame generated by the analyze() function:\n\nfeat_cmd1$summary$ampl_noSilence_sd\n\n[1] 0.0009200839\n\nfeat_cmd2$summary$ampl_noSilence_sd\n\n[1] 0.0009812488\n\n\nWe can observe that both commands have nearly identical standard deviations, indicating similar variations in amplitude for both commands.\nSimilarly, we could obtain information regarding the subjective loudness through the loudness column extracted through the analyze() function, providing a more comprehensive measure:\n\nfeat_cmd1$summary$loudness_mean\n\n[1] 0.483008\n\nfeat_cmd2$summary$loudness_mean\n\n[1] 0.4876107\n\nfeat_cmd1$summary$loudness_sd\n\n[1] 0.1864108\n\nfeat_cmd2$summary$loudness_sd\n\n[1] 0.1804802\n\n\nSimilar to the analysis of amplitude, we observe a close resemblance in both the average and standard deviation of loudness between the two commands.\nTo get into a higher level of granularity, we could also extract such information for the wake words associated with each command. To do so, first, we need to use the analyze() function, which we can do in the following way:\n\nfeat_w1 &lt;- analyze(w1, plot = F) \nfeat_w2 &lt;- analyze(w2, plot = F) \n\nNow, we can move forward to compute the average value and standard deviation of the loudness for the wake words associated with each command by accessing to the columns loudness_mean and loudness_sd:\n\nfeat_w1$summary$loudness_mean\n\n[1] 0.2332632\n\nfeat_w2$summary$loudness_mean\n\n[1] 0.314038\n\nfeat_w1$summary$loudness_sd\n\n[1] 0.0969784\n\nfeat_w2$summary$loudness_sd\n\n[1] 0.1460193\n\n\nAs we can observe, there are differences in loudness between the two wake words. The wake word in the second command exhibits both a higher average loudness (0.31 sone) and a greater standard deviation in loudness (0.15 sone) compared to the first (M = 0.31 sone, SD = 0.15 sone).\n\n\nFrequency associated characteristics\nThe fundamental frequency (F0) is a key vocal feature in the frequency domain. Similar to the approach taken with amplitude, we will now extract the average and standard deviation of the fundamental frequency using the features previously obtained through the analyze() function:\n\nfeat_cmd1$summary$pitch_mean\n\n[1] 229.3688\n\nfeat_cmd2$summary$pitch_mean\n\n[1] 234.4607\n\nfeat_cmd1$summary$pitch_sd\n\n[1] 40.54858\n\nfeat_cmd2$summary$pitch_sd\n\n[1] 28.99324\n\n\nObserving how the second command has a slightly higher average fundamental frequency (M = 234.46 Hz) than the first command (M = 229.37 Hz), while also having a higher standard deviation of the fundamental frequency (SD command 1 = 40.55 Hz; SD command 2 = 28.99 Hz).\nTo get into a higher level of granularity, we can apply the same analysis only to the wake word “Alexa”, by extracting the average and standard deviation of the fundamental frequency for the two wake words by accessing the previously created feat_w1 and feat_w2 objects:\n\nfeat_w1$summary$pitch_mean\n\n[1] 171.4803\n\nfeat_w2$summary$pitch_mean\n\n[1] 238.2434\n\nfeat_w1$summary$pitch_sd\n\n[1] 13.13345\n\nfeat_w2$summary$pitch_sd\n\n[1] 53.01586\n\n\nIn this case, the previously observed differences become even more pronounced—the second wake word has a much higher average fundamental frequency (M = 238.24 Hz) and standard deviation (SD = 53.02 Hz) than the first (M = 171.48 Hz; SD = 13.13 Hz).\nIn addition to extracting details about the fundamental frequency, we can delve into information about the formants. In Figure 8, we already saw that the track of the formants presented slight differences between the two wakewords. Consequently, our next step involves further characterising these differences, by extracting both the average and standard deviation of the frequency for the first formant (F1) across various commands and wake words. This information is accessible through the f1_freq_mean and f1_freq_sd columns in the detailed data.frame:\n\nfeat_cmd1$summary$f1_freq_mean\n\n[1] 346.1914\n\nfeat_cmd2$summary$f1_freq_mean\n\n[1] 392.5255\n\nfeat_cmd1$summary$f1_freq_sd\n\n[1] 117.9901\n\nfeat_cmd2$summary$f1_freq_sd\n\n[1] 242.5611\n\n\n\nfeat_w1$summary$f1_freq_mean\n\n[1] 377.39\n\nfeat_w2$summary$f1_freq_mean\n\n[1] 427.1445\n\nfeat_w1$summary$f1_freq_sd\n\n[1] 65.50526\n\nfeat_w2$summary$f1_freq_sd\n\n[1] 92.08512\n\n\nAs we can observe, the second command exhibits a slightly higher average frequency for the first formant (M = 392.53Hz) compared to the first command (M = 346.19 Hz). However, it’s noteworthy that the standard deviation of the first formant is greater for the second command (SD = 242.56 Hz) than for the first command (SD = 117.99 Hz).\nUpon closer examination of the wake words, this relationship undergoes partial modification. The wake word associated with the second command displays both a higher average and standard deviation for the frequency of the first formant (M = 427.14 Hz; SD = 92.09 Hz) in comparison to the wake word linked to the first command (M = 377.39 Hz; SD = 92.09 Hz).\nSimilarly, we can extract that information for the second formant (F2), by accessing the f2_freq_mean and f2_freq_sd columns in the detailed data data.frame:\n\nfeat_cmd1$summary$f2_freq_mean \n\n[1] 785.8252\n\nfeat_cmd2$summary$f2_freq_mean \n\n[1] 879.1609\n\nfeat_cmd1$summary$f2_freq_sd \n\n[1] 433.0889\n\nfeat_cmd2$summary$f2_freq_sd\n\n[1] 492.0018\n\n\n\nfeat_w1$summary$f2_freq_mean \n\n[1] 728.2679\n\nfeat_w2$summary$f2_freq_mean \n\n[1] 849.607\n\nfeat_w1$summary$f2_freq_sd \n\n[1] 253.4525\n\nfeat_w2$summary$f2_freq_sd\n\n[1] 130.4056\n\n\nExamining these values, we observe that the first command has a higher average and standard deviation for the frequency of the second formant (M = 785.83 Hz; SD = 433.09 Hz) compared to the second command (M = 879.16 Hz, SD = 492 Hz).\nZooming in on the wake words, we note that the average frequency for the wake word associated with the second command (M = 849.61 Hz) is higher than that for the wake word associated with the first command (M = 728.27 Hz). However, it’s interesting to point out that the wake word linked to the first command exhibits a higher standard deviation of the frequency of the second formant (SD = 130.41 Hz) compared to the wake word associated with the second command (SD = 130.41 Hz).\n\n\nSpectral associated characteristics\nSpectral features of a soundwave capture disturbances within the sound. Features assessing the spectral qualities of a soundwave typically gauge the level of disturbance or periodicity in the sound. Two such features of disturbances are the Harmonics-to-Noise Ratio (HNR) and the Wiener entropy (entropy), both directly extractable through the analyze() function.\n\n\n\n\n\n\nHarmonics-to-Noise Ratio and Weiner Entropy\n\n\n\nThe Harmonics-to-Noise Ratio (HNR) measures the additive noise level in a voice signal. Lower HNR values signify a higher proportion of noise in comparison to the harmonic components, often associated with breathy or hoarse sounds. Consequently, the higher the HNR, the clearer the voice sounds.\nThe Wiener entropy, commonly referred to as spectral flatness, serves to measure the degree to which a sound exhibits characteristics of a pure tone rather than resembling noise. This quantification is achieved by analyzing the shape of the spectrum, which represents the distribution of energy across different frequencies in the signal.\nWhen the spectrum is flat, indicating a balanced energy distribution, the Wiener entropy value approaches 1.0, signifying white noise. Conversely, if the spectrum is spiky, with energy concentrated at specific frequencies, the Wiener entropy value tends towards 0, indicating a pure tone.\n\n\nTherefore, we proceed to extract the average entropy and HNR for the two commands:\n\nfeat_cmd1$summary$entropy_mean\n\n[1] 0.03265698\n\nfeat_cmd2$summary$entropy_mean\n\n[1] 0.03072856\n\n\n\nfeat_cmd1$summary$HNR_mean\n\n[1] 15.92719\n\nfeat_cmd2$summary$HNR_mean\n\n[1] 16.74486\n\n\nUpon examining the average entropy for the two commands, it becomes evident that their values are quite comparable. It’s noteworthy, however, that these values are relatively close to 0, indicating a resemblance more akin to a pure tone than to white noise.\nIn terms of the average HNR, both commands also possess similar values, having the second command with a slightly higher value (M = 16.74 dB) than the first (M = 15.93 dB). The high HNR value for both commands indicates a substantial dominance of harmonics to noise within the audio signals.\nIn addition to comparing the means of the entropy and HNR, we will also observe their standard deviation as in the previous cases:\n\nfeat_cmd1$summary$entropy_sd\n\n[1] 0.03373136\n\nfeat_cmd2$summary$entropy_sd\n\n[1] 0.02609803\n\n\n\nfeat_cmd1$summary$HNR_sd\n\n[1] 6.61309\n\nfeat_cmd2$summary$HNR_sd\n\n[1] 6.178095\n\n\nSeeing how for both commands, the standard deviation of the entropy and the HNR is fairly similar between the two commands.\nAs in the previous cases, we could also zoom in on the isolated wake word for each command and extract these statistics regarding the HNR and the entropy:\n\nfeat_w1$summary$entropy_mean\n\n[1] 0.02015783\n\nfeat_w1$summary$entropy_sd\n\n[1] 0.01381312\n\nfeat_w1$summary$HNR_mean\n\n[1] 13.37802\n\nfeat_w1$summary$HNR_sd\n\n[1] 4.217952\n\nfeat_w2$summary$entropy_mean\n\n[1] 0.0267721\n\nfeat_w2$summary$entropy_sd\n\n[1] 0.02322313\n\nfeat_w2$summary$HNR_mean\n\n[1] 11.21771\n\nfeat_w2$summary$HNR_sd\n\n[1] 3.512729\n\n\nAs in the case of the commands, we can see how the different values between the two wake words are quite similar. However, the values for the HNR are lower than for the command, implying that there’s a lower proportion of harmonics in relation to noise. Thus, the speaker when saying the command has slightly hoarser voice compared to the rest of the command. This hoarseness could be linked to a direct increase in vocal intensity when initiating speech.\n\n\nVocal features summary\nAfter extracting the various vocal features from the two commands, we present a well-organized table, Table 1, which encapsulates these characteristics for each command. Furthermore, we also include the vocal characteristics for the isolated wake words associated with both commands.\n\n\n\n\nTable 1: Vocal features for the two commands and isolated wake words\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand 1\nCommand 2\nWake word 1\nWake word 2\n\n\n\n\nTime domain\n\n\n\n\n\n\nDuration\n6.93s\n7.68 s\n0.63 s\n0.63 s\n\n\nPercentage of voiced frames\n52.17 %\n55.88 %\n58.33 %\n62.5 %\n\n\nPercentage of voiced frames, excluding wake word and subsequent break\n70.43 %\n63.01 %\n—\n—\n\n\nAmplitude domain\n\n\n\n\n\n\nAverage root mean square of the amplitude\n0.0015\n0.0014\n7^{-4}\n7^{-4}\n\n\nStandard deviation of the root mean square of the amplitude\n9^{-4}\n0.001\n3^{-4}\n5^{-4}\n\n\nAverage loudness\n0.48 sone\n0.49 sone\n0.23 sone\n0.31 sone\n\n\nStandard deviation of the loudness\n0.19 sone\n0.18 sone\n0.1 sone\n0.15 sone\n\n\nFrequency domain\n\n\n\n\n\n\nAverage fundamental frequency\n229.37 Hz\n234.46 Hz\n171.48 Hz\n238.24 Hz\n\n\nStandard deviation of the fundamental frequency\n40.55 Hz\n28.99 Hz\n13.13 Hz\n53.02 Hz\n\n\nAverage first formant (F1) frequency\n346.19 Hz\n392.53 Hz\n377.39 Hz\n427.14 Hz\n\n\nStandard deviation of the first formant (F1) frequency\n117.99 Hz\n242.56 Hz\n65.51 Hz\n92.09 Hz\n\n\nAverage second formant (F2) frequency\n785.83 Hz\n879.16 Hz\n728.27 Hz\n849.61 Hz\n\n\nStandard deviation of the second formant (F2) frequency\n433.09 Hz\n492 Hz\n253.45 Hz\n130.41 Hz\n\n\nSpectral domain\n\n\n\n\n\n\nAverage Wiener entropy\n0.03\n0.03\n0.02\n0.03\n\n\nStandard deviation of the Wiener entropy\n0.03\n0.03\n0.01\n0.02\n\n\nAverage Harmonics-to-Noise Ratio\n15.93 dB\n16.74 dB\n13.38 dB\n11.22 dB\n\n\nStandard deviation of the Harmonics-to-Noise Ratio\n6.61 dB\n6.18 dB\n4.22 dB\n3.51 dB\n\n\n\n\n\n\n\n\nWakeword\nLooking at Table 1 we can easily notice that the second command’s wake word is spoken more loudly, especially in the initial part of the word, as we previously observed in Figure 9. This illustrates how, in the second command’s wake word, the speaker raises their voice to say “Ale.” Additionally, the second command has a higher percentage of voiced frames, mainly because the speaker prolongs the letter “e”, as seen in Figure 9. Furthermore, we observe that the second command’s wake word has a higher average and standard deviation of the fundamental frequency. Specifically, this frequency rises during the first part of the command, corresponding to “Ale”, and then decreases again, as shown in Figure 7. This indicates that in the initial part of the word “Alexa”, the voice is higher, suggesting the speaker is raising their voice. Moreover, in the second command’s wake word, the average frequencies for the first two formants are also higher, indicating a more forceful delivery.\n\n\nFull command\nThe second command is lengthier than the first, this is primarily because when excluding the wake word and the voice break between the wake word and the rest of the command, there’s a higher percentage of unvoiced frames, i.e. lengthier voice breaks. By listening to the audio and also looking at Figure 4, we can observe how the speaker is emphasizing the individual words, hoping that in this way the device will understand the command, unlike in the first command. As, in the case of the wake words, we can also observe an average higher fundamental frequency for the second command’s wake word, indicating a higher voice. In addition, the frequencies for the first and second formant are also higher, indicating a more forceful delivery.\nThus, we could hear how the speaker was expressing higher frustration on the second command and by decomposing the command into several audio characteristics that we have visualized and quantified, we have gained a deeper understanding of what’s driving our perception of the user’s frustration while interacting with Alexa."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#summary",
    "href": "posts/2023/intro-to-voice-analytics/index.html#summary",
    "title": "Introductory voice analytics with R",
    "section": "Summary",
    "text": "Summary\nIn this analysis, we delved into a user’s interaction with Alexa. The user initially instructed Alexa to play “Something’s Cooking in My Kitchen” by Dana. Unfortunately, Alexa struggled to comprehend the command due to the user’s Scottish accent. Frustrated by this, the user repeated the command in a tone reflecting her annoyance.\nTo understand the impact of these different deliveries on the commands, we compared the two instances. The first command was delivered in a natural and composed manner, while the second was tinged with frustration. Our approach involved going through the whole voice analytics pipeline breaking down the speech into various characteristics and scrutinizing these aspects for disparities between the commands.\nBy dissecting the speech and quantifying different characteristics, we were able to gain a nuanced understanding of what changed between the two commands. This not only enhanced our perception of user frustration but also provided valuable insights into the nuanced elements contributing to the interaction.\nIt’s important to note that this analysis serves as a basic tutorial. We directly compared two commands from the same user with almost identical content (except for the addition of the word “my” on the second command). In more advanced voice analytics, we would analyze more than just two audios from a single speaker, employing more sophisticated statistical methods or even training machine learning models to predict outcomes of interest. Hence, this post serves as an introduction, offering fundamental concepts of voice analytics from a practical perspective."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html",
    "href": "posts/2023/voiceR-R-package/index.html",
    "title": "My first R package: voiceR",
    "section": "",
    "text": "The subtleties of our speech often unveil more about us than the mere words we utter. These subtleties can be quantified through a constellation of distinct vocal features. Together, these features offer a glimpse into an individual’s speech pattern, revealing a trove of information. Within each individual’s unique vocal features lies valuable insights into their personal traits, such as age and gender, as well as their current emotional state. Furthermore, they have been linked to broader evaluative outcomes, including perceptions of physical attractiveness and strength. In medical contexts, these features have proven diagnostic, aiding in studying speech pathologies like vocal loading and enabling the detection of conditions such as Parkinson’s disease. Additionally, they have been instrumental in predicting and monitoring the treatment of clinical depression.\nWhile technically oriented fields like computer science, with their cadre of adept researchers, swiftly embraced and expanded the realm of voice analytics—utilizing deep learning models to dynamically recognize discrete human emotions—less technically inclined disciplines recognized the potential of voice analytics but fell short in harnessing its vast capabilities. These disciplines acknowledged but did not fully exploit the power of voice analytics to describe, comprehend, and predict affective and cognitive aspects of human expression.\nTo bridge this gap and offer a practical interface for voice analytics, we have developed an R package aiming at making voice analytics more accessible: the voiceR package, which today has been published to CRAN."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#what-is-voicer",
    "href": "posts/2023/voiceR-R-package/index.html#what-is-voicer",
    "title": "My first R package: voiceR",
    "section": "What is voiceR?",
    "text": "What is voiceR?\nvoiceR is an R package specifically designed to streamline and automate voice analytics for social science research. This package simplifies the entire process, from data processing and extraction to analysis and reporting of voice recording data in the behavioral and social sciences. It provides an intuitive and user-friendly interface, including an interactive Shiny app, making it accessible for researchers. One of its key features is batch processing, enabling the simultaneous reading and analysis of multiple voice files. Moreover, voiceR automates the extraction of crucial vocal features, facilitating further in-depth analysis. Notably, it goes a step further by automatically generating APA-formatted reports tailored for typical between-group comparisons in experimental social science research. Figure 1 offers a video summary of voiceR’s key features.\n\n\n\nVideo\n\n\nFigure 1: Overview of the main voiceR features"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#voicer-prerequisites",
    "href": "posts/2023/voiceR-R-package/index.html#voicer-prerequisites",
    "title": "My first R package: voiceR",
    "section": "voiceR prerequisites",
    "text": "voiceR prerequisites\n\nInstalling voiceR\nGetting started with voiceR is a straightforward process. Begin by installing the package from CRAN using the following command:\n\ninstall.packages(\"voiceR\")\n\nOnce you’ve successfully installed the package, you’re ready to embark on your voice analytics journey using voiceR.\n\n\nRequired file name structure\nvoiceR relies on a specific file naming convention to ensure seamless processing of audio files. This convention is comprised of up to three components of metadata about the file, two of which are optional:\n\nID: A unique identifier for the speaker or recording.\nCondition (optional): The experimental condition or another grouping variable.\nDimension (optional): Additional survey or experiment information, such as additional conditions.\n\nThe different file name components should be separated by a non-alphanumeric character, such as an underscore (_).\nvoiceR extracts these components to provide additional information about the audios and enable comparisons between groups.\nOrder of the components is not important, as long as you identify the correct file name pattern structure. For example, the following file names are all valid:\n\n12345_happy_male.wav (ID_Condition_Dimension)\n123bcf.wav (ID)\nCovidPositive_Patient1.wav (Condition_ID)\n\n\nUsing the Null placeholder\nIf there are parts of the file name that are not any of the required components, you can use the Null placeholder to avoid them. For example, if you have additional information in the file name that does not belong to any of the categories that voiceR processes, you can use the Null placeholder to ignore that information.\nFor example, imagine you have a file named Audio_Participant345_Happy.wav. The Audio component of the file name is not required, so you could define the following pattern: Null_ID_Condition. This file name would still be valid, and voiceR would ignore the first component given that we used the Null placeholder.\nFigure 2 demonstrates how the voiceR package uses the name pattern structure to identify the different components.\n\n\n\n\n\n\nFigure 2: Examples of File Name Patterns"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#comprehensive-functionality",
    "href": "posts/2023/voiceR-R-package/index.html#comprehensive-functionality",
    "title": "My first R package: voiceR",
    "section": "Comprehensive functionality",
    "text": "Comprehensive functionality\nvoiceR offers a suite of functions designed to simplify the voice analytics process. These functions cover reading and preprocessing audio files, automatic feature extraction, visualization of results, and even automatic report generation.\n\nReading multiple audio files\nThe initial step in the audio analytics process involves reading designated audio files. The readAudio() function in the voiceR package achieves this seamlessly, systematically processing all audio files in a specified directory and its subdirectories if specified. Users can customize this function by providing a file path (path) and an optional character vector to filter for specific patterns (filter). Upon execution, this function efficiently imports audio files into R, generating a comprehensive list of Wave objects, each representing an imported audio file.\n\n\nPreprocessing multiple audio files\nFollowing successful import, preprocessing becomes imperative. The preprocess() function in voiceR automates this process by normalizing amplitude and eliminating background noise from a list of Wave objects (audioList). Two optional logical parameters, normalizeAmplitude and removeNoise, allow users to tailor the preprocessing scope. Default settings include both amplitude normalization and noise removal. While suitable for most scenarios, advanced users can integrate functions from other packages, such as tuneR’s extractWave(), for more intricate preprocessing. The output is a preprocessed list of Wave objects, which can be stored locally using the saveAudio() function.\n\n\nAutomatic feature extraction for multiple audio files\nThe pivotal autoExtract() function facilitates the extraction of vocal features from raw or preprocessed audio files. Operating in two modes, it can either automatically read and analyze audio files based on a specified path and optional patterns or analyze a pre-existing list of audio files in the R environment. The function produces a table containing key audio features for each analyzed file, such as duration, voice breaks percentage, amplitude envelope root mean square, average loudness, average pitch, pitch standard deviation, average entropy, and average Harmonics to Noise Ratio.\n\n\nVisualizing results\nGiven the wealth of information produced by autoExtract(), effective visualization is paramount. The voiceR package offers two specialized functions for this purpose:\n\nnormalityPlots(): Generates density plots for each audio feature, facilitating normality assessment through the Shapiro-Wilk test.\ncomparisonPlots(): Produces box plots, aiding in the comparison of audio features across different conditions or dimensions. These plots include relevant statistical tests based on data normality.\n\nThese visualization functions, seamlessly integrated into the voiceR package, enhance the interpretability of audio data, enriching the depth and breadth of analysis.\n\n\nAutomatic Report Generation\nDespite the automation provided by voiceR functions, thorough documentation of primary findings remains essential. The autoReport() function addresses this need by utilizing autoExtract() output to generate an HTML report. This report encapsulates key vocal features of the analyzed audio files, including density plots, box plots, and automatically generated APA-formatted text and tables, highlighting differences between conditions or dimensions."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#explore-the-voicer-shiny-app",
    "href": "posts/2023/voiceR-R-package/index.html#explore-the-voicer-shiny-app",
    "title": "My first R package: voiceR",
    "section": "Explore the voiceR Shiny App",
    "text": "Explore the voiceR Shiny App\nFor the ultimate user-friendly experience, voiceR offers the voiceRApp() function. By invoking this function, you can launch the voiceR Shiny app, simplifying the selection and subsequent analysis of multiple audio files. It provides a dynamic view of results and the ability to download a comprehensive report summarizing key findings. Figure 3 provides a snapshot of the voiceR app’s initial screen.\n\n\n\n\n\n\nFigure 3: voiceR shiny app"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#dive-deeper-into-voicer",
    "href": "posts/2023/voiceR-R-package/index.html#dive-deeper-into-voicer",
    "title": "My first R package: voiceR",
    "section": "Dive Deeper into voiceR",
    "text": "Dive Deeper into voiceR\nFor a detailed understanding of voiceR’s capabilities and functionalities, consult the package documentation here."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nOct 25, 2023\n\n\nMoney and the ascent of Bitcoin\n\n\n\n\nSep 12, 2023\n\n\nMy first R package: voiceR\n\n\n\n\nJan 7, 2023\n\n\nIntroductory voice analytics with R\n\n\n\n\nApr 7, 2022\n\n\nStatistics Foundations: Confidence intervals\n\n\n\n\nDec 28, 2021\n\n\nStatistics Foundations: Sampling error\n\n\n\n\nSep 25, 2021\n\n\nStatistics Foundations: Populations and samples\n\n\n\n\nMay 19, 2021\n\n\nWhy potential inflation could lead to a financial crisis?\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html",
    "title": "Money and the ascent of Bitcoin",
    "section": "",
    "text": "In recent years, Bitcoin has captured the attention of nearly everyone. Its popularity has surged, driven by both positive and negative events, to the extent that it’s now a rarity to encounter someone unfamiliar with the concept of Bitcoin. However, there remains a somewhat elusive question that many find challenging to address: Can Bitcoin truly be categorized as money? In this blog post, our primary objective is to offer a comprehensive response to this question.\nTo accomplish this, we embark on a journey to establish a foundational understanding of money: its intrinsic nature, significance, the evolutionary path a commodity must traverse to attain the status of money, the essential attributes it must possess, and an exploration of the various forms of money that have prevailed throughout history.\nSubsequently, we delve into a succinct portrayal of Bitcoin, culminating in a decisive exploration of whether it can legitimately lay claim to the title of “money.”"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#barter",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#barter",
    "title": "Money and the ascent of Bitcoin",
    "section": "Barter",
    "text": "Barter\nBarter is the simplest form of exchange; it refers to the transfer of a good or service for another good or service. For this reason, it is typically considered direct exchange since no third object partakes of the transaction.\nBarter requires cooperation between individuals and double coincidence of wants, i.e., that both parties have and are willing to exchange the good or service that the other party desires for the good or service that the other party possesses. Therefore, this form of exchange involves high transaction costs due to the opportunity cost incurred in finding an individual with whom to make the barter."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#indirect-exchange-and-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#indirect-exchange-and-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Indirect exchange and money",
    "text": "Indirect exchange and money\nThese high transaction costs involved in the bartering process led to the emergence and prevalence of indirect exchange, i.e., a type of exchange in which a good or service is exchanged for a more widely acceptable item, which can be subsequently used to exchange for the goods or services desired. Therefore, for indirect exchange to occur, acquired goods must be more marketable than those surrendered. As the greater the marketability of a good, the more it will facilitate the final objective: the acquisition of the desired good or service.\nIn this way, in indirect exchange systems, the most marketable goods became a media of exchange, i.e., widely accepted. At the same time, as these goods became more widely accepted they further increased their marketability, bolstering their position as a medium of exchange. And, in turn, displaced those goods with lower marketability as means of exchange. Thus, leading to an inevitable scenario in which only a single good was universally employed as a medium of exchange: money."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#functions-of-money-and-their-development",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#functions-of-money-and-their-development",
    "title": "Money and the ascent of Bitcoin",
    "section": "Functions of money and their development",
    "text": "Functions of money and their development\nTherefore, we can define money as a generally accepted medium of exchange. Nonetheless, in several definitions of money, two secondary functions are attributed to it:\n\nStore of value: It allows to transmit value through time and space.\nUnit of account: It permits the valuation of goods and services.\n\nNotwithstanding, for a good to become money, it is not necessary that it initially fulfills all the above functions. Indeed, goods are converted into money through a process by which they usually acquire some of these functions first, and then others are subsequently developed. In addition, the acquisition of new functions establishes synergies with the previous ones, reinforcing and consolidating their position.\nFor example, as the practice of using a good as a medium of exchange becomes widespread, people begin to hold it in preference to others, thus developing its function as a store of value and reinforcing its function as a medium of exchange. As a result, acceptability becomes more widespread leading economic agents to set prices using this good as a reference, thereby becoming a unit of account.\nOn the other hand, for a good whose value is relatively stable, there will be economic agents interested in buying it not to satisfy their most direct needs, but to maintain their future purchasing power. In this way, it will be accepted by a growing number of agents and, therefore, become a medium of exchange. And, thus, economic agents begin to treat it as a unit of account."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#properties-of-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#properties-of-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Properties of money",
    "text": "Properties of money\nNevertheless, for money to fulfill the above functions, it must meet various characteristic requirements:\n\nPortability: It must be possible to transport or accumulate a large amount of value in a small amount of space, thereby facilitating transferability and hoarding.\nDivisibility: Money should be divisible into different units to enable precise pricing and facilitate transactions.\nUniformity: It must be easy to identify units of money having the same value, enabling the counterparty receiving the money to promptly discern its value. Thus, facilitating its transferability.\nDurability: It must remain intact over time without physically degrading or disappearing, therefore favoring its hoarding."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#types-of-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#types-of-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Types of money",
    "text": "Types of money\n\n\n\n\n\n\nNote\n\n\n\nThis subsection is merely for informational purposes and is not relevant for the understanding of subsequent sections of the post. Readers who wish to omit it, may do so by clicking here.\n\n\nThroughout history, money has taken many forms. Although today fiat money is the norm, commodity money characterized much of earlier history.\n\nCommodity money\nCommodity money refers to real units of a specific commodity universally accepted as a counterpart for goods and services. Accordingly, commodity money has intrinsic value. Historically, a myriad of commodities has served at one time or another as a medium of exchange: animal skins, salt, barley, tea, gold, silver, tobacco, etc.\nAs economies became more complex, increasing the number of payments, commodity money became cumbersome. The quality of the metals was continually tested to ensure that they had not been tampered with or that they were not of a lower grade than assumed. On the other hand, agricultural products were relatively difficult to transport compared to metals because of their lower unit value. For this reason, two alternatives emerged that sought to solve these problems: coinage and representative money.\nCoinage was a revolutionary invention that changed people’s way of thought. Coinage seems to have first occurred in the Kingdom of Lydia around 600 BC when the first electrum coins were minted, a natural alloy of gold and silver. (recent findings suggest that coinage may have originated in China a few years earlier, near Guanzhuang in Henan province). Consequently, metallic coins are a type of commodity money, which is highly transportable and divisible. Moreover, minted coins contained a mark that guaranteed their weight and purity, i.e., their value, thus solving the uniformity problem that untreated metals faced.\n\n\nRepresentative money\nRepresentative money is money whose value does not derive from the value of the material it is made of, but from what it represents, since each monetary unit is supposed to represent a fixed quantity of something that has real value.\nSome scholars have suggested that this form of money pre-dates coinage. In the ancient empires of Babylon, Egypt, China, and India temples, and palaces were considered inviolable, the former due to religious reasons and the latter due to the heavy protection they possessed. Therefore, they became safe places to store precious goods. Depositors received a certificate attesting deposits, which was a claim to the deposited goods. These certificates have been associated with multiple objects which were used in international trade, such as glazed scarabs in Egypt and cylindrical seals in Babylon and India. For this reason, these certificates are believed to have been used as a means of payment. Furthermore, due to the implementation of the gold standard, representative money occupied a central role during the 20th century.\n\n\nFiat money\nFiat money refers to money that has no intrinsic value and does not represent anything of intrinsic value. Public trust in both the issuer and the money itself is what drives its value. Such trust can be attributed, in most cases, to the confidence in the future stability of money’s purchasing power.\nSome authors have defined state-issued fiat money more critically as credit reimbursable for the payment of future tax obligations. And, therefore, associating fiat money as a way of using a government’s liabilities as a store of value.\nIn 1971, following the end of the Bretton Woods agreement, we find the emergence of modern fiat money. Nevertheless, in the fifth century B.C in Carthage, we already find one of the earliest known forms of widespread use of fiat money. This money was a small piece of leather sealed by the state, which enveloped a mysterious substance that nobody knew its composition except the maker. Only by breaking the seal, its composition could be known. However, in the presence of this event, this money was considered worthless.\nRecent studies have speculated that the mysterious substance was, in fact, tin or a compound of copper and tin and that the wrapping of this compound was not leather, but parchment."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#a-concise-overview-of-how-bitcoin-works",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#a-concise-overview-of-how-bitcoin-works",
    "title": "Money and the ascent of Bitcoin",
    "section": "A concise overview of how Bitcoin works",
    "text": "A concise overview of how Bitcoin works\nEach time a transaction occurs, the network records the Bitcoin address of the receiver and sender together with the amount transferred. This information is entered into the end of a ledger, called the blockchain. The blockchain is updated about every 10 minutes, and it is sent to every full node (computers connected to the Bitcoin network that verify all of the rules of Bitcoin).\nEvery transaction is encrypted with public-key cryptography and is verified by miners, computers connected to the Bitcoin network that secure the blockchain. The main objective of the miners is to fix the transaction history and prevent transaction fraud. This is done by solving a computer-intensive process by which individuals involved are rewarded with newly minted Bitcoins.\nMoreover, rewards given to miners are not always the same, yet they decline geometrically, with a 50% reduction every 210,000 blocks. This pattern was established because it approximates the rate at which gold is extracted."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Is Bitcoin money?",
    "text": "Is Bitcoin money?\nBitcoin meets all the necessary characteristics required to fulfill the functions that we previously stated that money must accomplish. As a digital asset, it is extensively portable, being its transferability and accumulation easy. In addition, it is deeply divisible: one Bitcoin can be divided into 100 million units, commonly known as satoshis. Likewise, the digital nature of Bitcoins makes them uniform and durable.\nHowever, the fact that it meets the necessary characteristics to fulfill the functions of money does not imply that it fulfills them. Consequently, before we can say whether Bitcoin is money or not, we must first analyze whether it fulfills these functions: (1) generally accepted medium of exchange, (2) store of value, and (3) unit of account.\n\nGenerally accepted medium of exchange: As of today, Bitcoin is not a generalized medium of exchange. We cannot go to the bakery next to our house and buy bread with it, nor can we go to a car dealership and buy a car with it.\nStore of value: Bitcoin has historically had severe price volatility, which is not favoring its function as a store of value.\nUnit of account: The limited adoption of Bitcoin as a means of payment and its price volatility do not foster its use as a unit of account.\n\nThus, we can say that Bitcoin currently cannot be considered money. Notwithstanding this, given the attractive properties of Bitcoin, we might ask ourselves a slightly more complex question: is Bitcoin in the process of becoming money?"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-in-the-process-of-becoming-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-in-the-process-of-becoming-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Is Bitcoin in the process of becoming money?",
    "text": "Is Bitcoin in the process of becoming money?\nIn the beginning, Bitcoin had a highly volatile price, as it was a new, virtually unknown asset that very few people owned. Nevertheless, Bitcoin was an asset with quite appealing monetary properties, coupled with a decentralized scheme and a finite money supply.\nThese properties led more and more economic agents to believe that Bitcoin could become a future store of value and, thus, decided to acquire and hold Bitcoin. Likewise, the growing demand for Bitcoin led to an increase in its popularity, which drove more economic agents to reach this reasoning. This escalating demand not only bolstered Bitcoin’s popularity but also created a self-reinforcing cycle of adoption, occasionally disrupted by external factors.\nThis progression led to a notable reduction in Bitcoin’s downside volatility, as demonstrated in Figure 1, making it increasingly attractive as a potential store of value. Nevertheless, this trajectory was not a continuous one. Instead, it was interrupted at different points by several external shocks. For instance, on February 8, 2021, coinciding with a low downside volatility period, Tesla announced a $1.5 billion Bitcoin. This event elevated Bitcoin’s appeal, indicating to investors that it could be a lucrative investment, thereby driving up its demand and price.\nHowever, in 2022, a series of external negative shocks, such as the TerraUSD stablecoin crash and the FTX collapse, shook investor confidence in the crypto market. This resulted in a decline in the prices of numerous cryptocurrencies, including Bitcoin, increasing its downside variability. However, after this significant price drop, Bitcoin’s value stabilized, suggesting to investors that the impact of these prior external factors might have dissipated. This stabilization restored their confidence, leading to gradual, albeit progressive, price increases and, consequently, one of the lowest levels of downside volatility for Bitcoin.\nThis newfound stability motivated investors, prompting numerous major companies like Ferrari to announce their willingness to accept Bitcoin as a payment method. Additionally and primarily, steps taken by Blackrock to launch a Bitcoin ETF further intensified demand, subsequently driving up its price.\n\n\n\n\n\n\n\n\nFigure 1: Bitcoin downside risk\n\n\n\n\n\nConsequently, the adoption of Bitcoin as a store of value is becoming more and more widespread. Once a store of value is well established enough, i.e., many agents understand that this asset is a good store of value, they can start to demand it against the sale of their goods.\nDespite this, not many companies do offer their goods or services in exchange for Bitcoin. However, if the popularity and the trend towards increased Bitcoin price stability are not affected mid-term, an increasing number of agents will accept Bitcoin as a means of payment.\nFinally, if Bitcoin’s function as a medium of exchange were to develop, it would increase its popularity and at the same time solidify its position as a store of value. Enabling future economic agents to start accounting with Bitcoin, i.e., opening the possibility of development to the function of unit of account.\nTherefore, we cannot say that Bitcoin is in the process of becoming money, but we can say that Bitcoin is currently in the process of becoming a store of value. That said, whether such a function is widely recognized depends on the maintenance of the trend in which it is now present: further decrease in its downward volatility without giving up its current popularity. Moreover, the development of other functions as a generalized medium of exchange and unit of account is still a long way off and is conditional on the soundness of the development of the store of value function. In addition, even if at some point the store of value function is fully developed, the development of other functions will still remain highly uncertain.\nFigure 2 summarizes the process by which Bitcoin could obtain the functions of money and thus become money. Take into account that this figure is an abstraction and does not consider various factors that could influence this process. This includes the potential impact of external shocks, exemplified by the events of 2022, as well as the variable durations of each transition phase. Moreover, it’s essential to acknowledge the non-linear nature of this progression, where steps forward can be accompanied by steps backward, adding complexity to the overall monetization process.\n\n\n\n\n\n\nFigure 2: Bitcoin monetization process\n\n\n\nRecently, Taleb has argued that Bitcoin can never be a store of value, since its fundamental value is 0. In the next subsection we address this criticism."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#against-talebs-argument-of-bitcoins-impossibility-to-become-a-store-of-value",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#against-talebs-argument-of-bitcoins-impossibility-to-become-a-store-of-value",
    "title": "Money and the ascent of Bitcoin",
    "section": "Against Taleb’s argument of Bitcoin’s impossibility to become a store of value",
    "text": "Against Taleb’s argument of Bitcoin’s impossibility to become a store of value\nIn the summer of 2021, Nassim Taleb published a short article entitled Bitcoin, currencies, and fragility, in which one of his arguments is that the value of Bitcoin is exactly 0 and, therefore, Bitcoin cannot be a store of value.\nTo argue this, Taleb relies on the premise that the fundamental value of any asset is equal to the sum of the present value of its expected future cash flows together with the terminal value that the asset will have.\nTherefore, as Bitcoin does not generate cash flows, i.e., the mere fact of owning Bitcoin as such does not result in monetary payments, meaning that the value of Bitcoin only depends on its terminal value.\nAdditionally, according to Taleb, Bitcoin is a technology. Therefore, Bitcoin, like any other technology, will eventually be replaced by another. As a result, its terminal value will be 0. Consequently, Taleb argues that since its fundamental value is 0, Bitcoin will not become money.\nNevertheless, in this argument, Taleb avoids two important points: (1) humans are not completely rational, and (2) Bitcoin is in the process of becoming a store of value as we saw in the previous subsection. Taleb may be right, Bitcoin may not yet be a store of value as such. But, this does not imply that it cannot become one, as we have seen in the previous subsection.\nThe reason behind this is irrationality in the early stages of Bitcoin, at that time it could be valid to say that Bitcoin had a value of 0. Nevertheless, multiple economic agents were attracted by it, which, as we have seen in the previous section, led to the start of the development of Bitcoin’s store of value function. As a result, many economic agents already consider Bitcoin as a store of value, while others expect it to become one in the near future.\nSuch a fact is critical since assets that act as a store of value provide the holder with a service: the transfer of value in space and time. Consequently, as Bitcoin is in the process of developing its store-of-value function, this implies that the expected flows of Bitcoin are no longer zero, but the implicit value of this service. Therefore, Bitcoin’s fundamental value should be greater than 0.\nTherefore, in the case of Bitcoin, we face an instance in which a collective irrationality has endowed this asset with a value that a priori it should not have. Nevertheless, as part of this process, the store of value property has begun to develop, which justifies that this asset has value, and, at the same time, this value allows it to act as a store of value."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html",
    "href": "posts/2021/statistics-foundations-sample-error/index.html",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "title": "Statistics Foundations: Sampling error",
    "section": "Examining Sampling Errors in Data Summarization: The Case of the Mean",
    "text": "Examining Sampling Errors in Data Summarization: The Case of the Mean\nAt the core of statistical analysis lies the foundational task of data summarization—a process that condenses data into a concise and understandable format. This fundamental procedure yields a clear and easily comprehensible overview of the data, facilitating a straightforward grasp of its essential characteristics.\nAmong these essential characteristics, two prominently stand out: central tendency and variability. Central tendency relates to the value around which the different data points cluster around. Conversely, variability quantifies the extent to which data points deviate from this central point, providing insights into the dispersion or spread of data relative to its central location. For instance, the mean serves as an example of a central tendency measure, while the standard deviation exemplifies a variability measure.\nTo visually illustrate the impact of sampling error, we will once again utilize the Kaggle dataset used in our previous post. This dataset contains information on 2,000 supermarket customers, including their age, annual income, and education level. For the purposes of this analysis, we will assume that this dataset represents our entire customer population.\nLet’s envision a scenario: our objective is to rapidly glean insights into the annual income of our customers. One straightforward strategy to achieve this is by computing the mean income, which furnishes us with a succinct metric representing the central tendency around which the majority of our customers’ annual incomes gravitate. In this endeavor, we observe that the mean annual income stands at a noteworthy $120,950, serving as a prominent reference point around which the annual incomes of our customers tend to concentrate.\n\n\n\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\n\nSampling 40 customers and calculating their annual income mean\nIn this hypothetical case, we possess information about the sample. Consequently, we can obtain information about our population without any error by directly observing it. Therefore, now we know that our population has an average annual income of $120,950. However, in real-life scenarios, and as previously said, obtaining data for the whole population may be unfeasible or even not possible. For this reason, we will assume that we extract a random sample of 40 customers and compute the mean annual income from this sample.\n\n\n\n\n\n\n\n\nFigure 2: Customer annual income distribution for our sample with 40 observations (thousands, $)\n\n\n\n\n\nAs observed, in Figure 2, the mean value within this sample diverges from that of the broader population. Specifically, the mean for this sample stands at $137,320, contrasting with the population mean of $120,950. This difference amounts to $16,370, and it encapsulates what we commonly refer to as “error.” Notably, in this instance, we possess knowledge about the population, allowing us to discern this difference.\nFor this reason, the terminology we use to describe the metrics summarizing the data characteristics varies depending on whether they are computed within the population or a sample. In the former case, they are referred to as parameters, whereas in the latter, they are known as statistics. In statistical notation, parameters are typically denoted by Greek letters, such as \\(\\sigma\\) for the standard deviation or \\(\\mu\\) for the mean, while statistics are denoted by Latin letters, such as \\(m\\) for the mean and \\(s\\) for the standard deviation.\n\n\nRandomness and sampling: Extracting several means of 40 observations\nMoreover, it’s crucial to note that this sample was derived through a process of random selection. In other words, we randomly picked 40 customers from our population, ensuring that each customer had an equal likelihood of being included. This randomness implies that if we were to generate another sample of 40 customers, it would be improbable for this new sample to mirror the exact composition of the previous one or yield the same mean.\n?@fig-anim-different-40-sample-distributions illustrates the annual income distribution of various samples, each consisting of 40 cutomers randomly selected from our initial population (including the sample we previously examined). It becomes evident that the distribution undergoes fluctuations across these diverse samples. Consequently, this variability gives rise to a spectrum of computed means, ranging from as low as $106,600 to as high as $137,320.\n\n\nNULL\n\n\n\n\nDigging deeper: Exploring mean customer annual income with 10,000 different 40 samples\nTo deepen our comprehension of the variance in computed means, we embark on a more extensive analysis by replicating the previous procedure but on a much larger scale: generating precisely 10,000 samples, each composed of 40 individuals. For every one of these 10,000 samples, we compute the mean annual income. The resulting distribution of these 10,000 means, each originating from distinct samples of 40 individuals randomly selected from our complete population, is visually represented in Figure 3 through a histogram.\n\n\n\n\n\n\n\n\nFigure 3: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\n\nFigure 3 reveals significant insights. Notably, there is a substantial variation in the computed average annual incomes across the various samples, spanning an extensive spectrum from $100,684 to $145,543. This disparity translates into an error range spanning from -$20,266 to $24,593 in contrast with the population’s mean.\nNonetheless, an intriguing revelation emerges from this analysis. Despite the marked variability in sample means, the overall average of these mean annual incomes, drawn from distinct samples, precisely mirrors the population average. This observation means that the average incomes for the different samples consistently cluster around this point.\nMoreover, it is worth noting that the distribution of annual income means extracted from these various samples adheres to a bell-shaped pattern, commonly known as a normal distribution. This pattern signifies that as we move farther away from the population average, the number of observations gradually diminishes.\nTaken together, this implies that, in most instances, the mean annual income estimated from our sample tends to be closer rather than farther away from the population’s mean annual income. Nevertheless, it’s important to acknowledge that there are still situations where significant deviations from the sample mean can occur. The key concern here lies in the fact that if we lacked information about the population and solely possessed a sample with an annual income mean of $145,543, we might mistakenly conclude that, on average, our customers are wealthier than they actually are.\n\n\nIncreasing sample size and analyzing mean annual income distribution of 10,000 samples\nAs previously mentioned, small samples encounter challenges in capturing the subtleties present within the population. Consequently, the larger the sample size, the more effectively we can apprehend these nuances. To illustrate this, we investigate how the variance of computed means changes when we collect samples of 150 customers, as opposed to the previous samples of 40 customers.\nOnce again, we generate 10,000 samples, each containing 150 customers, and calculate the mean annual income for each of these samples. Subsequently, we visualize the distribution of these mean annual incomes for these larger samples by creating a histogram.\n?@fig-10000-150-samples-mean provides a visual comparison between the distributions of means computed using 10,000 samples, each with 40 observations, and another set of samples, each comprising 150 observations.\n\n\nNULL\n\n\n?@fig-10000-150-samples-mean provides insights akin to those observed with 40 observations: the distribution of means exhibits a bell-shaped pattern, with the average closely approximating the population mean. However, a significant distinction emerges: the range within which sample means deviate from the overall mean, equivalent to the population average, is notably narrower. In essence, the variability is considerably reduced, indicating that the margin for error when using samples of 150 observations is substantially smaller than that with samples of 40.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nIn the previous examples, we’ve observed that when we calculate means from various samples, these means tend to follow a particular pattern – a bell-shaped distribution known as a normal distribution. This is not just a coincidence; it’s a fundamental concept in statistics called the Central Limit Theorem.\nThe Central Limit Theorem tells us that, regardless of the original shape of the data distribution, when we repeatedly draw samples of sufficient size from that data and calculate their means, those sample means will follow a normal distribution. This is a powerful idea because it allows us to make certain assumptions and conduct statistical analyses even when we don’t know the shape of the population’s distribution.\n\n\n\nIn this scenario, the distribution of means spans from $109,374 to $132,064, resulting in an error range of -$11,576 to $11,114 relative to the population mean. This range is significantly tighter compared to the error range obtained from samples of 40, where the deviation ranged from -$20,266 to $24,593."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "title": "Statistics Foundations: Sampling error",
    "section": "Measuring the sampling error",
    "text": "Measuring the sampling error\nThrough the repetitive extraction of samples of consistent size from a given population, as demonstrated in our previous examples (with both 40 and 150-sized samples), we gain valuable insights into the potential magnitude of errors associated with samples of a particular size.\nAs we’ve witnessed, the average of multiple means calculated from samples of identical size closely aligns with, or is essentially identical to, the true mean of the population. Consequently, the spread or dispersion of these computed means from this point provides a measure of the magnitude of the sampling error. Put simply, the variability in the mean derived from multiple samples of equal size offers a quantifiable measure of the magnitude of the sampling error for samples of that particular size. This measure is commonly known as the standard error of the mean (which we will abbreviate as SEM).\nMathematically, we can express this as the standard error of the mean being equal to the standard deviation of the means of the different samples. Specifically, we prefer using the standard deviation rather than the variance because the former has the same units as the mean, while the latter has squared units. For instance, in the case of annual income, the units for variance would be in dollars squared (\\(\\$^2\\)), while for the standard error, it’s just in dollars ($). In other words:\n\\(\\sqrt{Var(\\bar{X})} = SEM\\)\nThis expression can be translated into the following form:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\nHere, \\(σ\\) represents the population standard deviation and \\(n\\) is the sample size. Establishing an inverse relationship between the standard error and the sample size: as the sample size increases, the standard error decreases. This principle aligns with our intuitive understanding, as seen in the previous post for the statistics foundations series, and as visually depicted in ?@fig-10000-150-samples-mean.\n\n\n\n\n\n\nStandard Error Derivation\n\n\n\n\n\nLet’s recall that the mean of any variable is equal to the sum of the values of each observation of that variable divided by the total number of observations (which equals our sample size): \\(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\). Therefore, we can rewrite the previous formula as follows:\n\\(\\sqrt{\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nWe also know that the variance of a random variable multiplied by a constant “a” is equal to the variance of that variable multiplied by the square of that constant, i.e., \\(Var(aR) = a^2Var(R)\\), where \\(a\\) is a constant, and \\(R\\) is a random variable. This means that:\n\\(\\sqrt{\\frac{1}{n^2}\\text{Var}\\left(\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nAdditionally, when dealing with a set of pairwise independent random variables (where the variability in one doesn’t depend on the others, as in our case), the variance of their sum is equal to the sum of their individual variances, i.e., \\(\\text{Var}[R_1 + R_2 + \\cdots + R_n]\\) \\(=\\) \\(\\text{Var}[R_1] + \\text{Var}[R_2] + \\cdots + \\text{Var}[R_n]\\), where \\(R_1, R_2,…, R_n\\) are pairwise independent random variables. This allows us to rewrite the formula for SEM as:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}Var(X_i)} = SEM\\)\nLet’s remember that our individual variables, denoted as \\(X_i\\), come from a population with variance equal to \\(\\sigma^2\\). So, our formula becomes:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}\\sigma^2} = SEM\\)\nSince we’re summing up \\(n\\) identical values, we can simplify further:\n\\(\\sqrt{\\frac{1}{n^{2}}n\\sigma^2} = SEM\\)\nUltimately, this can be further simplified to:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\n\n\n\nApplying these formulas, we can now calculate the Standard Error of the mean for sample sizes of 40 and 150. When we compute the standard deviation of the means obtained from multiple samples of 40 customers, we obtain a value of 5.86. Conversely, for samples of 150 customers, we obtained a value of 3.01, which is approximately two times smaller in terms of standard error.\nAlternatively, we can utilize the formula that links the standard error to the population’s standard deviation and the sample size, represented as \\(\\frac{\\sigma}{\\sqrt{n}}\\). Given our population’s standard deviation for annual income is 38.11, dividing this by \\(\\sqrt{40}\\) yields a value of 6.03 for a sample size of 40, while dividing it by \\(\\sqrt{150}\\) results in a value of 3.11 for a sample size of 150.\nHowever, it’s important to note that there are disparities between the results obtained from these two approaches. This is because the first formula relies on a finite number of samples (10,000 in this case), and to obtain equivalent values, the number of samples would need to tend towards infinity, meaning a significantly larger amount of samples.\n\nEstimating the standard error\nUp until now, we have seen that we can compute the standard error of the mean by taking multiple samples of the same size from the population, calculating their means, and extracting the variability of such means.\nYet, let’s face it—in the real world, resources are finite, and repeatedly plucking samples from the population to estimate the standard error can be an extravagant expenditure of these precious assets. Instead, it’s often a more judicious allocation of resources to channel our efforts into amassing a larger sample. As we’ve come to appreciate, a larger sample which better captures the nuances of the population, decreasing the sampling error.\nIn addition to the aforementioned method, we’ve also explored an alternative approach for calculating the standard error—one that bypasses the need to repeatedly extract multiple samples from the population. This alternative method involves dividing the population’s standard deviation by the square root of the sample size (\\(\\frac{\\sigma}{\\sqrt{n}}\\)). However, it’s essential to note that this formula hinges on having access to information about the entire population. This requirement underscores the very reason why we find ourselves seeking to compute the standard error in the first place—a challenge born out of the impracticality of obtaining data for the entire population, compelling us to work with samples and consequently introducing the sampling error.\nNonetheless, it’s crucial to remind ourselves that the core objective when working with a sample is to glean insights into the larger population and derive meaningful conclusions from it. Within this context, it’s reasonable to assume that the standard deviation we observe within our sample (\\(s\\)) can serve as a dependable proxy for the population’s standard deviation (\\(\\sigma\\)). This assumption empowers us to substitute the population standard deviation in the traditional formula (\\(\\frac{\\sigma}{\\sqrt{n}}\\)) with the sample standard deviation (\\(s\\)) derived from that very population:\n\\(SE \\approx \\frac{s}{\\sqrt{n}}\\)\nBy making this substitution, we arrive at an approximation for the standard error. It provides us with a measure of the potential error we may encounter when drawing conclusions from a sample, all without the need for complete information about the population."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#a-final-note",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#a-final-note",
    "title": "Statistics Foundations: Sampling error",
    "section": "A final note",
    "text": "A final note\nThroughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. Throughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. When we compute these statistics from a sample, the values we obtain can deviate from those of the population, consequently introducing an element of error into our analyses.\nHowever, it’s crucial to acknowledge that the formulas we’ve previously derived for calculating the standard error aren’t universally applicable to all statistics. These formulas have been derived with the mean as their reference point. Nevertheless, it’s worth noting that the fundamental concept behind deriving formulas for computing the standard error for other statistics remains consistent: It is based on the variability of a specific statistic obtained from various samples of the same size."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#summary",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#summary",
    "title": "Statistics Foundations: Sampling error",
    "section": "Summary",
    "text": "Summary\n\nSampling is necessary because obtaining data for the entire population is often impractical.\nThe use of samples introduces the challenge of sampling errors.\nSampling errors arise because small samples cannot capture all the nuances present in the population.\nThis leads to variations in common measures like the mean between the sample and the population.\nTo illustrate this, we simulated the extraction of 10,000 samples of the same size from our exemplary population and observed that:\n\nThe mean from different samples exhibits variability.\nDespite this variability, the average of mean values from various samples tends to closely align with the population average.\nA normal distribution pattern is evident in the distribution of annual income means from different samples, indicating that samples with means close to the population mean are more likely.\nLarger sample sizes reduce the standard error and yield more accurate estimates, as they better capture the population’s intricacies.\n\nWhen extracting multiple samples of the same size from a population and calculating their means, the average of these sample means corresponds to the population mean. This allows us to quantify the degree of error associated with that sample size by measuring their variability, i.e., how much they deviate from the population mean.\nStandard error of the mean (SEM) is mathematically linked to the population’s standard deviation and sample size.\nIn practice, we estimate the SEM using our sample’s standard deviation.\nIncreasing the sample size results in a smaller standard error.\nStandard error serves as a valuable tool for quantifying the precision of sample-based estimates and is essential for robust statistical analysis.\n\n\n\nCode\n## Reading the \"population\" and visualizing it\nlibrary(gganimate)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.25, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n  \n## Sampling 40 customers and calculating their annual income mean + visualizing their distribution\n\nset.seed(150)\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40), ]\n\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data_sample$Income)\n\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.15, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n   \n ## Extracting several 40 customers and calculating their annual income mean + visualizing their distribution\n \n customer_data_samples &lt;- customer_data_sample\ncustomer_data_samples$Average_Sample &lt;- \"Sample 1\"\n\nseeds &lt;- seq(300, 5500, length.out = 8)\n\ni &lt;- 2\nfor(seed in seeds) {\n    \n    set.seed(seed)\n    customer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40),]\n    row.names(customer_data_sample) &lt;- NULL\n    average_income &lt;- round(mean(customer_data_sample$Income), 2)\n    customer_data_sample$Average_Sample &lt;- paste0(\"Sample \", i)\n    customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n    i &lt;- i + 1\n}\n\n\ndata_income_average &lt;- customer_data_samples %&gt;%\n  summarise(meanIncome = mean(Income), .by = Average_Sample)\n\n\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Average_Sample)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = meanIncome * 1.25, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Average_Sample, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n## Extracting 10,000 samples of 40 observations and calculate their mean + visualize the distribution of the means\nset.seed(3)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 9, label = paste0(\"Mean: \", round(average_income, 2))) \n\n## Extracting 10,000 samples of 150 observations and calculate their mean + visualize the distribution of the means\n\nset.seed(1500)\n# Create an empty numeric vector of length 10000 named 'sample_means'\nsample_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 150 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 150)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample_means), 2)\nsample_means_40 &lt;- data.frame(Income = sample40_means)\nsample_means_40$Observations &lt;- \"Samples of 40 Observations\"\nsample_means_150 &lt;- data.frame(Income = sample_means)\nsample_means_150$Observations &lt;- \"Samples of 150 Observations\"\nsample_means &lt;- rbind(sample_means_40, sample_means_150)\n\n\ndata_income_average &lt;- sample_means %&gt;%\n  summarise(meanIncome = mean(Income), .by = Observations)\n\n\np &lt;- ggplot(sample_means, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = average_income * 1.08, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 10)\nanim"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html",
    "title": "Statistics Foundations: Populations and samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "title": "Statistics Foundations: Populations and samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "title": "Statistics Foundations: Populations and samples",
    "section": "The ideal of population and the reality of sampling",
    "text": "The ideal of population and the reality of sampling\nTo comprehensively explore and enhance our understanding of a given issue, it is ideal to possess data pertaining to all entities impacted by that issue. For instance, in our pursuit of gaining deeper insights into our customer base, the ideal scenario involves having access to data for every single one of our customers. In this comprehensive dataset, we would find detailed information regarding each customer’s age, income, purchasing history, and other relevant attributes. Such a dataset would empower us to attain a holistic understanding of our customer base’s profile. In statistical terms, when we allude to data encompassing all entities affected by the issue of interest, we are referring to the population. Naturally, this population is contingent upon the specific focus of interest. For instance, if our objective shifted from understanding the profile of our overall customer base to that of our Swiss customers specifically, our population would comprise solely our Swiss customer subset.\nNevertheless, acquiring data for every single entity influenced by the matter of interest may often prove to be impractical, primarily due to the substantial expenses associated with such an undertaking or its constantly changing nature. Therefore, in the field of statistics, we operate with subsets derived from this population. Ideally, these subsets are constructed in such a way that each entity within them has an equal probability of being selected. Consequently, the resultant subset, known as a sample, is smaller in scale and serves as a reliable and representative image of the broader population. The fundamental concept underlying this approach is that through the observation and analysis of this sample, we can draw meaningful conclusions about the entire population, a process known as inference.\n\nLimitations and potential bias in sampling\nNonetheless, utilizing samples entails operating with an imperfect representation of the population—a mere approximation that may deviate from the true population characteristics. Even when each individual possesses an equal probability of selection, the random nature of the process can result in the overrepresentation or underrepresentation of certain specific types or groups of entities within our sample, thereby potentially leading to skewed findings and conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "title": "Statistics Foundations: Populations and samples",
    "section": "Sampling in practice: Exploring our consumer base annual income",
    "text": "Sampling in practice: Exploring our consumer base annual income\nTo illustrate these concepts, let’s delve into an example using a dataset from Kaggle. This dataset comprises data about 2,000 supermarket customers, encompassing a range of characteristics such as age, annual income, and education level. For the sake of this demonstration, let’s envision that this dataset encapsulates information about every single one of our customers, effectively serving as a representation of our customer population. Now, let’s suppose our objective is to gain a more profound insight into the annual income distribution among our customers.\n\nAnnual income distribution for our population\nHence, we can promptly delve into the examination of the data distribution, which we can observe through a histogram, depicted in Figure 1. This analysis unveils that the distribution of customer incomes exhibits an approximate normality, featuring an average annual income of $120,950 with a moderate degree of variability (Standard Deviation = $38,110). This implies that a substantial proportion of customers have incomes that closely align with the mean value, while fewer customers fall within the income extremes. Additionally, it is worth noting a slight rightward skew in the plot, indicating a minority of individuals with substantially higher incomes compared to the majority.\n\n\n\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\n\n\n\n\n\n\nLog-normal distribution\n\n\n\n\n\nIn the earlier paragraph, we highlighted that the “distribution of customer incomes exhibits an approximate normality.” To be more precise, the distribution we are discussing is formally identified as the log-normal distribution. Although it visually resembles a normal distribution, featuring a slightly bell-shaped curve, the log-normal distribution distinguishes itself by having a lower bound at zero and a positively skewed nature, leading to a more elongated right tail. The name “log-normal” stems from the phenomenon that transforming the variable into its logarithm results in a normal distribution. Figure 2 visually presents the original distribution of the customer’s annual income on the left and, on the right, showcases the result of applying a logarithmic transformation to the variable. This illustration vividly demonstrates how the logarithm transformation effectively transforms the distribution into a normal distribution.\n\n\n\n\n\n\n\n\nFigure 2: Original (left) and logarithmically transformed (right) customer annual income distributions\n\n\n\n\n\n\n\n\n\n\n“Collecting” a small customer sample and exploring their annual income\nHaving briefly explored the distribution of our customers’ annual income population, we must acknowledge the practical challenge of obtaining data from the entire population. Therefore, we opt to acquire a representative sample, a feasible alternative. In this scenario, we decided to select an easily obtainable sample of 20 customers, each having an equal probability of being included in the sample. Following the acquisition of this sample, we analyze their annual income distribution, which can be observed in Figure 3, revealing several noteworthy disparities.\nFirstly, we observe that certain income brackets, present in the population data, remain absent in our sample. Additionally, we notice a higher proportion of high-income customers in comparison to the population. These disparities culminate in a higher average customer annual income (Mean = $137,320) and increased variability (Standard Deviation = $47,770) within the sample. Consequently, if we were to draw inferences based on this data, our conclusions would suggest that individuals in the sample exhibit a higher average income and greater income variability compared to the population, thus drawing into error.\n\n\n\n\n\n\n\n\nFigure 3: Customer annual income distribution for our sample with 20 observations (thousands, $)\n\n\n\n\n\nDespite these disparities, our sample offers us an initial glimpse into the broader income distribution of the entire population. While our sample may not perfectly mirror the population due to its size and inherent limitations, it serves as a foundational reference point for comprehending income patterns within the larger group. It grants us a preliminary understanding of the income ranges, tendencies, and variations we can anticipate when considering the overall population’s income distribution. However, it’s essential to acknowledge the presence of these disparities and recognize that they may impact the conclusions we can draw regarding the population.\n\n\nCapturing nuances better with increased sample size\nThese observed disparities are unsurprising, given the inherent limitations of capturing the intricacies of our data with a small sample of just 20 consumers. Consequently, we observe the absence of individuals in various income brackets and a skewed composition compared to our population (which, in this hypothetical case, we have knowledge of, but in practice, we might not).\nOne might naturally question whether increasing the sample size could enhance the richness of our sample and consequently enable us to better capture the nuances present in the population. This would ultimately result in a more faithful representation. To investigate this, we will generate samples of varying sizes, specifically seven additional samples consisting of 40, 80, 120, 150, 300, 500, and 1000 randomly selected consumers from our population. ?@fig-anim-sample-distributions visually illustrates, through histograms, how the distribution changes for each of these sample sizes, including the initially created one with 20 consumers.\n\n\nNULL\n\n\nIn this Figure, we can discern that larger sample sizes excel in capturing the subtleties inherent in the population’s data distribution. Notably, the distribution of bigger samples closely mirrors that of the population, with fewer missing income ranges and diminished disparities.\nWhen we work with larger samples, we effectively broaden our scope of observation, incorporating a more diverse range of data points. This expanded sample size minimizes the influence of random variation and offers a more robust representation of the population. In essence, larger samples provide a more comprehensive cross-section of the population, enhancing our ability to accurately capture the underlying patterns, variations, and nuances present in the data.\nNow, you might be wondering: what constitutes the ideal sample size? There is not a one-size-fits-all answer; instead, the sample size should strike a balance. It must be large enough to capture the nuances of the population while still being feasible to acquire within budget and time constraints. It is crucial to recognize that a sample will inherently exhibit differences from the population, a fundamental aspect of statistical analysis."
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#summary",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#summary",
    "title": "Statistics Foundations: Populations and samples",
    "section": "Summary",
    "text": "Summary\n\nStatistics empowers us to glean valuable insights from data, enriching our comprehension of issues of interest.\nIdeally, a complete understanding of any issue necessitates data from every entity involved, referred to as the population.\nPractical constraints often require us to work with smaller, representative subsets, known as samples, where each entity in the population has an equal chance of inclusion.\nSamples, while essential for making inferences about the population, have inherent limitations due to their size, as they can’t fully capture the population’s intricacies, introducing errors into our inferences.\nLarger samples excel at capturing population nuances, offering a more faithful representation of its characteristics.\n\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gganimate)\nlibrary(scales)\n\n# Read customer data from CSV file and adjust income values\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income / 1000\n\n# Calculate the average and standard deviation of the income for the entire population\naverage_income_population &lt;- mean(customer_data$Income)\nstd_deviation_population &lt;- sd(customer_data$Income)\n\n# Create and display a histogram for the entire population's income distribution\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, \n           label = paste0(\"Mean: \", round(average_income_population, 2), \"\\nSD: \", round(std_deviation_population, 2))) \n\n# Set a random seed for reproducibility\nset.seed(150)\n\n# Create a random sample of 20 observations from the population\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 20), ]\n\n# Calculate the average and standard deviation of the income for the sample\naverage_income_sample &lt;- mean(customer_data_sample$Income)\nstd_deviation_sample &lt;- sd(customer_data_sample$Income)\n\n# Create and display a histogram for the sample's income distribution\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data_sample$Income, na.rm = TRUE) * 0.95, y = 13, \n           label = paste0(\"Mean: \", round(average_income_sample, 2), \"\\nSD: \", round(std_deviation_sample, 2))) \n\n\n\n\n\n## Create samples of different sizes and create animation with their distribution\n\n# Create an empty data frame to store the samples and add a column for observations\ncustomer_data_samples &lt;- data.frame()\ncustomer_data_samples$Observations &lt;- character(0)\n\n# Define the sample sizes\nsample_sizes &lt;- c(20, 40, 80, 120, 150, 300, 500, 1000)\n\n# Set a random seed for reproducibility\nset.seed(350)\n\n# Loop through each sample size\nfor (sample_size in sample_sizes) {\n  # Create a random sample of the specified size\n  customer_data_sample &lt;- customer_data[sample(nrow(customer_data), sample_size), ]\n  \n  # Create a label for the observations indicating the sample size\n  customer_data_sample$Observations &lt;- paste0(sample_size, \" Observations\")\n  \n  # Append the sample to the data frame\n  customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n}\n\n# Add a label for the population\ncustomer_data$Observations &lt;- \"Population\"\n\n# Append the population data to the data frame\ncustomer_data_samples &lt;- rbind(customer_data_samples, customer_data)\n\n# Convert the \"Observations\" column to a factor with custom labels\ncustomer_data_samples$Observations &lt;- factor(\n  customer_data_samples$Observations,\n  levels = c(\"20 Observations\", \"40 Observations\", \"80 Observations\", \"120 Observations\", \"150 Observations\", \"300 Observations\", \"500 Observations\", \"1000 Observations\", \"Population\")\n)\n\n# Loop through the levels of the \"Observations\" factor and update labels\nfor (i in seq_along(levels(customer_data_samples$Observations))) {\n  sample_type &lt;- levels(customer_data_samples$Observations)[i]\n  subset &lt;- customer_data_samples[customer_data_samples$Observations == sample_type, \"Income\"]\n  average_income &lt;- round(mean(subset), 2)\n  std_deviation_income &lt;- round(sd(subset), 2)\n  \n  # Update labels with mean and standard deviation information\n  if (i &lt;= 8) {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      levels(customer_data_samples$Observations)[i],\n      \"Sample\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  } else {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      \"Population\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  }\n}\n\n#Create and animate the plot\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = percent_format()) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 150, duration = 15)\nanim"
  }
]