[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge\nI am a technophile and data enthusiast, passionate about harnessing the potential of data to analyze competitive landscapes and derive strategic market insights, which provide actionable guidance, ultimately empowering organizations to gain a competitive advantage. Equally important to me is the art of effectively communicating these insights, no matter how intricate they may be.\nI possess 7 years of academic research experience, during which I explored how to leverage state-of-the-art data-driven methods to gain a deeper understanding of consumers and markets. Throughout this period, I have also assumed leadership roles and actively contributed to several multifaceted industry consultancy endeavors, primarily in the finance and tech sectors, in which I helped organizations enhance their understanding of consumer purchasing behavior and the complex dynamics that shape their markets."
  },
  {
    "objectID": "index.html#fran",
    "href": "index.html#fran",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nOct 25, 2023\n\n\nMoney and the ascent of Bitcoin\n\n\n\n\nSep 12, 2023\n\n\nMy first R package: voiceR\n\n\n\n\nJan 7, 2023\n\n\nIntroductory Voice Analytics with R\n\n\n\n\nApr 7, 2022\n\n\nStatistics Foundations: Confidence intervals\n\n\n\n\nDec 28, 2021\n\n\nStatistics Foundations: Sampling error\n\n\n\n\nSep 25, 2021\n\n\nStatistics Foundations: Populations and Samples\n\n\n\n\nMay 19, 2021\n\n\nWhy potential inflation could lead to a financial crisis?\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html",
    "title": "Money and the ascent of Bitcoin",
    "section": "",
    "text": "In recent years, Bitcoin has captured the attention of nearly everyone. Its popularity has surged, driven by both positive and negative events, to the extent that it’s now a rarity to encounter someone unfamiliar with the concept of Bitcoin. However, there remains a somewhat elusive question that many find challenging to address: Can Bitcoin truly be categorized as money? In this blog post, our primary objective is to offer a comprehensive response to this question.\nTo accomplish this, we embark on a journey to establish a foundational understanding of money: its intrinsic nature, significance, the evolutionary path a commodity must traverse to attain the status of money, the essential attributes it must possess, and an exploration of the various forms of money that have prevailed throughout history.\nSubsequently, we delve into a succinct portrayal of Bitcoin, culminating in a decisive exploration of whether it can legitimately lay claim to the title of “money.”"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#barter",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#barter",
    "title": "Money and the ascent of Bitcoin",
    "section": "Barter",
    "text": "Barter\nBarter is the simplest form of exchange; it refers to the transfer of a good or service for another good or service. For this reason, it is typically considered direct exchange since no third object partakes of the transaction.\nBarter requires cooperation between individuals and double coincidence of wants, i.e., that both parties have and are willing to exchange the good or service that the other party desires for the good or service that the other party possesses. Therefore, this form of exchange involves high transaction costs due to the opportunity cost incurred in finding an individual with whom to make the barter."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#indirect-exchange-and-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#indirect-exchange-and-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Indirect Exchange and money",
    "text": "Indirect Exchange and money\nThese high transaction costs involved in the bartering process led to the emergence and prevalence of indirect exchange, i.e., a type of exchange in which a good or service is exchanged for a more widely acceptable item, which can be subsequently used to exchange for the goods or services desired. Therefore, for indirect exchange to occur, acquired goods must be more marketable than those surrendered. As the greater the marketability of a good, the more it will facilitate the final objective: the acquisition of the desired good or service.\nIn this way, in indirect exchange systems, the most marketable goods became a media of exchange, i.e., widely accepted. At the same time, as these goods became more widely accepted they further increased their marketability, bolstering their position as a medium of exchange. And, in turn, displaced those goods with lower marketability as means of exchange. Thus, leading to an inevitable scenario in which only a single good was universally employed as a medium of exchange: money."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#functions-of-money-and-their-development",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#functions-of-money-and-their-development",
    "title": "Money and the ascent of Bitcoin",
    "section": "Functions of money and their development",
    "text": "Functions of money and their development\nTherefore, we can define money as a generally accepted medium of exchange. Nonetheless, in several definitions of money, two secondary functions are attributed to it:\n\nStore of value: It allows to transmit value through time and space.\nUnit of account: It permits the valuation of goods and services.\n\nNotwithstanding, for a good to become money, it is not necessary that it initially fulfills all the above functions. Indeed, goods are converted into money through a process by which they usually acquire some of these functions first, and then others are subsequently developed. In addition, the acquisition of new functions establishes synergies with the previous ones, reinforcing and consolidating their position.\nFor example, as the practice of using a good as a medium of exchange becomes widespread, people begin to hold it in preference to others, thus developing its function as a store of value and reinforcing its function as a medium of exchange. As a result, acceptability becomes more widespread leading economic agents to set prices using this good as a reference, thereby becoming a unit of account.\nOn the other hand, for a good whose value is relatively stable, there will be economic agents interested in buying it not to satisfy their most direct needs, but to maintain their future purchasing power. In this way, it will be accepted by a growing number of agents and, therefore, become a medium of exchange. And, thus, economic agents begin to treat it as a unit of account."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#properties-of-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#properties-of-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Properties of money",
    "text": "Properties of money\nNevertheless, for money to fulfill the above functions, it must meet various characteristic requirements:\n\nPortability: It must be possible to transport or accumulate a large amount of value in a small amount of space, thereby facilitating transferability and hoarding.\nDivisibility: Money should be divisible into different units to enable precise pricing and facilitate transactions.\nUniformity: It must be easy to identify units of money having the same value, enabling the counterparty receiving the money to promptly discern its value. Thus, facilitating its transferability.\nDurability: It must remain intact over time without physically degrading or disappearing, therefore favoring its hoarding."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#types-of-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#types-of-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Types of Money",
    "text": "Types of Money\n\n\n\n\n\n\nNote\n\n\n\nThis subsection is merely for informational purposes and is not relevant to the understanding of the later sections of the post. Readers who wish to omit it may do so by clicking here.\n\n\nThroughout history, money has taken many forms. Although today fiat money is the norm, commodity money characterized much of earlier history.\n\nCommodity money\nCommodity money refers to real units of a specific commodity universally accepted as a counterpart for goods and services. Accordingly, commodity money has intrinsic value. Historically, a myriad of commodities has served at one time or another as a medium of exchange: animal skins, salt, barley, tea, gold, silver, tobacco, etc.\nAs economies became more complex, increasing the number of payments, commodity money became cumbersome. The quality of the metals was continually tested to ensure that they had not been tampered with or that they were not of a lower grade than assumed. On the other hand, agricultural products were relatively difficult to transport compared to metals because of their lower unit value. For this reason, two alternatives emerged that sought to solve these problems: coinage and representative money.\nCoinage was a revolutionary invention that changed people’s way of thought. Coinage seems to have first occurred in the Kingdom of Lydia around 600 BC when the first electrum coins were minted, a natural alloy of gold and silver. (recent findings suggest that coinage may have originated in China a few years earlier, near Guanzhuang in Henan province). Consequently, metallic coins are a type of commodity money, which is highly transportable and divisible. Moreover, minted coins contained a mark that guaranteed their weight and purity, i.e., their value, thus solving the uniformity problem that untreated metals faced.\n\n\nRepresentative money\nRepresentative money is money whose value does not derive from the value of the material it is made of, but from what it represents, since each monetary unit is supposed to represent a fixed quantity of something that has real value.\nSome scholars have suggested that this form of money pre-dates coinage. In the ancient empires of Babylon, Egypt, China, and India temples, and palaces were considered inviolable, the former due to religious reasons and the latter due to the heavy protection they possessed. Therefore, they became safe places to store precious goods. Depositors received a certificate attesting deposits, which was a claim to the deposited goods. These certificates have been associated with multiple objects which were used in international trade, such as glazed scarabs in Egypt and cylindrical seals in Babylon and India. For this reason, these certificates are believed to have been used as a means of payment. Furthermore, due to the implementation of the gold standard, representative money occupied a central role during the 20th century.\n\n\nFiat money\nFiat money refers to money that has no intrinsic value and does not represent anything of intrinsic value. Public trust in both the issuer and the money itself is what drives its value. Such trust can be attributed, in most cases, to the confidence in the future stability of money’s purchasing power.\nSome authors have defined state-issued fiat money more critically as credit reimbursable for the payment of future tax obligations. And, therefore, associating fiat money as a way of using a government’s liabilities as a store of value.\nIn 1971, following the end of the Bretton Woods agreement, we find the emergence of modern fiat money. Nevertheless, in the fifth century B.C in Carthage, we already find one of the earliest known forms of widespread use of fiat money. This money was a small piece of leather sealed by the state, which enveloped a mysterious substance that nobody knew its composition except the maker. Only by breaking the seal, its composition could be known. However, in the presence of this event, this money was considered worthless.\nRecent studies have speculated that the mysterious substance was, in fact, tin or a compound of copper and tin and that the wrapping of this compound was not leather, but parchment."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#a-concise-overview-of-how-bitcoin-works",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#a-concise-overview-of-how-bitcoin-works",
    "title": "Money and the ascent of Bitcoin",
    "section": "A concise overview of how Bitcoin works",
    "text": "A concise overview of how Bitcoin works\nEach time a transaction occurs, the network records the Bitcoin address of the receiver and sender together with the amount transferred. This information is entered into the end of a ledger, called the blockchain. The blockchain is updated about every 10 minutes, and it is sent to every full node (computers connected to the Bitcoin network that verify all of the rules of Bitcoin).\nEvery transaction is encrypted with public-key cryptography and is verified by miners, computers connected to the Bitcoin network that secure the blockchain. The main objective of the miners is to fix the transaction history and prevent transaction fraud. This is done by solving a computer-intensive process by which individuals involved are rewarded with newly minted Bitcoins.\nMoreover, rewards given to miners are not always the same, yet they decline geometrically, with a 50% reduction every 210,000 blocks. This pattern was established because it approximates the rate at which gold is extracted."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Is Bitcoin money?",
    "text": "Is Bitcoin money?\nBitcoin meets all the necessary characteristics required to fulfill the functions that we previously stated that money must accomplish. As a digital asset, it is extensively portable, being its transferability and accumulation easy. In addition, it is deeply divisible: one Bitcoin can be divided into 100 million units, commonly known as satoshis. Likewise, the digital nature of Bitcoins makes them uniform and durable.\nHowever, the fact that it meets the necessary characteristics to fulfill the functions of money does not imply that it fulfills them. Consequently, before we can say whether Bitcoin is money or not, we must first analyze whether it fulfills these functions: (1) generally accepted medium of exchange, (2) store of value, and (3) unit of account.\n\nGenerally accepted medium of exchange: As of today, Bitcoin is not a generalized medium of exchange. We cannot go to the bakery next to our house and buy bread with it, nor can we go to a car dealership and buy a car with it.\nStore of value: Bitcoin has historically had severe price volatility, which is not favoring its function as a store of value.\nUnit of account: The limited adoption of Bitcoin as a means of payment and its price volatility do not foster its use as a unit of account.\n\nThus, we can say that Bitcoin currently cannot be considered money. Notwithstanding this, given the attractive properties of Bitcoin, we might ask ourselves a slightly more complex question: is Bitcoin in the process of becoming money?"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-in-the-process-of-becoming-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-in-the-process-of-becoming-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Is Bitcoin in the process of becoming money?",
    "text": "Is Bitcoin in the process of becoming money?\nIn the beginning, Bitcoin had a highly volatile price, as it was a new, virtually unknown asset that very few people owned. Nevertheless, Bitcoin was an asset with quite appealing monetary properties, coupled with a decentralized scheme and a finite money supply.\nThese properties led more and more economic agents to believe that Bitcoin could become a future store of value and, thus, decided to acquire and hold Bitcoin. Likewise, the growing demand for Bitcoin led to an increase in its popularity, which drove more economic agents to reach this reasoning. This escalating demand not only bolstered Bitcoin’s popularity but also created a self-reinforcing cycle of adoption, occasionally disrupted by external factors.\nThis progression led to a notable reduction in Bitcoin’s downside volatility, as demonstrated in Figure 1, making it increasingly attractive as a potential store of value. Nevertheless, this trajectory was not a continuous one. Instead, it was interrupted at different points by several external shocks. For instance, on February 8, 2021, coinciding with a low downside volatility period, Tesla announced a $1.5 billion Bitcoin. This event elevated Bitcoin’s appeal, indicating to investors that it could be a lucrative investment, thereby driving up its demand and price.\nHowever, in 2022, a series of external negative shocks, such as the TerraUSD stablecoin crash and the FTX collapse, shook investor confidence in the crypto market. This resulted in a decline in the prices of numerous cryptocurrencies, including Bitcoin, increasing its downside variability. However, after this significant price drop, Bitcoin’s value stabilized, suggesting to investors that the impact of these prior external factors might have dissipated. This stabilization restored their confidence, leading to gradual, albeit progressive, price increases and, consequently, one of the lowest levels of downside volatility for Bitcoin.\nThis newfound stability motivated investors, prompting numerous major companies like Ferrari to announce their willingness to accept Bitcoin as a payment method. Additionally and primarily, steps taken by Blackrock to launch a Bitcoin ETF further intensified demand, subsequently driving up its price.\n\n\n\n\n\nFigure 1: Bitcoin Downside Risk\n\n\n\n\nConsequently, the adoption of Bitcoin as a store of value is becoming more and more widespread. Once a store of value is well established enough, i.e., many agents understand that this asset is a good store of value, they can start to demand it against the sale of their goods.\nDespite this, not many companies do offer their goods or services in exchange for Bitcoin. However, if the popularity and the trend towards increased Bitcoin price stability are not affected mid-term, an increasing number of agents will accept Bitcoin as a means of payment.\nFinally, if Bitcoin’s function as a medium of exchange were to develop, it would increase its popularity and at the same time solidify its position as a store of value. Enabling future economic agents to start accounting with Bitcoin, i.e., opening the possibility of development to the function of unit of account.\nTherefore, we cannot say that Bitcoin is in the process of becoming money, but we can say that Bitcoin is currently in the process of becoming a store of value. That said, whether such a function is widely recognized depends on the maintenance of the trend in which it is now present: further decrease in its downward volatility without giving up its current popularity. Moreover, the development of other functions as a generalized medium of exchange and unit of account is still a long way off and is conditional on the soundness of the development of the store of value function. In addition, even if at some point the store of value function is fully developed, the development of other functions will still remain highly uncertain.\nFigure 2 summarizes the process by which Bitcoin could obtain the functions of money and thus become money. Take into account that this figure is an abstraction and does not consider various factors that could influence this process. This includes the potential impact of external shocks, exemplified by the events of 2022, as well as the variable durations of each transition phase. Moreover, it’s essential to acknowledge the non-linear nature of this progression, where steps forward can be accompanied by steps backward, adding complexity to the overall monetization process.\n\n\n\nFigure 2: Bitcoin Monetization Process\n\n\nRecently, Taleb has argued that Bitcoin can never be a store of value, since its fundamental value is 0. In the next subsection we address this criticism."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#against-talebs-argument-of-bitcoins-impossibility-to-become-a-store-of-value",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#against-talebs-argument-of-bitcoins-impossibility-to-become-a-store-of-value",
    "title": "Money and the ascent of Bitcoin",
    "section": "Against Taleb’s argument of Bitcoin’s impossibility to become a store of value",
    "text": "Against Taleb’s argument of Bitcoin’s impossibility to become a store of value\nIn the summer of 2021, Nassim Taleb published a short article entitled Bitcoin, currencies, and fragility, in which one of his arguments is that the value of Bitcoin is exactly 0 and, therefore, Bitcoin cannot be a store of value.\nTo argue this, Taleb relies on the premise that the fundamental value of any asset is equal to the sum of the present value of its expected future cash flows together with the terminal value that the asset will have.\nTherefore, as Bitcoin does not generate cash flows, i.e., the mere fact of owning Bitcoin as such does not result in monetary payments, meaning that the value of Bitcoin only depends on its terminal value.\nAdditionally, according to Taleb, Bitcoin is a technology. Therefore, Bitcoin, like any other technology, will eventually be replaced by another. As a result, its terminal value will be 0. Consequently, Taleb argues that since its fundamental value is 0, Bitcoin will not become money.\nNevertheless, in this argument, Taleb avoids two important points: (1) humans are not completely rational, and (2) Bitcoin is in the process of becoming a store of value as we saw in the previous subsection. Taleb may be right, Bitcoin may not yet be a store of value as such. But, this does not imply that it cannot become one, as we have seen in the previous subsection.\nThe reason behind this is irrationality in the early stages of Bitcoin, at that time it could be valid to say that Bitcoin had a value of 0. Nevertheless, multiple economic agents were attracted by it, which, as we have seen in the previous section, led to the start of the development of Bitcoin’s store of value function. As a result, many economic agents already consider Bitcoin as a store of value, while others expect it to become one in the near future.\nSuch a fact is critical since assets that act as a store of value provide the holder with a service: the transfer of value in space and time. Consequently, as Bitcoin is in the process of developing its store-of-value function, this implies that the expected flows of Bitcoin are no longer zero, but the implicit value of this service. Therefore, Bitcoin’s fundamental value should be greater than 0.\nTherefore, in the case of Bitcoin, we face an instance in which a collective irrationality has endowed this asset with a value that a priori it should not have. Nevertheless, as part of this process, the store of value property has begun to develop, which justifies that this asset has value, and, at the same time, this value allows it to act as a store of value."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "",
    "text": "In the previous post of the Statistics Foundations series, we explored the inherent errors associated with working with samples instead of the entire population. These errors stem from the limitations of samples in capturing the full spectrum of population nuances.\nWe delved into quantifying this error, commonly known as the standard error, by assessing the variability of a statistic derived from different samples of the same size drawn from the same population. To further explore this concept, we focused on the mean. We used the standard error definition to derive a formula that directly computes the standard error of the mean, dividing the population’s standard deviation by the square root of the sample size. However, we recognized that applying such a formula in practical scenarios can be unfeasible, as it hinges on possessing information about the entire population—information that is typically unavailable, underscoring the very reason why we employ samples in the first place.\nNevertheless, we uncovered a practical workaround by assuming that the standard deviation of our sample serves as an estimator for the population’s standard deviation. This substitution effectively transformed the formula into one that employs the sample’s standard deviation divided by the square root of its size, allowing us to estimate the standard error.\nSo far, our primary emphasis has been on understanding and quantifying the sampling error. But let’s take it a step further and connect this concept to something more practical and tangible. Imagine if we could use this knowledge to pinpoint a range within which our sample statistics are likely to fall and, by extension, where the true population parameters may lie. That’s where the notion of confidence intervals comes into the spotlight, a topic we’ll explore in this blog, with a primary focus on the mean.\nOnce more, we’ll be employing the same dataset from our previous posts, encompassing data from 2,000 supermarket customers, including details about their age, annual income, and educational level. As in our prior discussions, we’ll operate under the assumption that this dataset comprehensively captures information about all our customers, effectively representing our entire population."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#distributions",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#distributions",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Distributions",
    "text": "Distributions\nIn our previous discussions, we’ve touched upon the concept of distribution without providing a formal definition. In common parlance, this term is commonly used and readily understood, as demonstrated by phrases like “concentrated urban population distribution” or “disparities in wealth distribution within a country.” In these examples, “distribution” simply denotes how individuals or assets are spread across a specific area.\nIn statistics, the concept of distribution remains conceptually consistent. A variable’s distribution illustrates how the different values of that variable spread out and which are more prevalent. Our exploration of distributions has primarily involved the examination of a variable’s histogram—a visual representation that conveys the frequency of values within predefined intervals, commonly known as “bins”. This approach provides a direct and intuitive means of discerning the spread of values and identifying those that occur more and less frequently. For instance, let’s revisit the annual income distribution of our “population”.\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\nFigure 1 offers a visual representation of our population’s annual income distribution. This graphic, for instance, reveals that a substantial portion of our customers falls within the income range of $80,000 to $150,000, specifically encompassing 1,427 individuals, equivalent to 71.35% of our customer base.\nNow, consider a scenario where we randomly select a customer and, before checking any of their information, we make an educated guess about their income. In this situation, it’s reasonable to infer that their income is highly likely to fall within the range of $80,000 to $150,000, as the majority of our customers are concentrated in this income range.\nConsequently, distributions provide us with a framework to describe variables using the language of probability. This is why, in statistics, we often refer to them as probability distributions. To illustrate, in the previous example, we could state that the probability of a randomly chosen customer having an income between $80,000 and $150,000 is 71.35%. The connection between a variable’s values and their associated probabilities can be mathematically expressed through a function, which directly relates the variable’s values to their respective probabilities.\n\n\n\n\n\n\nDiscrete and Continuous Distributions\n\n\n\n\n\nWhen discussing probability distributions, it is crucial to distinguish between two fundamental types: continuous and discrete distributions.\nDiscrete distributions are characterized by elements that can only assume a finite number of values within a defined range. Examples of such distributions include the number of children in a family or the count of customers in a shop on a given day. These variables take on specific, countable values.\nOn the other hand, continuous distributions consist of elements that can take any value within a specified interval. While our everyday thinking and calculations often involve finite numbers, consider scenarios where precise measurements are vital, such as in pharmaceutical drug development, where even minuscule differences in weight can have significant implications. In this context, the weight of substances is treated as a continuous variable.\nIn continuous distributions, owing to their infinite range of potential values, it is not possible to precisely calculate the probability associated with a specific value. Conversely, in the case of discrete distributions, where a finite and countable set of values exists, we can accurately determine the probability associated with each individual value. In practical terms, this means that for continuous variables, we can only compute the probability of a variable falling within a certain range. While for discrete variables, we can calculate the probability of it assuming a specific value or falling within a particular range.\n\n\n\nMany probability distributions are frequently encountered and have earned distinctive names due to their importance. One such example is the uniform distribution, where every potential value of a variable is equally likely to occur. Another prominent distribution is the normal distribution, recognizable by its bell-shaped curve.\nWhile there are undeniably several other frequent probability distributions, for the purpose of this discussion, we will concentrate on the normal distribution. This emphasis is justified by its pivotal role in facilitating the translation of sample mean and sampling error, quantified by the standard error, into intervals within which we can confidently predict the likely range of the population mean. These intervals are commonly known as confidence intervals.\n\nThe Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is one of the most valuable continuous distributions, primarily because many statistics are normally distributed in their sampling distribution (as we saw in the previous post for the case of the mean).\nThe normal distribution is easily recognizable by its classic bell-shaped curve, as depicted in Figure 2. This curve resembles a perfectly symmetrical hill with a clear peak at its center, which represents the distribution’s mean. As you move away from this peak, the curve gradually slopes downward and then gently turns outward. This smooth descent and outward turn reveal a pattern of how data spreads—the likelihood of observing values becomes lower as you move further away from the mean, making values closer to the mean more probable.\nAdditionally, the symmetry of the normal distribution means that the probabilities of finding values above and below the mean are identical. In simpler terms, it implies that the chances of observing values on one side of the peak are the same as on the other side.\n\n\n\n\n\nFigure 2: Exemplary shape of a normal distribution\n\n\n\n\n\nMean and Standard Deviation: Shaping the Normal Distribution\nThe entire shape of a normal distribution can be effectively described using just two key parameters: the mean and the standard deviation.\n\n\n\n\n\n\nNotation\n\n\n\n\n\nGiven that the mean and the standard deviation effectively describe the entire shape of a normal distribution. As such, we typically employ the following notation to succinctly represent a normal distribution: \\(N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the variable’s mean and \\(\\sigma\\) is the variable’s standard de\nTherefore, we can denote that a random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as:\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\n\n\nThe mean, as mentioned earlier, represents the distribution’s center and the location of its peak. On the other hand, the standard deviation characterizes the curve’s shape. It indicates whether the curve is relatively flat or sharply peaked.\nFigure 3 offers an interactive visual representation showcasing how tweaking the mean and the standard deviation influences a normal distribution. When we adjust the mean, while keeping the standard deviation constant, we how the entire distribution shifts. An increase in the mean nudges it to the right, while a decrease causes it to veer to the left. Conversely, changing the standard deviation while maintaining the mean constant is like stretching or compressing the data. A smaller standard deviation suggests that most data points group closer to the mean, yielding a tall, slender curve. On the contrary, a larger standard deviation indicates that data points are more dispersed, resulting in a shorter, broader curve.\n\nviewof current_sd = Inputs.range(\n  [1, 5],\n  {value: 1, step: 1, label: \"Standard Deviation\"}\n)\n\nviewof current_mean = Inputs.range(\n  [0, 20],\n  {value: 0, step: 5, label: \"Mean\"}\n)\n\nfiltered = transpose(data).filter(function(normal_distribution) {\n  return current_sd === normal_distribution.sd_value &&\n    current_mean === normal_distribution.mean_value;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered,\n  Plot.binX(\n    {y: \"proportion\"},\n    {x: \"values\", \n     curve: \"natural\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n     interval : 0.75\n    }\n  )\n  \n    \n).plot({x: {domain: [-20, 40], grid: true}, y: {domain: [0, 0.3]}})\n\n\n\n\n\nFigure 3: Exemplary normal distributions with varying mean and standard deviation\n\n\n\n\n\nThe Normal Probability Density Function\nAs we’ve just observed and articulated, the core characteristics of a normal distribution revolve around its mean and standard deviation. Mathematically, the shape of a normal distribution can be portrayed through a functional relationship between the values of a normally distributed variable \\(X\\), characterized by a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\), and their probability density, known as the probability density function:\n\\(f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\)\nThe pivotal element within this formula is the exponential term \\(\\left({ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\right)\\). This term effectively communicates the rate at which the probability density diminishes as we distance ourselves from the mean (\\(\\mu\\))—as \\(x\\) moves farther from \\(\\mu\\), the lower the probability density. It’s essential to note that a larger standard deviation (\\(\\sigma\\)) results in a reduction of the magnitude of this expression, which, in turn, moderates the pace of the probability density decay.\n\n\n\n\n\n\nProbability Densities\n\n\n\n\n\nIt is essential to differentiate between probability densities and probabilities, as these two concepts fundamentally diverge. As we have underscored previously, computing the exact probability of a continuous variable taking a specific value is unfeasible, given that continuous variables can theoretically encompass an infinite range of values.\nTo illustrate this, consider measuring an individual’s height, which may be reported as 175cm. However, if we possessed an incredibly precise measuring instrument, it might record the height as 174.9999945 cm. In practice, we typically round such measurements to a more practical form, like 175 cm, instead of expressing them as infinite decimals.\nWhen we talk about probability densities, we essentially employ a similar principle — grouping values near one another. This grouping enables us to represent the likelihood of a value falling near a specific point, such as 175cm, without claiming it is precisely 175cm. It’s important to note that probability densities, in isolation, lack a direct interpretation as probabilities. However, they are ingeniously constructed to ensure that the area beneath the density curve always maintains its interpretability as genuine probabilities.\n\n\n\n\n\nThe 68-95-99.7 Rule\nSince the “probability” of specific values in a normal distribution is dictated by the mean and the standard deviation, we can directly associate the likelihood of specific events with these two parameters, particularly concerning how many standard deviations we deviate from the mean. This relationship gives rise to a widely recognized rule, commonly known as the 68-95-99.7 rule. According to this rule, there’s an approximate probability of 68% that a particular observation falls within one standard deviation from the mean (i.e., between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\)), roughly 95% within two standard deviations from the mean (i.e., between \\(\\mu - 2\\sigma\\) and \\(\\mu + 2\\sigma\\)), and approximately 99.7% within three standard deviations from the mean (i.e., between \\(\\mu - 3\\sigma\\) and \\(\\mu + 3\\sigma\\)). In this context, the values 1, 2, and 3, representing the number of standard deviations from the mean, are often referred to as critical values. These values help to define specific regions in the distribution.\nIn simpler terms, this rule tells us that for a normally distributed variable, roughly 68% of observations are within one standard deviation of the mean, about 95% are within two standard deviations, and approximately 99.7% are within three standard deviations. Figure 4 provides a visual representation of this rule.\n\n\n\n\n\nFigure 4: Visual representation of the 68-95-99.7 rule"
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Sample means and the normal distribution",
    "text": "Sample means and the normal distribution\nNow that we have a better understanding of probability distributions and how they help us assess where most data points are likely to cluster, as well as to assess the probability of an unknown data point falling within a specific range, let’s revisit our example involving supermarket customers. In the previous post, we uncovered an essential concept: sample means drawn from the same population and of the same size conform to a normal distribution with its center—the distribution’s mean—aligning with the population’s mean.\n\nVisualizing the Concept\nTo provide a visual representation of this concept, Figure 5 displays a histogram that provides an overview of the means obtained from 10,000 samples, with each sample containing 40 randomly selected customers from our population. In essence, this histogram visualizes the mean sampling distribution for samples of 40 observations drawn from our population.\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\nAs evident from the figure, these sample means display a characteristic bell-shaped distribution, i.e., following a normal distribution, with the distribution mean precisely aligning with the population mean, which, in this case, is $120,950. Additionally, the standard deviation of this distribution, which is equivalent to the standard error of the mean, is $5,862.\n\n\nApplying the 68-95-99.7 Rule\nBy applying the principles of the 68-95-99.7 rule, as explained earlier, we can infer that approximately 68% of samples drawn from the population will fall within one standard deviation from the mean, about 95% within two standard deviations, and nearly 99.7% within three. Notably, the standard deviation of sample means, computed from various samples of the same size and from the same population, corresponds to their standard error, whereas the mean aligns with the population mean. Consequently, we can rephrase this as follows: about 68% of sample means will be within one standard error of the population’s mean, approximately 95% within two standard errors, and nearly 99.7% within three standard errors of the population’s mean.\n\n\nPractical Example and Intuition\nNow, let’s apply this understanding in practice. Consider a scenario where we know the standard error of the mean (SEM = $5,862), but the population mean remains uncertain. However, we want to have an idea of the value the population mean could take. To do so, we select a sample of 40 customers from our population and calculate the mean of their annual income, which turns out to be $134,320.\nThis process of taking a sample and computing its mean is akin to randomly selecting a value from the distribution we discussed earlier, the mean sampling distribution. Therefore, it is highly likely that the value we obtain from this sample will be found within three standard errors of the population’s mean, as approximately 99.7% of sample means would be found within this range. This potential difference that we could have from the population mean is commonly known as the margin of error.\nConsequently, we can reverse the previous statement, affirming that the population mean is very likely to fall within three standard errors of the current sample mean. Therefore, we can say that our population’s annual income mean will be found with a 99.7% confidence within $116,734 (\\(134,320-3\\times5,862\\)) and $151,906 (\\(134,320+3\\times5,862\\)). We talk about 99.7% confidence because there’s a small chance (0.03%) that the population mean could be farther away than 3 standard errors. That’s why we refer to these intervals as confidence intervals, as they give us with some level of confidence, an interval in which the population mean will be found.\n\n\nVisualizing the intuition behind confidence intervals\nFigure 6 visually illustrates the intuition behind our reasoning. We assume that the mean obtained from our sample could belong to any mean sampling distribution, which center, i.e., distribution mean and, in turn, population mean, is found within this ±3SEM area, highlighted with a slightly grayer shade. Even the mean we obtained could be the center of the distribution, i.e., the population mean.\n\n\n\n\n\nFigure 6: Exemplary possible sample means distributions\n\n\n\n\n\n\nDirectly measuring errors\nIn our previous example, we gained an understanding of confidence intervals by examining the distribution of sample means. However, we can take this a step further by directly translating the distribution of sample means into an error distribution.\nError, in this context, refers to the difference from the population’s mean. To calculate it, we simply subtract the population mean from each individual sample. Non-zero values indicate a disparity between the sample mean and the population mean, with the magnitude of the value signifying the extent of this difference.\nWhen we subtract a consistent value from every observation of a variable, we effectively shift the variable’s mean by the same amount. Given that the mean of the sampling distribution aligns with the population mean, subtracting this value from the mean calculated for each sample effectively centers the distribution around 0.\nMoreover, as we’ve discussed before, altering the mean of a normal distribution corresponds to shifting the distribution horizontally, repositioning its central point where it is symmetrical. Therefore, this subtraction effectively repositions the distribution’s center to zero.\nLet’s revisit the previous scenario where we calculated the mean annual income from 10,000 different samples, each containing 40 customers drawn from our population. We visualized the distribution of the computed means in Figure 5. We now proceed to subtract the population mean from every computed individual sample mean, obtaining the errors incurred. We then plot the error distribution using a histogram, as depicted in Figure 7. As shown in the figure, it retains the same shape as Figure 5, yet it is now centered at zero, i.e. the point that signifies the absence of error.\n\n\n\n\n\nFigure 7: Error Distribution of the Means for 10,000 Samples of 40 Customers (thousands, $)\n\n\n\n\nSo, rather than focusing on the distribution of various sample means, we are now examining how errors, the differences between sample means and population means, are distributed. Notably, this error is expressed in the same units as our variable, i.e., in thousands of dollars.\nIdeally, we aim to obtain errors that remain invariant across different measurement scales, enabling us to compare error distributions across different variables, even when they use distinct units of measurement. Achieving this requires us to utilize a common measurement unit, with the standard deviation commonly being the preferred metric for this purpose.\nConsequently, we proceed by dividing each value of our variable by its standard deviation, effectively transforming measurement units into standard deviation units. This process of dividing each observation by the standard deviation is essentially equivalent to dividing the standard deviation by itself, resulting in a standard deviation of 1.\nKeep in mind that the standard deviation of the sampling distribution of the mean is identical to the standard error of the mean. Therefore, our process involves dividing the diverse errors by the standard error, resulting in a error measured in standard errors of the mean distribution with a mean of 0 and a standard deviation of 1, as depicted in Figure 8. As we can see, the distribution is still normal, we only shifted its mean to 0 and converted its standard deviation to 1.\n\n\n\n\n\nFigure 8: Sampling error of the mean measured in standard errors distribution for 10,000 Samples of 40 Customers\n\n\n\n\nIn summary, our process involved subtracting the mean of the sampling distribution of the mean, i.e., the population mean from each computed sample mean, essentially transforming the sample means into errors—difference between the computed mean and the population mean. Following this, we divided these errors by the standard deviation of the sampling distribution of the mean, i.e. the standard error of the mean, thereby representing them in a consistent unit of measurement. The outcome is a distribution with a mean of 0 and a standard deviation of 1, allowing us to compare these standardized errors across various variables effectively. Hence the whole process can be represented through the following formula:\n\\[\n\\frac{\\bar{x}-\\mu}{SE} = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nThis is what we call the standardized version of the sample mean."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Dealing with uncertainty: When standard error information is lacking",
    "text": "Dealing with uncertainty: When standard error information is lacking\nOur idealized assumption of knowing the exact standard error of the mean is often impractical in real-world scenarios. As we discussed in the previous post, calculating the standard error usually necessitates either drawing numerous samples of the same size from the same population, calculating the mean for each sample, and then determining the standard deviation of those sample means, or dividing the population’s standard deviation by the square root of our sample size. Unfortunately, both of these methods are frequently unfeasible. To overcome this challenge, we resort to estimating the standard error by dividing the sample’s standard deviation by the square root of the sample size, thereby introducing an additional layer of uncertainty into our calculations.This can be easily visualized by replacing the population’s standard deviation to the sample’s standard deviation in the previous formula:\n\\[  \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}} \\]\nAs previously shown, when we have exact knowledge of the standard error, the distribution of the standardized version of the sample mean follows a normal distribution. However, when we lack precise knowledge of the standard error and instead use an estimate, does the distribution of the standardized version of the sample mean still follow a normal distribution? To verify this, we proceed to visualize the distribution of the standardized version of the sample mean when we don’t know the standard error precisely and estimate it by dividing the sample’s standard deviation by the square root of the sample size. Given that the standard error depends on the sample size, we repeat this process for various sample sizes. For each size, we extract 100,000 samples and compute the standardized version of the sample mean with the estimated standard error. These resulting distributions are illustrated in Figure 9.\n\n\n\n\n\nFigure 9: Sampling error of the mean measured in estimated standard errors distribution for 100,000 Samples of varying number of Customers\n\n\n\n\nAs evident in Figure 9, these errors exhibit a distribution that closely resembles the normal distribution, particularly when dealing with larger sample sizes. However, for smaller sample sizes, the distribution exhibits broader tails compared to the typical normal distribution. In reality, this altered distribution is known as the Student’s t-distribution, commonly referred to as the t-distribution for simplicity.\n\nEmbracing the t-Distribution\nThe t-distribution is not a single function but rather a family of functions. Similar to the normal curve, each t-distribution is symmetric, with its mean positioned at the center. However, unlike the normal distribution, in the case of the t-distribution, the mean is always fixed at 0. This means that the t-distribution is centered around 0 by default, and its shape and spread are determined by a parameter known as the degrees of freedom. The concept of degrees of freedom, in a sense, varies from application to application, but in this domain we can understand it as the number of independent pieces of information to calculate a statistic, i.e. the mean for us. For the mean the degrees of freedom are equivalent to the number of observations minus one (\\(n-1\\)).\nThe intuition behind the appearance of t-distribution when using the estimated standard error arises from the added uncertainty because of the use of this estimation. It provides a more appropriate and conservative model for the variability of sample means when the standard error is unknown. Unlike the normal distribution, the t-distribution takes sample size into account, presenting wider tails that aptly accommodate the increased uncertainty and variability associated with estimating the standard error from a sample.\nFigure 10 illustrates how the t-distribution transforms with varying degrees of freedom.As degrees of freedom increase, the distribution’s tails gradually become narrower, nearing a state that closely resembles the normal distribution. This transformation is a consequence of larger sample sizes, which enable a more refined depiction of the underlying population. Consequently, it leads to reduced sampling errors and, consequently, enhances the precision of our estimations of the standard error, reducing uncertainty.\n\nviewof current_df = Inputs.range(\n  [1, 50],\n  {value: 1, step: 1, label: \"Degrees of freedom\"}\n)\n\n\n\nfiltered2 = transpose(data2).filter(function(df_distribution) {\n  return current_df === df_distribution.df;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered2,\n  { x: \"x\",\n    y: \"values\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n    }\n  ).plot({x: {domain: [-6, 6], grid: true}, y: {domain: [0, 0.45]}})\n\n\n\n\n\nFigure 10: Exemplary t distributions with varying degrees of freedom\n\n\n\nThe t-distribution, in contrast to the normal distribution, exhibits broader tails, making it unsuitable for applying the 68-95-99.7 rule we discussed earlier. With these wider tails, our expectations change: we can no longer anticipate approximately 68% of potential mean values falling within 1 standard error of the mean, 95% within 2, and 99.7% within 3; these proportions are now reduced.\nAs the characteristics of the t-distribution are contingent upon degrees of freedom, we must consider them when determining the number of standard errors from the mean required to encompass a specific proportion of values. These precise proportions will vary as degrees of freedom change. Nevertheless, as degrees of freedom increase, the distribution of standardized sample means approximates a normal distribution, allowing us to eventually employ the 68-95-99.7 rule.\nTable 1 provides the critical values for t-distributions across a range of degrees of freedom and confidence levels. Notably, as the sample size becomes sufficiently large, the critical values for the t-distribution closely mirror those of a (standardized) normal distribution. In fact, when dealing with an infinite number of degrees of freedom, the critical values for the t-distribution converge to those of a normal distribution. Consequently, for sufficiently large sample sizes, it’s entirely justified to work directly with a normal distribution due to its close approximation to the t-distribution in such cases.\n\n\nDisplay Table 1\n\n\n\nTable 1: T-distribution critical values\n\n\n\n\n\n\n\n\nDegrees of Freedom (df)\n68% Critical Value\n95% Critical Value\n99.7% Critical Value\n\n\n\n\n1\n1.819\n12.706\n212.205\n\n\n2\n1.312\n4.303\n18.216\n\n\n3\n1.189\n3.182\n8.891\n\n\n4\n1.134\n2.776\n6.435\n\n\n5\n1.104\n2.571\n5.376\n\n\n6\n1.084\n2.447\n4.800\n\n\n7\n1.070\n2.365\n4.442\n\n\n8\n1.060\n2.306\n4.199\n\n\n9\n1.053\n2.262\n4.024\n\n\n10\n1.046\n2.228\n3.892\n\n\n20\n1.020\n2.086\n3.376\n\n\n30\n1.011\n2.042\n3.230\n\n\n40\n1.007\n2.021\n3.160\n\n\n50\n1.004\n2.009\n3.120\n\n\n60\n1.003\n2.000\n3.094\n\n\n70\n1.002\n1.994\n3.075\n\n\n80\n1.001\n1.990\n3.061\n\n\n90\n1.000\n1.987\n3.051\n\n\n100\n0.999\n1.984\n3.042\n\n\n150\n0.998\n1.976\n3.017\n\n\nInfinity\n0.994\n1.960\n2.968\n\n\n\n\n\n\n\nEstimating confidence intervals for our exemplary sample of 40 customers\nHaving established that estimating, rather than precisely knowing the standard error, introduces an additional layer of uncertainty, we must adapt our approach when calculating margins of error and confidence intervals. Instead of relying on the normal distribution, we turn to the t-distribution, a distribution whose shape varies with the sample size. For smaller samples, it features wider tails, transitioning towards a normal distribution as the sample size increases. These wider tails account for the added uncertainty, resulting in more conservative estimates.\nReturning to our example, where we examined a sample of 40 customers with a mean of $134,320 the first step is to estimate the standard deviation for this sample, which amounts to $70,439. Therefore, we estimate the standard error by dividing this standard deviation by the square root of 40, resulting in an estimated standard error of $11,137.38.\nFor a desired confidence level of 99.7%, we should use a critical value of 3.166. This value corresponds to the critical value for a confidence level of 99.7% and 39 degrees of freedom (40-1). Moreover, note that this critical value is larger than the value of 3, which we used when we knew the standard error and, thus, had a sampling distribution of the mean that followed a normal distribution. Consequently, we can calculate the margin of error by multiplying the critical value for the chosen confidence level by the estimated standard error, yielding a margin of error of $35,260.95 (\\(3.166 \\times \\$11,137.38\\) = \\(\\$35,260.95\\)). With 99.7% confidence, the population mean is estimated to fall between (\\(\\$134,320\\pm\\$35,260.95\\)) $99,059.05 and $169,581.\nThese confidence intervals, which are wider than the previous ones obtained when we knew the exact standard error, provide a range of $99,059.05 to $169,581, as opposed to the narrower intervals of $116,734 to $151,906. This increase in width reflects the additional caution necessitated by the t-distribution’s wider tails and the inherent uncertainty associated with estimating the standard error.\n\n\n\n\n\n\n95% confidence as a standard\n\n\n\n\n\nIn the examples provided thus far, we’ve been working with a relatively high confidence level of 99.7%. While such a level of confidence is valuable in certain contexts, it’s worth highlighting that in everyday practical applications, a 95% confidence level is the more prevalent choice.\nA 95% confidence level offers a balanced compromise between precision and practicality. This means that we are willing to tolerate a 5% chance of not capturing the population mean within the defined intervals, in return for the benefits of having narrower confidence intervals."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#a-final-note",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#a-final-note",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "A final note",
    "text": "A final note\nIn the preceding sections, we saw that we can define confidence intervals by leveraging the shape of the sampling distribution. This sampling distribution can be converted into an error distribution measured in standard errors (or estimated standard errors). And from such distribution, we can find critical values that define the points within which the majority of errors will be found, in a way that we can calculate confidence intervals. Thus, given that such a distribution for other statistics follows a symmetrical as previously seen, we can extract some simple formula for the computation of the confidence interval of such statistics.\nA confidence interval consists of two limits, defining the lower and upper bounds of the interval, where the point estimate lies at the center. The confidence interval (CI) for an estimator of a parameter \\(\\theta\\) can be expressed in the following way:\n\\[CI_{\\theta} = \\hat{\\theta}\\pm MOE\\]\nmeaning that the upper confidence interval for a parameter \\(\\theta\\) is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)), i.e., the value obtained in the sample for that parameter, plus the margin of error (MOE). While the lower confidence interval is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)) minus the margin of error.\nThe margin of error corresponds to half the width of the interval and is given by:\n\\[MOE = \\Phi^{-1}_{1-\\frac{\\alpha}{2}} \\times \\hat{\\sigma}_{\\theta} \\]\nHere, \\(\\hat{\\sigma}_{\\theta}\\) represents the estimated standard error of \\(\\theta\\) and \\(\\Phi^{-1}_{1-\\frac{\\alpha}{2}}\\) denotes the critical value. More specifically it refers to the inverse quantile function, which includes a confidence level equal to \\(1-\\alpha\\), i.e., a function that tells us the point in which that proportion of the distribution is found.\nWhile the exact sampling distribution for some statistics can be unknown, as well as their standard error, the central limit theorem often provides a justification for using a normal approximation. For this reason, for most statistics, we tend to assume that their sampling distribution follows a normal distribution (given that the sample is large enough. An example is the t-distribution which for large enough samples approximates the normal distribution). This approximation tends to be accurate, but even in cases in which it’s not that accurate, it’s better to have an approximate confidence interval than a solely-point estimate."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#summary",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#summary",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Summary",
    "text": "Summary\n\nSampling error arises from using samples rather than the entire population for analysis, as samples may not capture all population nuances.\nThe sampling error can be quantified using the standard error, which measures the variability of a statistic calculated from different samples.\nThe standard error helps define a range within which sample statistics are likely to fall, providing insights into potential population parameters.\nThe sampling distribution of the mean follows a normal distribution, with its mean equal to the population mean and standard deviation equal to the standard error.\nThe 68-95-99.7 rule provides a shorthand for understanding the distribution: approximately 68% of values are within 1 standard deviation from the mean, 95% within two standard deviations, and 99.7% within three standard deviations. These values defining specific regions are known as critical values.\nTranslating this to standard errors, 68% of values are about 1 standard error from the population mean, 95% about two standard errors, and 99.7% about three standard errors.\nA sample, with 95% confidence, is expected to be about 2 standard errors from the mean, and with 99.7% confidence, about 3 standard errors.\nThis argument can be reversed: with 95% confidence, a sample is about 2 standard errors from the population mean, and with 99.7% confidence, about 3 standard errors.\nThe margin of error, which creates a likely population range (confidence interval), is determined by multiplying the critical value by the standard error.\nThe confidence interval is obtained by adding the margin of error to the sample mean (upper interval) and subtracting it from the sample mean (lower interval).\nIn most cases, the standard error is unknown and needs to be estimated by dividing the sample’s standard deviation by the square root of its size.\nWhen estimating the standard error, the assumption that the sampling distribution of the mean follows a normal distribution no longer holds; it follows a Student t-distribution.\nThe Student t-distribution (t-distribution) varies based on degrees of freedom, resembling the normal distribution but having wider tails with smaller sample sizes (degrees of freedom).\nThe wider tails of the t-distribution result in larger magnitude critical values than the normal distribution, leading to wider confidence intervals.\n\n\n\nCode\n# Customer annual income distribution (thousands, $) ---------------\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\nstd_deviation_income &lt;- sd(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n    scale_x_continuous(breaks = round(seq(30, 320, by = 20))) +\n   annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, label = paste0(\"Mean: \", round(average_income, 2), \"\\nSD: \", round(std_deviation_income, 2))) \n\n# Create exemplary normality plot ---------------\n\nset.seed(150)\n\n#Create a normal distribution\nnormal_distribution &lt;- rnorm(30000)\nnormal_distribution &lt;- data.frame(values = normal_distribution)\n\n\n\n\nggplot(normal_distribution, aes(x = values)) +\n  geom_density(fill = \"steelblue\", color = \"steelblue\", alpha = 0.4, adjust = 2) + \n  labs(\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) \n    \n# Visual representation of the 68-95-99.7 rule ---------------\n\nmu = 0\nsigma = 1\nx &lt;- seq(-5*sigma, 5*sigma, length.out = 1000)\ny &lt;- dnorm(x, mean = mu, sd = sigma)\ndata &lt;- data.frame(x, y)\n\nggplot(data.frame(data), aes(x)) + \n    geom_ribbon(data = subset(data, x &gt;= mu - 3 * sigma & x &lt;= mu + 3 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#c1f5ef\") +\n    geom_ribbon(data = subset(data, x &gt;= mu - 2 * sigma & x &lt;= mu + 2 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#90ebe1\") +\n    geom_ribbon(data = subset(data, x &gt;= mu - 1 * sigma & x &lt;= mu + 1 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#34d1bf\") +\n  geom_ribbon(data = subset(data, x &gt;= -0.01 & x &lt;= 0.01),\n                aes(ymax = y), ymin = 0, fill = \"black\") +\n    theme_minimal() +\n    geom_vline(xintercept = c(mu - 3 * sigma, mu - 2 * sigma, mu - 1 * sigma, mu + 1 * sigma, mu + 2 * sigma, mu + 3 * sigma), \n               linetype = \"dashed\", color = \"black\", alpha = 0.5) +\n    geom_segment(x = 0 - 1 * sigma +0.05, xend = 0 + 1 * sigma -0.05, y = 0.45, yend = 0.45, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n  annotate(\"text\", x = 0, y = 0.46, label = \"68%\") +\n  geom_segment(x = 0 - 2 * sigma +0.05, xend = 0 + 2 * sigma -0.05, y = 0.5, yend = 0.5, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n   annotate(\"text\", x = 0, y = 0.51, label = \"95%\") +\n  geom_segment(x = 0 - 3 * sigma +0.05, xend = 0 + 3 * sigma -0.05, y = 0.55, yend = 0.55, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n   annotate(\"text\", x = 0, y = 0.56, label = \"99.7%\") +\n  ylim(c(0, 0.6)) +\n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), labels = c(expression(mu ~ \"- 3\" ~ sigma), expression(mu ~ \"- 2\" ~ sigma), expression(mu ~ \"-\" ~ sigma), expression(mu), expression(mu ~ \"+\" ~ sigma), expression(mu ~ \"+ 2\" ~ sigma),  expression(mu ~ \"+ 3\" ~ sigma))) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n        axis.title.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(),\n        plot.caption = element_text(hjust = 0)) \n        \n# Average income distribution of 10,000 samples of 40 customers (thousands, $) ---------------\n\nset.seed(0)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 10, label = paste0(\"Mean: \", round(average_income, 2))) \n\n# Exemplary possible sample means distributions ---------------\n\nlibrary(gganimate)\nsample_mean &lt;- 134.320\nstandard_error &lt;- sd(sample40_means)\n\nsample_means_modified &lt;- data.frame(Income = NA, possible_mean_sample = NA)\npossible_mean_sample &lt;- seq(sample_mean - 3 * standard_error, sample_mean + 3 * standard_error, length.out = 7)\npossible_mean_sample[2] &lt;- 120.950\n\nfor (possible_mean in possible_mean_sample) {\n  new_sample_mean &lt;- data.frame(Income = sample_means$Income - mean(sample_means$Income) + possible_mean)\n  new_sample_mean$possible_mean_sample &lt;- possible_mean\n  sample_means_modified &lt;- rbind(sample_means_modified, new_sample_mean)\n}\n\nsample_means_modified &lt;- sample_means_modified[-1,]\nsample_means_modified$possible_mean_sample &lt;- round(sample_means_modified$possible_mean_sample, 2)\nsample_means_modified$title &lt;- paste0(\"Assuming samples extracted from a population \\nwith M = \",sample_means_modified$possible_mean_sample, \" (SEM = \", round(standard_error,2), \")\")\nsample_means_modified$title &lt;- as.factor(sample_means_modified$title)\n\npossible_mean_sample &lt;- data.frame(Mean_value = possible_mean_sample, title = unique(sample_means_modified$title))\n\n\np &lt;- ggplot(sample_means_modified, aes(x = Income)) +\n  annotate(\"rect\", xmin = sample_mean - 3 * standard_error, xmax = sample_mean + 3 * standard_error,  ymin = 0, ymax = 0.075, alpha = .1) +\n  annotate(\"text\", x = sample_mean - 3 * standard_error * 1.05, y = 0.025, label = \"Sample Mean - 3SE\", alpha = 0.3, angle = 90)  +\n  annotate(\"text\", x = sample_mean + 3 * standard_error * 1.05, y = 0.025, label = \"Sample Mean + 3SE\", alpha = 0.3, angle = -90)  +\n  annotate(\"text\", x = sample_mean, y = 0.077, label = \"Sample Mean \", alpha = 0.3)  +\n  geom_vline(xintercept = sample_mean, size = 0.4, alpha = 0.4, linetype = \"dashed\") +\n  geom_density(fill = \"steelblue\", alpha = 0.4) +\n  geom_vline(data = possible_mean_sample, aes(xintercept = Mean_value), color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = possible_mean_sample, aes(x = Mean_value * 1.08, y = 0.05, label = paste0(\"Mean: \", round(Mean_value, 2)))) + \n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Probability Density\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0, size = 12),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    transition_states(title, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n# Error Distribution of the Means for 10,000 Samples of 40 Customers (thousands, $) ---------------\n\nsample_means_error &lt;- sample_means\nsample_means_error$error &lt;- sample_means$Income - mean(sample_means$Income)\n\nsd_sample_means_error &lt;- sd(sample_means_error$error)\n\n\nggplot(sample_means_error, aes(x = error)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Error (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = 3.2, y = 11, label = paste0(\"Mean: 0\\nSD: \", round(sd_sample_means_error, 2))) \n   \n# Sampling error of the mean measured in standard errors distribution for 10,000 Samples of 40 Customers ---------------\n\nsample_means_error$error_div_sd &lt;- sample_means_error$error / sd(sample_means$Income)\n\nsd_sample_means_error &lt;- sd(sample_means_error$error_div_sd)\n\n\n\nggplot(sample_means_error, aes(x = error_div_sd)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Error measured in standard errors\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = 1, y = 11, label = paste0(\"Mean: 0\\nSD: \", round(sd_sample_means_error, 2))) \n\n# Sampling error of the mean measured in estimated standard errors distribution for 100,000 Samples of varying number of Customers ---------------\n\nset.seed(34)\nsample_sizes &lt;- c(seq(2, 10, by = 1), seq(20, 60, by = 20))\nsamples_different_sizes &lt;- data.frame(values = NA, sizes = NA, sd = NA)\nfor(sample_size in sample_sizes){\n  \n  sample_means_diff_size &lt;- numeric(length = 100000)\n\n  for(i in seq(1, 100000)){\n     sample_means_diff_size[i] &lt;- mean(sample(customer_data$Income, sample_size))\n     \n  }\n  samples_different_sizes2 &lt;- data.frame(values = sample_means_diff_size, sizes = sample_size, sd = sd(sample_means_diff_size))\n  samples_different_sizes &lt;- rbind(samples_different_sizes, samples_different_sizes2)\n \n}\n\nsamples_different_sizes &lt;- samples_different_sizes[-1,]\n\n\nsamples_different_sizes$error_divided_by_sd &lt;- (samples_different_sizes$values - mean(customer_data$Income)) /(samples_different_sizes2$sd/sqrt(samples_different_sizes2$sizes))\n\n\nsamples_different_sizes$sizes &lt;- as.factor(samples_different_sizes$sizes)\nlevels(samples_different_sizes$sizes) &lt;- paste(\"Number of observations per sample:\", levels(samples_different_sizes$sizes))\ndata_error_div_sd_average &lt;- samples_different_sizes %&gt;%\n  summarise(mean_error_div_sd = mean(error_divided_by_sd), .by = sizes)\n\np &lt;- ggplot(samples_different_sizes, aes(x = error_divided_by_sd, fill = sizes)) +\n  geom_density() +\n  labs(\n    x = \"Error measured in estimated standard errors\",\n    y = \"Probability Density\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_manual(values = c(\n  \"#006699\", \"#FF9E00\", \"#B5113E\", \"#3B125F\", \"#007F7B\", \"#1A4C3C\",\n  \"#7C0A02\", \"#00567F\", \"#7500A4\", \"#B760DE\", \"#002E3E\", \"#7F7F7F\"\n)) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(sizes) +\n  enter_fade() +\n  view_zoom_manual(xmin = c(-120, -100,  -90,  -75,  -70,  -70,  -60, -60, -60,  -45,  -30,  -30), xmax = -1 * c(-120, -100,  -90,  -75,  -70,  -70,  -60, -60, -60,  -45,  -30,  -30), ymin = rep(0, 12), ymax = c(rep(0.02, 5), rep(0.025, 3), 0.03,  0.035, 0.05, 0.06)) +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 25)\nanim"
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html",
    "href": "posts/2021/statistics-foundations-sample-error/index.html",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "title": "Statistics Foundations: Sampling error",
    "section": "Examining Sampling Errors in Data Summarization: The Case of the Mean",
    "text": "Examining Sampling Errors in Data Summarization: The Case of the Mean\nAt the core of statistical analysis lies the foundational task of data summarization—a process that condenses data into a concise and understandable format. This fundamental procedure yields a clear and easily comprehensible overview of the data, facilitating a straightforward grasp of its essential characteristics.\nAmong these essential characteristics, two prominently stand out: central tendency and variability. Central tendency relates to the value around which the different data points cluster around. Conversely, variability quantifies the extent to which data points deviate from this central point, providing insights into the dispersion or spread of data relative to its central location. For instance, the mean serves as an example of a central tendency measure, while the standard deviation exemplifies a variability measure.\nTo visually illustrate the impact of sampling error, we will once again utilize the Kaggle dataset used in our previous post. This dataset contains information on 2,000 supermarket customers, including their age, annual income, and education level. For the purposes of this analysis, we will assume that this dataset represents our entire customer population.\nLet’s envision a scenario: our objective is to rapidly glean insights into the annual income of our customers. One straightforward strategy to achieve this is by computing the mean income, which furnishes us with a succinct metric representing the central tendency around which the majority of our customers’ annual incomes gravitate. In this endeavor, we observe that the mean annual income stands at a noteworthy $120,950, serving as a prominent reference point around which the annual incomes of our customers tend to concentrate.\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\nSampling 40 customers and calculating their annual income mean\nIn this hypothetical case, we possess information about the sample. Consequently, we can obtain information about our population without any error by directly observing it. Therefore, now we know that our population has an average annual income of $120,950. However, in real-life scenarios, and as previously said, obtaining data for the whole population may be unfeasible or even not possible. For this reason, we will assume that we extract a random sample of 40 customers and compute the mean annual income from this sample.\n\n\n\n\n\nFigure 2: Customer annual income distribution for our sample with 40 observations (thousands, $)\n\n\n\n\nAs observed, in Figure 2, the mean value within this sample diverges from that of the broader population. Specifically, the mean for this sample stands at $137,320, contrasting with the population mean of $120,950. This difference amounts to $16,370, and it encapsulates what we commonly refer to as “error.” Notably, in this instance, we possess knowledge about the population, allowing us to discern this difference.\nFor this reason, the terminology we use to describe the metrics summarizing the data characteristics varies depending on whether they are computed within the population or a sample. In the former case, they are referred to as parameters, whereas in the latter, they are known as statistics. In statistical notation, parameters are typically denoted by Greek letters, such as \\(\\sigma\\) for the standard deviation or \\(\\mu\\) for the mean, while statistics are denoted by Latin letters, such as \\(m\\) for the mean and \\(s\\) for the standard deviation.\n\n\nRandomness and sampling: Extracting several means of 40 observations\nMoreover, it’s crucial to note that this sample was derived through a process of random selection. In other words, we randomly picked 40 customers from our population, ensuring that each customer had an equal likelihood of being included. This randomness implies that if we were to generate another sample of 40 customers, it would be improbable for this new sample to mirror the exact composition of the previous one or yield the same mean.\nFigure 3 illustrates the annual income distribution of various samples, each consisting of 40 cutomers randomly selected from our initial population (including the sample we previously examined). It becomes evident that the distribution undergoes fluctuations across these diverse samples. Consequently, this variability gives rise to a spectrum of computed means, ranging from as low as $106,600 to as high as $137,320.\n\n\n\n\n\nFigure 3: Customer annual income distribution for different samples of 40 customers (thousands, $)\n\n\n\n\n\n\nDigging deeper: Exploring mean customer annual income with 10,000 different 40 samples\nTo deepen our comprehension of the variance in computed means, we embark on a more extensive analysis by replicating the previous procedure but on a much larger scale: generating precisely 10,000 samples, each composed of 40 individuals. For every one of these 10,000 samples, we compute the mean annual income. The resulting distribution of these 10,000 means, each originating from distinct samples of 40 individuals randomly selected from our complete population, is visually represented in Figure 4 through a histogram.\n\n\n\n\n\nFigure 4: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nFigure 4 reveals significant insights. Notably, there is a substantial variation in the computed average annual incomes across the various samples, spanning an extensive spectrum from $100,684 to $145,543. This disparity translates into an error range spanning from -$20,266 to $24,593 in contrast with the population’s mean.\nNonetheless, an intriguing revelation emerges from this analysis. Despite the marked variability in sample means, the overall average of these mean annual incomes, drawn from distinct samples, precisely mirrors the population average. This observation means that the average incomes for the different samples consistently cluster around this point.\nMoreover, it is worth noting that the distribution of annual income means extracted from these various samples adheres to a bell-shaped pattern, commonly known as a normal distribution. This pattern signifies that as we move farther away from the population average, the number of observations gradually diminishes.\nTaken together, this implies that, in most instances, the mean annual income estimated from our sample tends to be closer rather than farther away from the population’s mean annual income. Nevertheless, it’s important to acknowledge that there are still situations where significant deviations from the sample mean can occur. The key concern here lies in the fact that if we lacked information about the population and solely possessed a sample with an annual income mean of $145,543, we might mistakenly conclude that, on average, our customers are wealthier than they actually are.\n\n\nIncreasing sample size and analyzing mean annual income distribution of 10,000 samples\nAs previously mentioned, small samples encounter challenges in capturing the subtleties present within the population. Consequently, the larger the sample size, the more effectively we can apprehend these nuances. To illustrate this, we investigate how the variance of computed means changes when we collect samples of 150 customers, as opposed to the previous samples of 40 customers.\nOnce again, we generate 10,000 samples, each containing 150 customers, and calculate the mean annual income for each of these samples. Subsequently, we visualize the distribution of these mean annual incomes for these larger samples by creating a histogram.\nFigure 5 provides a visual comparison between the distributions of means computed using 10,000 samples, each with 40 observations, and another set of samples, each comprising 150 observations.\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers vs 10,000 samples of 150 customers (thousands, $)\n\n\n\n\nFigure 5 provides insights akin to those observed with 40 observations: the distribution of means exhibits a bell-shaped pattern, with the average closely approximating the population mean. However, a significant distinction emerges: the range within which sample means deviate from the overall mean, equivalent to the population average, is notably narrower. In essence, the variability is considerably reduced, indicating that the margin for error when using samples of 150 observations is substantially smaller than that with samples of 40.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nIn the previous examples, we’ve observed that when we calculate means from various samples, these means tend to follow a particular pattern – a bell-shaped distribution known as a normal distribution. This is not just a coincidence; it’s a fundamental concept in statistics called the Central Limit Theorem.\nThe Central Limit Theorem tells us that, regardless of the original shape of the data distribution, when we repeatedly draw samples of sufficient size from that data and calculate their means, those sample means will follow a normal distribution. This is a powerful idea because it allows us to make certain assumptions and conduct statistical analyses even when we don’t know the shape of the population’s distribution.\n\n\n\nIn this scenario, the distribution of means spans from $109,374 to $132,064, resulting in an error range of -$11,576 to $11,114 relative to the population mean. This range is significantly tighter compared to the error range obtained from samples of 40, where the deviation ranged from -$20,266 to $24,593."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "title": "Statistics Foundations: Sampling error",
    "section": "Measuring the sampling error",
    "text": "Measuring the sampling error\nThrough the repetitive extraction of samples of consistent size from a given population, as demonstrated in our previous examples (with both 40 and 150-sized samples), we gain valuable insights into the potential magnitude of errors associated with samples of a particular size.\nAs we’ve witnessed, the average of multiple means calculated from samples of identical size closely aligns with, or is essentially identical to, the true mean of the population. Consequently, the spread or dispersion of these computed means from this point provides a measure of the magnitude of the sampling error. Put simply, the variability in the mean derived from multiple samples of equal size offers a quantifiable measure of the magnitude of the sampling error for samples of that particular size. This measure is commonly known as the standard error of the mean (which we will abbreviate as SEM).\nMathematically, we can express this as the standard error of the mean being equal to the standard deviation of the means of the different samples. Specifically, we prefer using the standard deviation rather than the variance because the former has the same units as the mean, while the latter has squared units. For instance, in the case of annual income, the units for variance would be in dollars squared (\\(\\$^2\\)), while for the standard error, it’s just in dollars ($). In other words:\n\\(\\sqrt{Var(\\bar{X})} = SEM\\)\nThis expression can be translated into the following form:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\nHere, \\(σ\\) represents the population standard deviation and \\(n\\) is the sample size. Establishing an inverse relationship between the standard error and the sample size: as the sample size increases, the standard error decreases. This principle aligns with our intuitive understanding, as seen in the previous post for the statistics foundations series, and as visually depicted in Figure 5.\n\n\n\n\n\n\nStandard Error Derivation\n\n\n\n\n\nLet’s recall that the mean of any variable is equal to the sum of the values of each observation of that variable divided by the total number of observations (which equals our sample size): \\(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\). Therefore, we can rewrite the previous formula as follows:\n\\(\\sqrt{\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nWe also know that the variance of a random variable multiplied by a constant “a” is equal to the variance of that variable multiplied by the square of that constant, i.e., \\(Var(aR) = a^2Var(R)\\), where \\(a\\) is a constant, and \\(R\\) is a random variable. This means that:\n\\(\\sqrt{\\frac{1}{n^2}\\text{Var}\\left(\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nAdditionally, when dealing with a set of pairwise independent random variables (where the variability in one doesn’t depend on the others, as in our case), the variance of their sum is equal to the sum of their individual variances, i.e., \\(\\text{Var}[R_1 + R_2 + \\cdots + R_n]\\) \\(=\\) \\(\\text{Var}[R_1] + \\text{Var}[R_2] + \\cdots + \\text{Var}[R_n]\\), where \\(R_1, R_2,…, R_n\\) are pairwise independent random variables. This allows us to rewrite the formula for SEM as:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}Var(X_i)} = SEM\\)\nLet’s remember that our individual variables, denoted as \\(X_i\\), come from a population with variance equal to \\(\\sigma^2\\). So, our formula becomes:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}\\sigma^2} = SEM\\)\nSince we’re summing up \\(n\\) identical values, we can simplify further:\n\\(\\sqrt{\\frac{1}{n^{2}}n\\sigma^2} = SEM\\)\nUltimately, this can be further simplified to:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\n\n\n\nApplying these formulas, we can now calculate the Standard Error of the mean for sample sizes of 40 and 150. When we compute the standard deviation of the means obtained from multiple samples of 40 customers, we obtain a value of 5.86. Conversely, for samples of 150 customers, we obtained a value of 3.01, which is approximately two times smaller in terms of standard error.\nAlternatively, we can utilize the formula that links the standard error to the population’s standard deviation and the sample size, represented as \\(\\frac{\\sigma}{\\sqrt{n}}\\). Given our population’s standard deviation for annual income is 38.11, dividing this by \\(\\sqrt{40}\\) yields a value of 6.03 for a sample size of 40, while dividing it by \\(\\sqrt{150}\\) results in a value of 3.11 for a sample size of 150.\nHowever, it’s important to note that there are disparities between the results obtained from these two approaches. This is because the first formula relies on a finite number of samples (10,000 in this case), and to obtain equivalent values, the number of samples would need to tend towards infinity, meaning a significantly larger amount of samples.\n\nEstimating the standard error\nUp until now, we have seen that we can compute the standard error of the mean by taking multiple samples of the same size from the population, calculating their means, and extracting the variability of such means.\nYet, let’s face it—in the real world, resources are finite, and repeatedly plucking samples from the population to estimate the standard error can be an extravagant expenditure of these precious assets. Instead, it’s often a more judicious allocation of resources to channel our efforts into amassing a larger sample. As we’ve come to appreciate, a larger sample which better captures the nuances of the population, decreasing the sampling error.\nIn addition to the aforementioned method, we’ve also explored an alternative approach for calculating the standard error—one that bypasses the need to repeatedly extract multiple samples from the population. This alternative method involves dividing the population’s standard deviation by the square root of the sample size (\\(\\frac{\\sigma}{\\sqrt{n}}\\)). However, it’s essential to note that this formula hinges on having access to information about the entire population. This requirement underscores the very reason why we find ourselves seeking to compute the standard error in the first place—a challenge born out of the impracticality of obtaining data for the entire population, compelling us to work with samples and consequently introducing the sampling error.\nNonetheless, it’s crucial to remind ourselves that the core objective when working with a sample is to glean insights into the larger population and derive meaningful conclusions from it. Within this context, it’s reasonable to assume that the standard deviation we observe within our sample (\\(s\\)) can serve as a dependable proxy for the population’s standard deviation (\\(\\sigma\\)). This assumption empowers us to substitute the population standard deviation in the traditional formula (\\(\\frac{\\sigma}{\\sqrt{n}}\\)) with the sample standard deviation (\\(s\\)) derived from that very population:\n\\(SE \\approx \\frac{s}{\\sqrt{n}}\\)\nBy making this substitution, we arrive at an approximation for the standard error. It provides us with a measure of the potential error we may encounter when drawing conclusions from a sample, all without the need for complete information about the population."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#a-final-note",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#a-final-note",
    "title": "Statistics Foundations: Sampling error",
    "section": "A final note",
    "text": "A final note\nThroughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. Throughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. When we compute these statistics from a sample, the values we obtain can deviate from those of the population, consequently introducing an element of error into our analyses.\nHowever, it’s crucial to acknowledge that the formulas we’ve previously derived for calculating the standard error aren’t universally applicable to all statistics. These formulas have been derived with the mean as their reference point. Nevertheless, it’s worth noting that the fundamental concept behind deriving formulas for computing the standard error for other statistics remains consistent: It is based on the variability of a specific statistic obtained from various samples of the same size."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#summary",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#summary",
    "title": "Statistics Foundations: Sampling error",
    "section": "Summary",
    "text": "Summary\n\nSampling is necessary because obtaining data for the entire population is often impractical.\nThe use of samples introduces the challenge of sampling errors.\nSampling errors arise because small samples cannot capture all the nuances present in the population.\nThis leads to variations in common measures like the mean between the sample and the population.\nTo illustrate this, we simulated the extraction of 10,000 samples of the same size from our exemplary population and observed that:\n\nThe mean from different samples exhibits variability.\nDespite this variability, the average of mean values from various samples tends to closely align with the population average.\nA normal distribution pattern is evident in the distribution of annual income means from different samples, indicating that samples with means close to the population mean are more likely.\nLarger sample sizes reduce the standard error and yield more accurate estimates, as they better capture the population’s intricacies.\n\nWhen extracting multiple samples of the same size from a population and calculating their means, the average of these sample means corresponds to the population mean. This allows us to quantify the degree of error associated with that sample size by measuring their variability, i.e., how much they deviate from the population mean.\nStandard error of the mean (SEM) is mathematically linked to the population’s standard deviation and sample size.\nIn practice, we estimate the SEM using our sample’s standard deviation.\nIncreasing the sample size results in a smaller standard error.\nStandard error serves as a valuable tool for quantifying the precision of sample-based estimates and is essential for robust statistical analysis.\n\n\n\nCode\n## Reading the \"population\" and visualizing it\nlibrary(gganimate)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.25, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n  \n## Sampling 40 customers and calculating their annual income mean + visualizing their distribution\n\nset.seed(150)\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40), ]\n\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data_sample$Income)\n\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.15, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n   \n ## Extracting several 40 customers and calculating their annual income mean + visualizing their distribution\n \n customer_data_samples &lt;- customer_data_sample\ncustomer_data_samples$Average_Sample &lt;- \"Sample 1\"\n\nseeds &lt;- seq(300, 5500, length.out = 8)\n\ni &lt;- 2\nfor(seed in seeds) {\n    \n    set.seed(seed)\n    customer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40),]\n    row.names(customer_data_sample) &lt;- NULL\n    average_income &lt;- round(mean(customer_data_sample$Income), 2)\n    customer_data_sample$Average_Sample &lt;- paste0(\"Sample \", i)\n    customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n    i &lt;- i + 1\n}\n\n\ndata_income_average &lt;- customer_data_samples %&gt;%\n  summarise(meanIncome = mean(Income), .by = Average_Sample)\n\n\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Average_Sample)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = meanIncome * 1.25, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Average_Sample, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n## Extracting 10,000 samples of 40 observations and calculate their mean + visualize the distribution of the means\nset.seed(3)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 9, label = paste0(\"Mean: \", round(average_income, 2))) \n\n## Extracting 10,000 samples of 150 observations and calculate their mean + visualize the distribution of the means\n\nset.seed(1500)\n# Create an empty numeric vector of length 10000 named 'sample_means'\nsample_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 150 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 150)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample_means), 2)\nsample_means_40 &lt;- data.frame(Income = sample40_means)\nsample_means_40$Observations &lt;- \"Samples of 40 Observations\"\nsample_means_150 &lt;- data.frame(Income = sample_means)\nsample_means_150$Observations &lt;- \"Samples of 150 Observations\"\nsample_means &lt;- rbind(sample_means_40, sample_means_150)\n\n\ndata_income_average &lt;- sample_means %&gt;%\n  summarise(meanIncome = mean(Income), .by = Observations)\n\n\np &lt;- ggplot(sample_means, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = average_income * 1.08, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 10)\nanim"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "The Ideal of Population and the Reality of Sampling",
    "text": "The Ideal of Population and the Reality of Sampling\nTo comprehensively explore and enhance our understanding of a given issue, it is ideal to possess data pertaining to all entities impacted by that issue. For instance, in our pursuit of gaining deeper insights into our customer base, the ideal scenario involves having access to data for every single one of our customers. In this comprehensive dataset, we would find detailed information regarding each customer’s age, income, purchasing history, and other relevant attributes. Such a dataset would empower us to attain a holistic understanding of our customer base’s profile. In statistical terms, when we allude to data encompassing all entities affected by the issue of interest, we are referring to the population. Naturally, this population is contingent upon the specific focus of interest. For instance, if our objective shifted from understanding the profile of our overall customer base to that of our Swiss customers specifically, our population would comprise solely our Swiss customer subset.\nNevertheless, acquiring data for every single entity influenced by the matter of interest may often prove to be impractical, primarily due to the substantial expenses associated with such an undertaking or its constantly changing nature. Therefore, in the field of statistics, we operate with subsets derived from this population. Ideally, these subsets are constructed in such a way that each entity within them has an equal probability of being selected. Consequently, the resultant subset, known as a sample, is smaller in scale and serves as a reliable and representative image of the broader population. The fundamental concept underlying this approach is that through the observation and analysis of this sample, we can draw meaningful conclusions about the entire population, a process known as inference.\n\nLimitations and Potential Bias in Sampling\nNonetheless, utilizing samples entails operating with an imperfect representation of the population—a mere approximation that may deviate from the true population characteristics. Even when each individual possesses an equal probability of selection, the random nature of the process can result in the overrepresentation or underrepresentation of certain specific types or groups of entities within our sample, thereby potentially leading to skewed findings and conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "Sampling in Practice: Exploring Our Consumer Base Annual Income",
    "text": "Sampling in Practice: Exploring Our Consumer Base Annual Income\nTo illustrate these concepts, let’s delve into an example using a dataset from Kaggle. This dataset comprises data about 2,000 supermarket customers, encompassing a range of characteristics such as age, annual income, and education level. For the sake of this demonstration, let’s envision that this dataset encapsulates information about every single one of our customers, effectively serving as a representation of our customer population. Now, let’s suppose our objective is to gain a more profound insight into the annual income distribution among our customers.\n\nAnnual Income Distribution for Our Population\nHence, we can promptly delve into the examination of the data distribution, which we can observe through a histogram, depicted in Figure 1. This analysis unveils that the distribution of customer incomes exhibits an approximate normality, featuring an average annual income of $120,950 with a moderate degree of variability (Standard Deviation = $38,110). This implies that a substantial proportion of customers have incomes that closely align with the mean value, while fewer customers fall within the income extremes. Additionally, it is worth noting a slight rightward skew in the plot, indicating a minority of individuals with substantially higher incomes compared to the majority.\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\n\n\n\n\n\nLog-normal distribution\n\n\n\n\n\nIn the earlier paragraph, we highlighted that the “distribution of customer incomes exhibits an approximate normality.” To be more precise, the distribution we are discussing is formally identified as the log-normal distribution. Although it visually resembles a normal distribution, featuring a slightly bell-shaped curve, the log-normal distribution distinguishes itself by having a lower bound at zero and a positively skewed nature, leading to a more elongated right tail. The name “log-normal” stems from the phenomenon that transforming the variable into its logarithm results in a normal distribution. Figure 2 visually presents the original distribution of the customer’s annual income on the left and, on the right, showcases the result of applying a logarithmic transformation to the variable. This illustration vividly demonstrates how the logarithm transformation effectively transforms the distribution into a normal distribution.\n\n\n\n\n\nFigure 2: Original (left) and logarithmically transformed (right) customer annual income distributions\n\n\n\n\n\n\n\n\n\n“Collecting” a Small Customer Sample and Exploring their Annual Income\nHaving briefly explored the distribution of our customers’ annual income population, we must acknowledge the practical challenge of obtaining data from the entire population. Therefore, we opt to acquire a representative sample, a feasible alternative. In this scenario, we decided to select an easily obtainable sample of 20 customers, each having an equal probability of being included in the sample. Following the acquisition of this sample, we analyze their annual income distribution, which can be observed in Figure 3, revealing several noteworthy disparities.\nFirstly, we observe that certain income brackets, present in the population data, remain absent in our sample. Additionally, we notice a higher proportion of high-income customers in comparison to the population. These disparities culminate in a higher average customer annual income (Mean = $137,320) and increased variability (Standard Deviation = $47,770) within the sample. Consequently, if we were to draw inferences based on this data, our conclusions would suggest that individuals in the sample exhibit a higher average income and greater income variability compared to the population, thus drawing into error.\n\n\n\n\n\nFigure 3: Customer annual income distribution for our sample with 20 observations (thousands, $)\n\n\n\n\nDespite these disparities, our sample offers us an initial glimpse into the broader income distribution of the entire population. While our sample may not perfectly mirror the population due to its size and inherent limitations, it serves as a foundational reference point for comprehending income patterns within the larger group. It grants us a preliminary understanding of the income ranges, tendencies, and variations we can anticipate when considering the overall population’s income distribution. However, it’s essential to acknowledge the presence of these disparities and recognize that they may impact the conclusions we can draw regarding the population.\n\n\nCapturing Nuances Better with Increased Sample Size\nThese observed disparities are unsurprising, given the inherent limitations of capturing the intricacies of our data with a small sample of just 20 consumers. Consequently, we observe the absence of individuals in various income brackets and a skewed composition compared to our population (which, in this hypothetical case, we have knowledge of, but in practice, we might not).\nOne might naturally question whether increasing the sample size could enhance the richness of our sample and consequently enable us to better capture the nuances present in the population. This would ultimately result in a more faithful representation. To investigate this, we will generate samples of varying sizes, specifically seven additional samples consisting of 40, 80, 120, 150, 300, 500, and 1000 randomly selected consumers from our population. Figure 4 visually illustrates, through histograms, how the distribution changes for each of these sample sizes, including the initially created one with 20 consumers.\n\n\n\n\n\nFigure 4: Customer annual income distribution for varying sample sizes (thousands, $)\n\n\n\n\nIn this Figure, we can discern that larger sample sizes excel in capturing the subtleties inherent in the population’s data distribution. Notably, the distribution of bigger samples closely mirrors that of the population, with fewer missing income ranges and diminished disparities.\nWhen we work with larger samples, we effectively broaden our scope of observation, incorporating a more diverse range of data points. This expanded sample size minimizes the influence of random variation and offers a more robust representation of the population. In essence, larger samples provide a more comprehensive cross-section of the population, enhancing our ability to accurately capture the underlying patterns, variations, and nuances present in the data.\nNow, you might be wondering: what constitutes the ideal sample size? There is not a one-size-fits-all answer; instead, the sample size should strike a balance. It must be large enough to capture the nuances of the population while still being feasible to acquire within budget and time constraints. It is crucial to recognize that a sample will inherently exhibit differences from the population, a fundamental aspect of statistical analysis."
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#summary",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#summary",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "Summary",
    "text": "Summary\n\nStatistics empowers us to glean valuable insights from data, enriching our comprehension of issues of interest.\nIdeally, a complete understanding of any issue necessitates data from every entity involved, referred to as the population.\nPractical constraints often require us to work with smaller, representative subsets, known as samples, where each entity in the population has an equal chance of inclusion.\nSamples, while essential for making inferences about the population, have inherent limitations due to their size, as they can’t fully capture the population’s intricacies, introducing errors into our inferences.\nLarger samples excel at capturing population nuances, offering a more faithful representation of its characteristics.\n\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gganimate)\nlibrary(scales)\n\n# Read customer data from CSV file and adjust income values\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income / 1000\n\n# Calculate the average and standard deviation of the income for the entire population\naverage_income_population &lt;- mean(customer_data$Income)\nstd_deviation_population &lt;- sd(customer_data$Income)\n\n# Create and display a histogram for the entire population's income distribution\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, \n           label = paste0(\"Mean: \", round(average_income_population, 2), \"\\nSD: \", round(std_deviation_population, 2))) \n\n# Set a random seed for reproducibility\nset.seed(150)\n\n# Create a random sample of 20 observations from the population\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 20), ]\n\n# Calculate the average and standard deviation of the income for the sample\naverage_income_sample &lt;- mean(customer_data_sample$Income)\nstd_deviation_sample &lt;- sd(customer_data_sample$Income)\n\n# Create and display a histogram for the sample's income distribution\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data_sample$Income, na.rm = TRUE) * 0.95, y = 13, \n           label = paste0(\"Mean: \", round(average_income_sample, 2), \"\\nSD: \", round(std_deviation_sample, 2))) \n\n\n\n\n\n## Create samples of different sizes and create animation with their distribution\n\n# Create an empty data frame to store the samples and add a column for observations\ncustomer_data_samples &lt;- data.frame()\ncustomer_data_samples$Observations &lt;- character(0)\n\n# Define the sample sizes\nsample_sizes &lt;- c(20, 40, 80, 120, 150, 300, 500, 1000)\n\n# Set a random seed for reproducibility\nset.seed(350)\n\n# Loop through each sample size\nfor (sample_size in sample_sizes) {\n  # Create a random sample of the specified size\n  customer_data_sample &lt;- customer_data[sample(nrow(customer_data), sample_size), ]\n  \n  # Create a label for the observations indicating the sample size\n  customer_data_sample$Observations &lt;- paste0(sample_size, \" Observations\")\n  \n  # Append the sample to the data frame\n  customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n}\n\n# Add a label for the population\ncustomer_data$Observations &lt;- \"Population\"\n\n# Append the population data to the data frame\ncustomer_data_samples &lt;- rbind(customer_data_samples, customer_data)\n\n# Convert the \"Observations\" column to a factor with custom labels\ncustomer_data_samples$Observations &lt;- factor(\n  customer_data_samples$Observations,\n  levels = c(\"20 Observations\", \"40 Observations\", \"80 Observations\", \"120 Observations\", \"150 Observations\", \"300 Observations\", \"500 Observations\", \"1000 Observations\", \"Population\")\n)\n\n# Loop through the levels of the \"Observations\" factor and update labels\nfor (i in seq_along(levels(customer_data_samples$Observations))) {\n  sample_type &lt;- levels(customer_data_samples$Observations)[i]\n  subset &lt;- customer_data_samples[customer_data_samples$Observations == sample_type, \"Income\"]\n  average_income &lt;- round(mean(subset), 2)\n  std_deviation_income &lt;- round(sd(subset), 2)\n  \n  # Update labels with mean and standard deviation information\n  if (i &lt;= 8) {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      levels(customer_data_samples$Observations)[i],\n      \"Sample\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  } else {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      \"Population\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  }\n}\n\n#Create and animate the plot\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = percent_format()) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 150, duration = 15)\nanim"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\nFigure 1. S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\nFigure 1. S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A time of low interest rates",
    "text": "A time of low interest rates\nMoreover, these economic disturbances led governments to act severely. Keynesian policies for the activation of the economy began to play a fundamental role. To encourage consumption, interest rates were lowered to near historic lows. Figure 2 displays the US 10 year note bond yield between 1920 and 2020. In this figure, we can see how the dotcom crash pushed down this yield. However, after that it started recovering until July 2007, when the financial crisis started, after that, that yield continued a downward trend.\n\n\n\nFigure 2. US 10 Year Note Bond Yield"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Stocks as the only attractive investment vehicle",
    "text": "Stocks as the only attractive investment vehicle\nSuch a long period of low interest rates inflated stock prices. Figure 3, shows how cyclically adjusted S&P 500 price-to-earnings ratio has been rising during the last decade, being quite high in comparison to other periods of time (specially before the dot-com bubble). Stock prices started trading at a premium, because there were no attractive alternative investments, this channelled much of the liquidity into equities (bonds with almost no interest or even negative interests were not an attractive investment anymore). Moreover, this low interest rate setting has prompted greater investor leverage, due to its low cost. Hence, low interest rates justify high stock prices, since stocks are highly attractive relative to bonds and debt is stimulated due to its reduced cost.\n\n\n\nFigure 3. Cyclically adjusted S&P 500 price-to-earnings rations"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "An unexpected event: A global pandemic",
    "text": "An unexpected event: A global pandemic\n2020 was not a good year, this year will always be remembered as the year of the COVID. COVID brought many changes in our lives, which undoubtedly had an impact on the economy. As a result, governments continued to pursue stimulative policies and interest rates remained at very low levels.\nOne of the best illustrations of those stimulative policies is the amount of dollars printed in 2020: 21% of the United States dollar was printed in 2020, as shown in Figure 4. This large injection was used for both direct and indirect assistance in the COVID situation. This, at the same time, indirectly channelled part of this aid to the financial markets, increasing their value.\n\n\n\nFigure 4. Annual Money Stock Growth (Trillions of USD)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A double-edged sword",
    "text": "A double-edged sword\nInjecting money to the economy is always something controversial. Through the law of supply and demand it is easy to infer that a considerable increase in supply (without a similar increase in demand) will reduce the price of a good. In money, when this happens, we say that the money loses value, i.e. one monetary unit can acquire fewer products. In other words, this is what we call inflation. Even Warren Buffet has shown concern for inflation in the past month:\n\n“We are seeing very substantial inflation […] We are raising prices. People are raising prices to us and it’s being accepted.”\n\nThis raise on prices can be already tracked on several indices such as Bloomberg’s agriculture index, shown in Figure 5 and also in the annual single-family home price, as shown in Figure 6 (Figure 6 Source).\n\n\n\nFigure 5. Bloomberg agriculture index\n\n\n\n\n\nFigure 6. Annual Single-family home price\n\n\nThe appearance of inflation means that interest rates should rise. Something that was already pointed by Janet Yellen at the start of this month:\n\n“It may be that interest rates will have to rise somewhat to make sure that our economy doesn’t overheat”\n\nBut what would happen if inflation persists and interest rates have to be risen? Remember that I previously said that stocks are trading at a premium due to the lack of attractive alternative investments. This would no longer be true and this premium would no longer be a thing. In addition, an increase in interest rates would reduce the attractiveness of leverage, thereby encouraging investor’s deleverage. And, thus, we should expect a decrease in the stock price."
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "The market can not be timed",
    "text": "The market can not be timed\nEven with all these indicators, it is difficult to say whether this will happen in the short to medium term. There are many factors which could deter inflation away and interest rates low. As an example, Berkshire Hathaway has been stacking cash during the last years, as shown in Figure 7, playing a slightly more defensive position. This may be due the fact that they already saw that low interest rates during the last decade were driving the stock market at high prices. However, interest rates are still low and during that time the stock market has continued rising.\n\n\n\nFigure 7. Berkshire’s Cash Holdings"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Summary",
    "text": "Summary\n\nThe first decade of the 2000s was characterized by two economic crisis, which defined an economy with very low interest rates\nLow interest rates increased stock market prices, since there were no attractive alternative investments and leverage was cheap. Thus, stock markets traded at a premium.\nThe Covid crisis led to a massive injection of money into the economy.\nThis money injection is a double-edged sword which may have brought an exuberance illusion awakening a ghost that has been dormant in recent years, inflation.\nThe emergence of inflation would imply an increase in interest rates.\nAn increase in interest rates would mean that the premium paid for stocks would be lost.\nDespite all these facts, timing the market is no easy task. And thus, other factors could keep inflation away and interest rates away, prolonging this situation."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html",
    "href": "posts/2023/intro-to-voice-analytics/index.html",
    "title": "Introductory Voice Analytics with R",
    "section": "",
    "text": "In this tutorial, we will analyze a compelling video featuring a female Scottish user attempting, albeit humorously, to issue a command to Amazon Alexa to play a song on Spotify. This viral video, though amusing, highlights a common frustration many users encounter when trying to communicate effectively with voice-controlled interfaces.\nVideo\nFor our comprehensive analysis, we began by extracting the audio file from the previous video and converting it into the Waveform audio format. Our investigation is centered on two pivotal aspects of this interaction:\n\nSpeech Formation of the Wakeword “Alexa”\nVocal Changes During the Issuance of a Command (“Alexa, play something is cooking in my kitchen on Spotify by Dana”)\n\nTo facilitate our analysis, we meticulously edited the voice recordings, retaining only the segments containing the three consecutive wakewords and the two subsequent user commands. It is noteworthy that the third utterance of “Alexa” was not associated with a command but rather followed by a barrage of profanity and abusive language. Consequently, our subsequent sections will delve into the examination of this specific case. You can download the files for this example here.\nOur analytical approach primarily leverages the renowned seewave package, which has emerged as the gold standard in R-sound analysis. This versatile package encompasses an impressive array of 130 functions designed for the analysis, manipulation, representation, editing, and synthesis of time-based audio waveforms. While seewave serves as our cornerstone, we also make reference to other valuable packages, such as tuneR, soundgen, and phonTools, for their specialized functionalities as needed."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#understanding-user-frustration",
    "href": "posts/2023/intro-to-voice-analytics/index.html#understanding-user-frustration",
    "title": "Introductory Voice Analytics with R",
    "section": "",
    "text": "In this tutorial, we will analyze a compelling video featuring a female Scottish user attempting, albeit humorously, to issue a command to Amazon Alexa to play a song on Spotify. This viral video, though amusing, highlights a common frustration many users encounter when trying to communicate effectively with voice-controlled interfaces.\nVideo\nFor our comprehensive analysis, we began by extracting the audio file from the previous video and converting it into the Waveform audio format. Our investigation is centered on two pivotal aspects of this interaction:\n\nSpeech Formation of the Wakeword “Alexa”\nVocal Changes During the Issuance of a Command (“Alexa, play something is cooking in my kitchen on Spotify by Dana”)\n\nTo facilitate our analysis, we meticulously edited the voice recordings, retaining only the segments containing the three consecutive wakewords and the two subsequent user commands. It is noteworthy that the third utterance of “Alexa” was not associated with a command but rather followed by a barrage of profanity and abusive language. Consequently, our subsequent sections will delve into the examination of this specific case. You can download the files for this example here.\nOur analytical approach primarily leverages the renowned seewave package, which has emerged as the gold standard in R-sound analysis. This versatile package encompasses an impressive array of 130 functions designed for the analysis, manipulation, representation, editing, and synthesis of time-based audio waveforms. While seewave serves as our cornerstone, we also make reference to other valuable packages, such as tuneR, soundgen, and phonTools, for their specialized functionalities as needed."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#data-acquisition-and-processing",
    "href": "posts/2023/intro-to-voice-analytics/index.html#data-acquisition-and-processing",
    "title": "Introductory Voice Analytics with R",
    "section": "Data Acquisition and Processing",
    "text": "Data Acquisition and Processing\n\nReading Sound Files\nAs previously mentioned, the primary focus of this tutorial centers around the utilization of the seewave package. While it is important to note that seewave lacks native capabilities for sound file reading, we adeptly overcome this limitation by harnessing functions from complementary packages. It is crucial to emphasize that different packages generate distinct classes of sound objects, each optimized for specific sound manipulation tasks. Consequently, when choosing an alternative package to load sound data, it becomes paramount to consider this inherent class compatibility.\nIn the context of seewave, its core functionality is primarily tailored to work seamlessly with sound objects of the Wave class. These Wave class sound objects are conventionally created using the tuneR package. Hence, when working with seewave, it is strongly recommended to employ tuneR for sound data loading.\nThe process of loading the two user commands from the interaction with Amazon Alexa and saving these sound files as new objects, cmd1 and cmd2, is accomplished as follows:\nTo begin, we employ the readWave() function from the tuneR package. This function efficiently loads or reads a sound file from a specified location. Here is the step-by-step procedure for loading and saving the user’s interaction commands:\n\nlibrary(tuneR)\ncmd1 &lt;- readWave(\"alexa_cmd1.wav\")\ncmd2 &lt;- readWave(\"alexa_cmd2.wav\")\n\nUpon invoking these newly created objects, we obtain an informative output that reveals the underlying structure of the objects. This output not only underscores the seamless integration of the ‘wave’ class objects generated by the tuneR package but also provides key insights into their fundamental characteristics. These characteristics encompass:\n\nNumber of Samples: This indicates the total count of discrete data points in the audio waveform.\nDuration (in seconds): The elapsed time in seconds, capturing the length of the audio.\nSampling Rate (in Hertz): Denoting the rate at which individual samples are taken per second.\nNumber of Channels: It signifies whether the audio is mono (single channel) or stereo (two channels).\nBit Rate: Representing the number of bits processed per unit of time.\n\nUpon inspecting both objects, it becomes evident that they share identical sampling rates, channel numbers, and bit rates. However, a notable distinction emerges in their duration, with the second sound file being 820 milliseconds longer than the first. This divergence in duration warrants further investigation and could potentially yield valuable insights into the audio data.\n\ncmd1\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\ncmd2\n\n\nWave Object\n    Number of Samples:      137881\n    Duration (seconds):     3.13\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nFurthermore, the readWave() function offers additional parameters, notably from and to, which facilitate the selection of specific segments within the audio data for reading. By default, these parameters operate in samples units, defining the segment based on sample counts. However, the readWave() function also introduces the units argument, affording the flexibility to modify the units of from and to to seconds, minutes, or hours.\nTo illustrate, suppose we wish to load only the initial 0.5 seconds and the segment from 0.5 seconds to 2 seconds from the audio file and store them in separate objects, denoted as cmd1.s1 and cmd1.s2, respectively. Achieving this precision is straightforward, involving the configuration of the from, to, and units arguments as demonstrated below:\n\n(cmd1.s1 &lt;- readWave(\"alexa_cmd1.wav\",from=0,to=0.5,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      22050\n    Duration (seconds):     0.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(cmd1.s2 &lt;- readWave(\"alexa_cmd1.wav\",from=0.5,to=2,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      66150\n    Duration (seconds):     1.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\nPlaying sound file\nSound analysis is an iterative process, requiring frequent listening to parts of a soundwave. Although, R itself cannot play sound files the seewave’s listen( ) function allows us to call the default audio player of the user’s operating system.\nFor this reason, we first load the seewave package:\n\nlibrary(seewave)\n\nWarning: package 'seewave' was built under R version 4.2.3\n\n\nThus, we could play the previously loaded cmd1 variable by using the listen() function:\n\nlisten(cmd1)\n\nMuch like the readWave() function, the listen() function offers analogous from and to arguments to precisely determine the sections of interest for auditory playback. Furthermore, it introduces an f argument, providing the capability to manipulate the sampling frequency rate. This adjustment allows for the alteration of the original sound’s pitch, facilitating the creation of both higher and lower-pitched auditory renditions. The code snippet below demonstrates this transformative process:\n\nlisten(cmd1, f=cmd1@samp.rate*1.1)\n\n\nlisten(cmd1, f=cmd1@samp.rate/1.1)\n\n\n\nEditing sound Files\nIn certain applications, it becomes necessary to perform additional edits on voice files. These edits could include tasks such as (1) extracting specific segments of a soundwave for in-depth analysis, (2) eliminating a series of utterances from a soundwave, (3) trimming periods of silence at the beginning or end of a sound file, and (4) filtering out all unvoiced frames from a sound file.\nBoth the tuneR and seewave packages offer a suite of functions designed to address these various editing procedures:\n\nextractWave( ): This function facilitates the extraction of desired segments from a soundwave. Users can specify the segments using the from and to arguments, which we discussed earlier. The extractWave() function defaults to samples as the unit, but this can be adjusted using the ‘xunit’ argument.\ndeletew( ): To remove specific portions from a soundwave, the deletew() function is employed. By default, it operates in units of time.\nnoSilence( ): This function is particularly useful for removing periods of silence from the beginning or end of a sound file. Users can fine-tune this process by specifying the desired silence level.\nzapsilw( ): To eliminate all unvoiced frames from a sound file, the zapsilw() function comes into play. Users can tailor this operation by setting the ‘threshold’ argument, which measures the amplitude threshold in percent distinguishing silence from signal. Additionally, the zapsilw() function offers the default feature of generating oscillograms for both the original sound file and the modified version post unvoiced frame removal, providing visual insight into the process.\n\nThese functions empower users to efficiently manipulate sound files, ensuring they are tailored to meet the specific requirements of their analyses. To illustrate their practical utility, let’s delve into some illustrative examples.\nAs an instance, consider the application of the extractWave() function to pinpoint the initial 700 milliseconds of the soundwave residing in cmd1.\n\n#Extract first 700ms\ncmd1.xtr &lt;- extractWave(cmd1, from = 0, to = 0.7, xunit = \"time\") \n\nHere, the from and to arguments are employed to specify the time range of interest. In this case, we isolate the first 700 milliseconds, facilitating a focused analysis.\nAlternatively, instead of extracting this segment, we can achieve its removal using the deletew() function:\n\n#Delete first 700ms\ncmd1.rem &lt;- deletew(cmd1, from=0, to=0.7, output=\"Wave\") \n\nContinuing with our exploration of sound manipulation, we can also replicate the first 700 milliseconds threefold with the repw() function:\n\n#Repeat first 700ms three times\ncmd1.rep3 &lt;- repw(cmd1, f=cmd1@samp.rate, times=3, output=\"Wave\")\n\nTo refine the audio data further, we may opt to remove solely the unvoiced segments at the beginning and end using the noSilence() function:\n\n#Remove only unvoiced start and ending\ncmd1.cut &lt;- noSilence(cmd1)\ncmd1.cut\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\ncmd1\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nIn this particular instance, a comparison between the original sound cmd1 and the processed version cmd1.cut, where we have removed unvoiced segments from the beginning and end, reveals no discernible alteration. This lack of change stems from the fact that cmd1 initially did not contain any unvoiced segments at either its outset or conclusion.\nAlternatively, we could use the zapsilw() function to cleanse all the unvoiced elements of cmd1:\n\n#Remove all unvoiced frames of a soundwave\ncmd1.nosil &lt;- zapsilw(cmd1, threshold=1, output=\"Wave\")\ncmd1\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\ncmd1.nosil\n\n\nWave Object\n    Number of Samples:      62425\n    Duration (seconds):     1.42\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nFigure 1: Oscillograms of First and Original Command (Upper Panel) and with voice breaks removed (Lower Panel)\n\n\n\n\nIn this case, a close examination of Figure 1 reveals that the zapsilw() function has been remarkably successful in eliminating all the unvoiced areas within the soundwave. This process has resulted in a cleaner and more refined audio representation.\nFurthermore, when we compare cmd1 to cmd1.nosil, it becomes evident that the duration of the latter is noticeably shorter, clocking in at almost 1 second less. This reduction in duration underscores the effective removal of unvoiced segments, affirming the utility of this process in streamlining the audio data while preserving its crucial vocal elements.\n\n\nWriting sound files\nFollowing sound file editing, it is often essential to preserve the revised version for future use. To accomplish this, the seewave package offers the convenient savewav( ) function, designed explicitly for storing R sound objects as .wav files. The process involves specifying the following parameters:\n\nR Sound Object: As the first argument, designate the R sound object that you intend to save as a .wav file.\nSampling Frequency (f): Next, specify the sampling frequency to be associated with the saved .wav file.\nFilename (filename): Finally, provide the desired filename under which the edited sound object will be stored.\n\nIt’s worth noting that if the sampling frequency is not explicitly defined, the seewave package will automatically utilize the same sampling frequency as the edited R object, simplifying the saving process.\nAs an illustrative step, let’s go ahead and save the modified version of cmd1, where all unvoiced segments have been successfully removed, as a .wav file within our system:\n\nsavewav(cmd1.nosil, filename = \"cmd1_noSilence.wav\")"
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#visualizing-sound",
    "href": "posts/2023/intro-to-voice-analytics/index.html#visualizing-sound",
    "title": "Introductory Voice Analytics with R",
    "section": "Visualizing sound",
    "text": "Visualizing sound\nHaving covered the processes of reading, editing, and saving sound objects, we now embark on the journey of visualizing the essential attributes of a sound wave. Visualization entails the transformation of a sound wave into a graphical or statistical representation. The primary means of depicting a sound wave typically involve showcasing its (1) amplitude, (2) frequency, and (3) a blend of amplitude and frequency variations over time. Two common visualizations employed for this purpose are oscillograms, which capture amplitude, and spectrograms, which provide insights into frequency and the interplay between frequency and amplitude over time.\n\nVisualizing Amplitude\nOscillograms offer a visual representation of the instantaneous amplitude of a soundwave plotted against time. They are often referred to as waveforms, as they graphically depict the variations within the sound wave itself. Oscillograms serve as valuable tools for discerning potential changes in loudness over time within a soundwave. In R, you can create oscillograms using the oscillo() function from the seewave package. This function requires just one argument, the sound object. Moreover, oscillo() provides the flexibility to customize various visual aspects, such as the title (using the title argument), label color (via the collab argument), and wave color (by setting the ‘colwave’ argument). Additionally, you can specify the ‘from’ and ‘to’ arguments, similar to what we did during data processing, to generate an oscillogram for a specific time interval in seconds.\nTo gain insights from the oscillograms of the two Alexa commands, we aim to first visualize the entire soundwave and then zoom in to focus solely on the articulation of the wakeword.\nTo plot all four graphs within a single plotting region, we partition the plot into four distinct sections using the standard par and mfrow arguments in R. Furthermore, to exclusively display the wakeword, we make use of the from and to arguments within the oscillo() function.\n\n\n\n\n\nFigure 2: Oscillograms of First and Second Command (Upper Panel) and Wakeword (Lower Panel)\n\n\n\n\nUpon closer examination, as depicted in Figure 2, a few notable observations come to light at first glance:\n\nDifference in Duration: It becomes apparent that the second command is slightly longer than the first. Remarkably, both commands share identical content, both stating “Alexa, play by Dana.” A deeper dive into the oscillograms reveals the underlying reason: in the second command, there are longer vocal breaks, representing the time gaps between each spoken word.\nEmphasis on Individual Words: Notably, there are striking variations in the emphasis placed on individual words when issuing the two commands. For instance, in the lower panel of Figure 2, we can discern the pronounced differences in overall amplitudes. Particularly, the word “Alexa” in the second command exhibits significantly greater amplitude compared to its counterpart in the first command. This observation suggests that, without needing to listen to the voice files or comprehend the spoken content, we can already deduce that the second command tends to be louder overall than the first. Such a distinction may hint at potential heightened emotions such as anger, stress, or irritability on the part of the speaker.\n\nThese initial insights gleaned from the oscillograms provide valuable cues for further analysis and interpretation of the audio data.\n\n\nVisualizing Fundamental Frequency\nThe graphical representation of the fundamental frequency over time, often referred to as an f0 contour or pitch track, holds significant value in acoustic analysis. This visualization unveils essential aspects, including the speaker’s fundamental frequency range, pitch variations throughout speech, distinctions between voiceless and voiced segments, as well as patterns of regular and irregular phonation.\nIn the context of the seewave package, the fund() function takes center stage. It skillfully estimates the fundamental frequency of an R object, employing a short-term cepstral transform, and automatically generates a visual plot of the fundamental frequency.\nTo demonstrate the practical application and insights derived from this analysis, we narrow our focus to the wakewords uttered by the speaker during their interaction with Amazon Alexa. It’s worth noting that although the user issued only two complete commands, the interaction involved three distinct wakewords. The third wakeword, notably, did not lead to the issuance of a command but instead featured the use of offensive language directed at Alexa.\nTo proceed with this exploration, we initiate the process by reading these distinct wakewords using the readWave() function:\n\n(w1 &lt;- readWave(\"alexa_wakeword_1.wav\"))\n\n\nWave Object\n    Number of Samples:      30902\n    Duration (seconds):     0.7\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(w2 &lt;- readWave(\"alexa_wakeword_2.wav\"))\n\n\nWave Object\n    Number of Samples:      32804\n    Duration (seconds):     0.74\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(w3 &lt;- readWave(\"alexa_wakeword_3.wav\"))\n\n\nWave Object\n    Number of Samples:      46116\n    Duration (seconds):     1.05\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nUpon inspecting the output generated after calling each sound object, we readily observe differences in the durations of these wakewords. Specifically, they span 0.7 seconds, 0.74 seconds, and 1.05 seconds, respectively, highlighting the temporal distinctions among them.\nWith the three wakewords now successfully imported into R, we proceed to concatenate them into a single soundwave using seewave’s bind() function. We undertake this concatenation to facilitate a comprehensive visual evaluation of how the fundamental frequency evolves from one command to another.\n\n(wake_all &lt;- bind(w1,w2,w3))\n\n\nWave Object\n    Number of Samples:      109822\n    Duration (seconds):     2.49\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nThis amalgamation sets the stage for the application of the fund() function to the concatenated sound object. Additionally, we leverage the ‘threshold’ argument within the fund() function, allowing us to eliminate smaller amplitude variations during signal detection, specified as a percentage. The results generated by the fund() function, represented as a matrix with two columns (x denoting time and y representing fundamental frequency), are stored in a new variable ff. Subsequently, to illustrate the evolution of the speaker’s fundamental frequency over time, we employ the time and frequency parameters in a linear model. This modeling unveils an upward trend, as portrayed in Figure 3, showcasing the progression from the first to the second and ultimately the third utterance of the wakeword “Alexa.”\n\nff &lt;- fund(wake_all,threshold=1)\nmod &lt;- lm(ff[,2]~ff[,1])\nabline(mod,col=\"red\",lwd=2,lty=2)\n\n\n\n\nFigure 3: Increasing Fundamental Over Time from First, Second, to Third Wakeword\n\n\n\n\nTo enhance our analysis further, we employ the formanttrack() function from the phonTools package. This function serves as a practical tool to track the distribution of acoustic energy across various frequency bands, typically in 1000Hz intervals. It’s worth noting that the phonTools package exclusively supports mono audio. Therefore, when working with stereo files, as in this scenario, only one of its channels must be specified as an argument. The channel of a wave object can be accessed using the @ operator along with the name of the desired channel (left or right). Additionally, since phonTools is not specifically designed to handle wave objects, the sampling frequency of the sound file must be manually set using the fs argument.\nFigure 4 illustrates the first three frequency bands for each wakeword, respectively. These displays complement our earlier observations, illustrating the frequency escalation from the first to the third wakeword, along with an overall increase in variability over time. Given the strong correlation between heightened frequency levels and experiences of stress, anger, or frustration, these visualizations further underscore the extended duration during which the user encountered difficulties in their interaction with Amazon Alexa.\n\nlibrary(phonTools)\npar(mfrow=c(1,3))\nformanttrack(w1@left, fs=w1@samp.rate, formants=3, periodicity=.5)\nformanttrack(w2@left, fs=w2@samp.rate, formants=3, periodicity=.5)\nformanttrack(w3@left, fs=w3@samp.rate, formants=3, periodicity=.5)\n\n\n\n\nFigure 4: Visualizing the First (Black), Second (Red), and Third (Green) Formants Across Wakewords\n\n\n\n\n\n\nSpectrograms\nSpectrograms provide a rich, multi-dimensional portrayal of a soundwave, offering insights into its composition. In this representation, time unfolds along the x-axis, frequency extends along the y-axis, and a third dimension portrays amplitude levels (loudness) through varying color codes. The seewave package equips us with the spectro() function, which constructs a spectrographic visualization of a time wave. This versatile function demands only a time wave object, such as a sound object, as its input.Furthermore, the spectro() function offers several customization options to tailor the appearance of the spectrogram. For instance, the flim argument permits us to specify the minimum and maximum frequencies displayed. Additionally, osc introduces an oscillogram at the bottom of the spectrogram plot, while dBref allows us to define a reference value for the dB range of amplitude.\nFigure 5 provides a more nuanced understanding, summarizing the significant distinctions between the initial two commands and the subsequent three consecutive wakewords spoken by the user. Across all spectrograms, three noteworthy points emerge:\n\nIncrease in Loudness: There is a noticeable escalation in power or loudness, depicted by the deepening reddish color gradients.\nProlonged Voice Breaks: A trend of longer voice breaks becomes evident from the first command to the second.\nFrequency Stability: The first command exhibits relatively stable frequency, whereas the second command displays a moderate upward trend.\n\nCollectively, these observations suggest that the heightened frequency, amplified loudness, and extended voice breaks likely mirror the speaker’s experiences of tension, anger, stress, and frustration during their interaction with Amazon Alexa.\n\nspectro(cmd1, osc=TRUE, flim=c(0,6), dB = \"max0\", dBref = 2*10e-5)\nspectro(cmd2, osc=TRUE, flim=c(0,6), dB = \"max0\", dBref = 2*10e-5)\nspectro(wake_all, osc=TRUE, flim=c(0,6), dB = \"max0\", dBref = 2*10e-5) \n\n\n\n\n\n\nFigure 5: Spectrogram of the User’s Commands and First Three Wakewords"
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#acoustic-feature-extraction",
    "href": "posts/2023/intro-to-voice-analytics/index.html#acoustic-feature-extraction",
    "title": "Introductory Voice Analytics with R",
    "section": "Acoustic Feature Extraction",
    "text": "Acoustic Feature Extraction\nIn addition to the tasks we’ve explored thus far, the extraction of acoustic characteristics from sound files holds paramount importance in their analysis. These extracted features serve a variety of purposes, whether utilized as predictors or outcomes in statistical models. In this section, we shed light on crucial functions spread across different packages for extracting these vocal attributes, categorizing them into distinct domains: time, amplitude, frequency, and spectral.\n\nTime Domain\nThe most fundamental measure in the time domain is the duration, typically expressed in seconds or milliseconds, which quantifies the temporal extent of a soundwave. The duration( ) within the seewave package offers a straightforward means of extracting this duration, providing the duration of the sound object in seconds. When we apply this function to the two commands, we ascertain that the first command has a duration of 2.31 seconds, while the second command spans 3.13 seconds.\n\nduration(cmd1)\n\n[1] 2.314082\n\nduration(cmd2)\n\n[1] 3.126553\n\n\nSimilarly, when we apply the same procedure to the wakewords, we discover that they have durations of 0.7 seconds, 0.74 seconds and 1.05 seconds, respectively.\n\nduration(w1)\n\n[1] 0.7007256\n\nduration(w2)\n\n[1] 0.7438549\n\nduration(w3)\n\n[1] 1.045714\n\n\nThe soundgen package includes the analyze() function, which provides the ability to extract several features across the time, amplitude, frequency, and spectral domain respectively. For example, using this function we can directly extract the number of voiced and unvoiced frames of a sound object. To do so, the results from the soundgen function should be stored in an R object. This object results in a list of two data.frames: a detailed data.frame (you can retrieve it by using $detailed) in which each row represents a Short-Time Fourier Transform frame and each column represents a vocal feature and a summarized data.frame (you can retrieve it by using $summary).\nThus, we can proceed to use the analyze() function to extract several vocal features from the first and second commands:\n\nlibrary(soundgen)\nfeat_cmd1 &lt;- analyze(\"alexa_cmd1.wav\", plot = F)\nfeat_cmd2 &lt;- analyze(\"alexa_cmd2.wav\", plot = F)\n\nSince the summary object represents a data.frame, we can access its columns using the $ operator followed by the column name. Hence, in order to check which proportion of frames are voiced, we can directly call the voiced column from the output extracted by the analyze() function:\n\n#Returns the proportion of voiced samples\nfeat_cmd1$summary$voiced\n\n[1] 0.4395604\n\nfeat_cmd2$summary$voiced\n\n[1] 0.3145161\n\n\nRevealing a a greater percentage of vocal breaks in the second (43.96%) compared to the first (31.45%) command.\n\n\nAmplitude Domain\nThe amplitude of a soundwave dictates its power or loudness, with smaller amplitudes indicating softer sounds and larger amplitudes indicating louder ones. It essentially measures how far air particles deviate from their equilibrium position. To calculate the amplitude at various points in time, we can utilize the oscillo function from the seewave package, as demonstrated earlier, with its plot argument set to FALSE. This provides us with various statistics, such as the maximum and minimum amplitudes. In the code snippet below, we observe that the first command generally had a higher volume compared to the second, evident in the smaller minimum and larger maximum values.\n\nmin(oscillo(cmd1, plot = F))\n\n[1] -28068\n\nmax(oscillo(cmd1, plot = F))\n\n[1] 19895\n\nmin(oscillo(cmd2, plot = F))\n\n[1] -29746\n\nmax(oscillo(cmd2, plot = F))\n\n[1] 20029\n\n\nAnother method to estimate soundwave amplitude involves computing the root-mean-squared (RMS) of the amplitude envelope. This can be accomplished by combining the rms() function to calculate the RMS and the env() function to calculate the envelope. This approach provides us with insight into the sound file’s average loudness, revealing that, on average (after removing silent portions), the first command was louder than the second.\nThe Root Mean Square (RMS) is calculated as follows:\n\\[RMS = \\sqrt{\\frac{1}{N}\\sum_{n=1}^{N} x_i^2}\\] Here, \\(x_i\\) represents each amplitude envelope point and \\(N\\) represents the total number of points.\n\npar(mfrow=c(1,2))\nrms(env(zapsilw(cmd1, plot = F),f=cmd1@samp.rate))\n\n[1] 8771.827\n\nrms(env(zapsilw(cmd2, plot = F),f=cmd1@samp.rate))\n\n[1] 9386.833\n\n\n\n\n\nFigure 6: Amplitude envelope of Command 1 (right) and Command 2 (left)\n\n\n\n\nMoreover, the analyze() function from the soundgen package yields a subjective unit of loudness measured in sone. This measure is stored in the ‘loudness’ column within the data.frame produced by analyze(). Using the mean() function in R on the loudness column computes the average loudness in sone, reaffirming our earlier observations that the second command was generally louder than the first.\n\nfeat_cmd1$summary$loudness_mean\n\n[1] 15.72149\n\nfeat_cmd2$summary$loudness_mean\n\n[1] 17.66282\n\n\nAdditionally, soundgen provides the getLoudness() function, offering a visual representation of loudness spectrum for soundwaves. It presents a grayscale visualization of the spectrum, with darker areas indicating higher sound pressure levels, and thus, higher loudness.\n\nloudness1 &lt;- getLoudness(\"alexa_cmd1.wav\", plot = FALSE)\n\nloudness2 &lt;- getLoudness(\"alexa_cmd2.wav\", plot = FALSE)\n\n\n\nFrequency domain\nIn this section we focus on the extraction of two main features in the frequency domain: (1) pitch of soundwave and (2) the extent of variability in frequency of the sound-wave. Both of them can be directly extracted from the summary of the results extracted from soundgen’s analyze( ) function: (1) pitch_mean and (2) pitch_sd. Consistent with our expectation, we find a strong increase from 220.38Hz to 247.23Hz in the average pitch. Thus, the extracted features confirm the “shrill” and aroused sound of the user’s voice after the repeated failure from the first to the second voice command.\n\nfeat_cmd1$summary$pitch_mean\n\n[1] 220.3752\n\nfeat_cmd2$summary$pitch_mean\n\n[1] 247.2293\n\nfeat_cmd1$summary$pitch_sd\n\n[1] 61.99661\n\nfeat_cmd2$summary$pitch_sd\n\n[1] 32.97635\n\n\nTo get into a higher level of granularity, we can apply those same analysis only to the wakeword “Alexa”. In that way, we can see how these key differences are even more prominent: 174.18Hz on the first wakeword, 278.79Hz on the second and 299.33Hz on the third.\n\nfeat_w1 &lt;- analyze(\"alexa_wakeword_1.wav\", plot = F)\nfeat_w2 &lt;- analyze(\"alexa_wakeword_2.wav\", plot = F)\nfeat_w3 &lt;- analyze(\"alexa_wakeword_3.wav\", plot = F)\n\nfeat_w1$summary$pitch_mean\n\n[1] 174.1786\n\nfeat_w2$summary$pitch_mean\n\n[1] 278.7902\n\nfeat_w3$summary$pitch_mean\n\n[1] 299.3255\n\nfeat_w1$summary$pitch_sd\n\n[1] 13.46018\n\nfeat_w2$summary$pitch_sd\n\n[1] 103.5946\n\nfeat_w3$summary$pitch_sd\n\n[1] 124.2244\n\n\n\n\nSpectral domain\nSpectral features of a soundwave reflect perturbances of a soundwave. Measures of spectral qualities of a soundave generally assess the amount of perturbance or periodicity of sound. Two such measure of perturbances can be directly extracted using the Harmonics-to-Noise ratio (HNR) and level of entropy of the soundwave from the analyze( ) function of the soundgen package. Comparing the level of entropy and periodicity between the two commands confirms the moderately larger level of entropy and greater perturbances in the second compared to the first command respectively.\n\nfeat_cmd1$summary$entropy_mean\n\n[1] 0.1250395\n\nfeat_cmd2$summary$entropy_mean\n\n[1] 0.1437937\n\n\n\nfeat_cmd1$summary$HNR_mean\n\n[1] 5.90924\n\nfeat_cmd2$summary$HNR_mean\n\n[1] 5.422622"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html",
    "href": "posts/2023/voiceR-R-package/index.html",
    "title": "My first R package: voiceR",
    "section": "",
    "text": "The subtleties of our speech often unveil more about us than the mere words we utter. These subtleties can be quantified through a constellation of distinct vocal features. Together, these features offer a glimpse into an individual’s speech pattern, revealing a trove of information. Within each individual’s unique vocal features lies valuable insights into their personal traits, such as age and gender, as well as their current emotional state. Furthermore, they have been linked to broader evaluative outcomes, including perceptions of physical attractiveness and strength. In medical contexts, these features have proven diagnostic, aiding in studying speech pathologies like vocal loading and enabling the detection of conditions such as Parkinson’s disease. Additionally, they have been instrumental in predicting and monitoring the treatment of clinical depression.\nWhile technically oriented fields like computer science, with their cadre of adept researchers, swiftly embraced and expanded the realm of voice analytics—utilizing deep learning models to dynamically recognize discrete human emotions—less technically inclined disciplines recognized the potential of voice analytics but fell short in harnessing its vast capabilities. These disciplines acknowledged but did not fully exploit the power of voice analytics to describe, comprehend, and predict affective and cognitive aspects of human expression.\nTo bridge this gap and offer a practical interface for voice analytics, we have developed an R package aiming at making voice analytics more accessible: the voiceR package, which today has been published to CRAN."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#what-is-voicer",
    "href": "posts/2023/voiceR-R-package/index.html#what-is-voicer",
    "title": "My first R package: voiceR",
    "section": "What is voiceR?",
    "text": "What is voiceR?\nvoiceR is an R package specifically designed to streamline and automate voice analytics for social science research. This package simplifies the entire process, from data processing and extraction to analysis and reporting of voice recording data in the behavioral and social sciences. It provides an intuitive and user-friendly interface, including an interactive Shiny app, making it accessible for researchers. One of its key features is batch processing, enabling the simultaneous reading and analysis of multiple voice files. Moreover, voiceR automates the extraction of crucial vocal features, facilitating further in-depth analysis. Notably, it goes a step further by automatically generating APA-formatted reports tailored for typical between-group comparisons in experimental social science research. Figure 1 offers a video summary of voiceR’s key features.\n\n\nVideo\nFigure 1: Overview of the main voiceR features"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#voicer-prerequisites",
    "href": "posts/2023/voiceR-R-package/index.html#voicer-prerequisites",
    "title": "My first R package: voiceR",
    "section": "voiceR Prerequisites",
    "text": "voiceR Prerequisites\n\nInstalling voiceR\nGetting started with voiceR is a straightforward process. Begin by installing the package from CRAN using the following command:\n\ninstall.packages(\"voiceR\")\n\nOnce you’ve successfully installed the package, you’re ready to embark on your voice analytics journey using voiceR.\n\n\nRequired File Name Structure\nvoiceR relies on a specific file naming convention to ensure seamless processing of audio files. This convention is comprised of up to three components of metadata about the file, two of which are optional:\n\nID: A unique identifier for the speaker or recording.\nCondition (optional): The experimental condition or another grouping variable.\nDimension (optional): Additional survey or experiment information, such as additional conditions.\n\nThe different file name components should be separated by a non-alphanumeric character, such as an underscore (_).\nvoiceR extracts these components to provide additional information about the audios and enable comparisons between groups.\nOrder of the components is not important, as long as you identify the correct file name pattern structure. For example, the following file names are all valid:\n\n12345_happy_male.wav (ID_Condition_Dimension)\n123bcf.wav (ID)\nCovidPositive_Patient1.wav (Condition_ID)\n\n\nUsing the Null Placeholder\nIf there are parts of the file name that are not any of the required components, you can use the Null placeholder to avoid them. For example, if you have additional information in the file name that does not belong to any of the categories that voiceR processes, you can use the Null placeholder to ignore that information.\nFor example, imagine you have a file named Audio_Participant345_Happy.wav. The Audio component of the file name is not required, so you could define the following pattern: Null_ID_Condition. This file name would still be valid, and voiceR would ignore the first component given that we used the Null placeholder.\nFigure 2 demonstrates how the voiceR package uses the name pattern structure to identify the different components.\n\n\n\nFigure 2: Examples of File Name Patterns"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#comprehensive-functionality",
    "href": "posts/2023/voiceR-R-package/index.html#comprehensive-functionality",
    "title": "My first R package: voiceR",
    "section": "Comprehensive Functionality",
    "text": "Comprehensive Functionality\nvoiceR offers a suite of functions designed to simplify the voice analytics process. These functions cover reading and preprocessing audio files, automatic feature extraction, visualization of results, and even automatic report generation.\n\n\nReading Multiple Audio Files\nThe initial step in the audio analytics process involves reading designated audio files. The readAudio() function in the voiceR package achieves this seamlessly, systematically processing all audio files in a specified directory and its subdirectories if specified. Users can customize this function by providing a file path (path) and an optional character vector to filter for specific patterns (filter). Upon execution, this function efficiently imports audio files into R, generating a comprehensive list of Wave objects, each representing an imported audio file.\n\n\nPreprocessing Multiple Audio Files\nFollowing successful import, preprocessing becomes imperative. The preprocess() function in voiceR automates this process by normalizing amplitude and eliminating background noise from a list of Wave objects (audioList). Two optional logical parameters, normalizeAmplitude and removeNoise, allow users to tailor the preprocessing scope. Default settings include both amplitude normalization and noise removal. While suitable for most scenarios, advanced users can integrate functions from other packages, such as tuneR’s extractWave(), for more intricate preprocessing. The output is a preprocessed list of Wave objects, which can be stored locally using the saveAudio() function.\n\n\nAutomatic Feature Extraction for Multiple Audio Files\nThe pivotal autoExtract() function facilitates the extraction of vocal features from raw or preprocessed audio files. Operating in two modes, it can either automatically read and analyze audio files based on a specified path and optional patterns or analyze a pre-existing list of audio files in the R environment. The function produces a table containing key audio features for each analyzed file, such as duration, voice breaks percentage, amplitude envelope root mean square, average loudness, average pitch, pitch standard deviation, average entropy, and average Harmonics to Noise Ratio.\n\n\nVisualizing Results\nGiven the wealth of information produced by autoExtract(), effective visualization is paramount. The voiceR package offers two specialized functions for this purpose:\n\nnormalityPlots(): Generates density plots for each audio feature, facilitating normality assessment through the Shapiro-Wilk test.\ncomparisonPlots(): Produces box plots, aiding in the comparison of audio features across different conditions or dimensions. These plots include relevant statistical tests based on data normality.\n\nThese visualization functions, seamlessly integrated into the voiceR package, enhance the interpretability of audio data, enriching the depth and breadth of analysis.\n\n\nAutomatic Report Generation\nDespite the automation provided by voiceR functions, thorough documentation of primary findings remains essential. The autoReport() function addresses this need by utilizing autoExtract() output to generate an HTML report. This report encapsulates key vocal features of the analyzed audio files, including density plots, box plots, and automatically generated APA-formatted text and tables, highlighting differences between conditions or dimensions."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#explore-the-voicer-shiny-app",
    "href": "posts/2023/voiceR-R-package/index.html#explore-the-voicer-shiny-app",
    "title": "My first R package: voiceR",
    "section": "Explore the voiceR Shiny App",
    "text": "Explore the voiceR Shiny App\nFor the ultimate user-friendly experience, voiceR offers the voiceRApp() function. By invoking this function, you can launch the voiceR Shiny app, simplifying the selection and subsequent analysis of multiple audio files. It provides a dynamic view of results and the ability to download a comprehensive report summarizing key findings. Figure 3 provides a snapshot of the voiceR app’s initial screen.\n\n\n\nFigure 3: voiceR shiny app"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#dive-deeper-into-voicer",
    "href": "posts/2023/voiceR-R-package/index.html#dive-deeper-into-voicer",
    "title": "My first R package: voiceR",
    "section": "Dive Deeper into voiceR",
    "text": "Dive Deeper into voiceR\nFor a detailed understanding of voiceR’s capabilities and functionalities, consult the package documentation here."
  }
]