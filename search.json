[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge\nI am a technophile and data enthusiast, passionate about harnessing the potential of data to analyze competitive landscapes and derive strategic market insights, which provide actionable guidance, ultimately empowering organizations to gain a competitive advantage. Equally important to me is the art of effectively communicating these insights, no matter how intricate they may be.\nI possess 7 years of academic research experience, during which I explored how to leverage state-of-the-art data-driven methods to gain a deeper understanding of consumers and markets. Throughout this period, I have also assumed leadership roles and actively contributed to several multifaceted industry consultancy endeavors, primarily in the finance and tech sectors, in which I helped organizations enhance their understanding of consumer purchasing behavior and the complex dynamics that shape their markets."
  },
  {
    "objectID": "index.html#fran",
    "href": "index.html#fran",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nApr 7, 2022\n\n\nStatistics Foundations: Confidence intervals\n\n\n\n\nDec 28, 2021\n\n\nStatistics Foundations: Sampling error\n\n\n\n\nSep 25, 2021\n\n\nStatistics Foundations: Populations and Samples\n\n\n\n\nMay 19, 2021\n\n\nWhy potential inflation could lead to a financial crisis?\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2021-12-28-statistics-foundations-sample-error/index.html",
    "href": "posts/2021-12-28-statistics-foundations-sample-error/index.html",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021-12-28-statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "href": "posts/2021-12-28-statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021-12-28-statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "href": "posts/2021-12-28-statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "title": "Statistics Foundations: Sampling error",
    "section": "Examining Sampling Errors in Data Summarization: The Case of the Mean",
    "text": "Examining Sampling Errors in Data Summarization: The Case of the Mean\nAt the core of statistical analysis lies the foundational task of data summarization—a process that condenses data into a concise and understandable format. This fundamental procedure yields a clear and easily comprehensible overview of the data, facilitating a straightforward grasp of its essential characteristics.\nAmong these essential characteristics, two prominently stand out: central tendency and variability. Central tendency relates to the value around which the different data points cluster around. Conversely, variability quantifies the extent to which data points deviate from this central point, providing insights into the dispersion or spread of data relative to its central location. For instance, the mean serves as an example of a central tendency measure, while the standard deviation exemplifies a variability measure.\nTo visually illustrate the impact of sampling error, we will once again utilize the Kaggle dataset used in our previous post. This dataset contains information on 2,000 supermarket customers, including their age, annual income, and education level. For the purposes of this analysis, we will assume that this dataset represents our entire customer population.\nLet’s envision a scenario: our objective is to rapidly glean insights into the annual income of our customers. One straightforward strategy to achieve this is by computing the mean income, which furnishes us with a succinct metric representing the central tendency around which the majority of our customers’ annual incomes gravitate. In this endeavor, we observe that the mean annual income stands at a noteworthy $120,950, serving as a prominent reference point around which the annual incomes of our customers tend to concentrate.\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\nSampling 40 customers and calculating their annual income mean\nIn this hypothetical case, we possess information about the sample. Consequently, we can obtain information about our population without any error by directly observing it. Therefore, now we know that our population has an average annual income of $120,950. However, in real-life scenarios, and as previously said, obtaining data for the whole population may be unfeasible or even not possible. For this reason, we will assume that we extract a random sample of 40 customers and compute the mean annual income from this sample.\n\n\n\n\n\nFigure 2: Customer annual income distribution for our sample with 40 observations (thousands, $)\n\n\n\n\nAs observed, in Figure 2, the mean value within this sample diverges from that of the broader population. Specifically, the mean for this sample stands at $137,320, contrasting with the population mean of $120,950. This difference amounts to $16,370, and it encapsulates what we commonly refer to as “error.” Notably, in this instance, we possess knowledge about the population, allowing us to discern this difference.\nFor this reason, the terminology we use to describe the metrics summarizing the data characteristics varies depending on whether they are computed within the population or a sample. In the former case, they are referred to as parameters, whereas in the latter, they are known as statistics. In statistical notation, parameters are typically denoted by Greek letters, such as \\(\\sigma\\) for the standard deviation or \\(\\mu\\) for the mean, while statistics are denoted by Latin letters, such as \\(m\\) for the mean and \\(s\\) for the standard deviation.\n\n\nRandomness and sampling: Extracting several means of 40 observations\nMoreover, it’s crucial to note that this sample was derived through a process of random selection. In other words, we randomly picked 40 customers from our population, ensuring that each customer had an equal likelihood of being included. This randomness implies that if we were to generate another sample of 40 customers, it would be improbable for this new sample to mirror the exact composition of the previous one or yield the same mean.\nFigure 3 illustrates the annual income distribution of various samples, each consisting of 40 cutomers randomly selected from our initial population (including the sample we previously examined). It becomes evident that the distribution undergoes fluctuations across these diverse samples. Consequently, this variability gives rise to a spectrum of computed means, ranging from as low as $106,600 to as high as $137,320.\n\n\n\n\n\nFigure 3: Customer annual income distribution for different samples of 40 customers (thousands, $)\n\n\n\n\n\n\nDigging deeper: Exploring mean customer annual income with 10,000 different 40 samples\nTo deepen our comprehension of the variance in computed means, we embark on a more extensive analysis by replicating the previous procedure but on a much larger scale: generating precisely 10,000 samples, each composed of 40 individuals. For every one of these 10,000 samples, we compute the mean annual income. The resulting distribution of these 10,000 means, each originating from distinct samples of 40 individuals randomly selected from our complete population, is visually represented in Figure 4 through a histogram.\n\n\n\n\n\nFigure 4: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nFigure 4 reveals significant insights. Notably, there is a substantial variation in the computed average annual incomes across the various samples, spanning an extensive spectrum from $100,684 to $145,543. This disparity translates into an error range spanning from -$20,266 to $24,593 in contrast with the population’s mean.\nNonetheless, an intriguing revelation emerges from this analysis. Despite the marked variability in sample means, the overall average of these mean annual incomes, drawn from distinct samples, precisely mirrors the population average. This observation means that the average incomes for the different samples consistently cluster around this point.\nMoreover, it is worth noting that the distribution of annual income means extracted from these various samples adheres to a bell-shaped pattern, commonly known as a normal distribution. This pattern signifies that as we move farther away from the population average, the number of observations gradually diminishes.\nTaken together, this implies that, in most instances, the mean annual income estimated from our sample tends to be closer rather than farther away from the population’s mean annual income. Nevertheless, it’s important to acknowledge that there are still situations where significant deviations from the sample mean can occur. The key concern here lies in the fact that if we lacked information about the population and solely possessed a sample with an annual income mean of $145,543, we might mistakenly conclude that, on average, our customers are wealthier than they actually are.\n\n\nIncreasing sample size and analyzing mean annual income distribution of 10,000 samples\nAs previously mentioned, small samples encounter challenges in capturing the subtleties present within the population. Consequently, the larger the sample size, the more effectively we can apprehend these nuances. To illustrate this, we investigate how the variance of computed means changes when we collect samples of 150 customers, as opposed to the previous samples of 40 customers.\nOnce again, we generate 10,000 samples, each containing 150 customers, and calculate the mean annual income for each of these samples. Subsequently, we visualize the distribution of these mean annual incomes for these larger samples by creating a histogram.\nFigure 5 provides a visual comparison between the distributions of means computed using 10,000 samples, each with 40 observations, and another set of samples, each comprising 150 observations.\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers vs 10,000 samples of 150 customers (thousands, $)\n\n\n\n\nFigure 5 provides insights akin to those observed with 40 observations: the distribution of means exhibits a bell-shaped pattern, with the average closely approximating the population mean. However, a significant distinction emerges: the range within which sample means deviate from the overall mean, equivalent to the population average, is notably narrower. In essence, the variability is considerably reduced, indicating that the margin for error when using samples of 150 observations is substantially smaller than that with samples of 40.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nIn the previous examples, we’ve observed that when we calculate means from various samples, these means tend to follow a particular pattern – a bell-shaped distribution known as a normal distribution. This is not just a coincidence; it’s a fundamental concept in statistics called the Central Limit Theorem.\nThe Central Limit Theorem tells us that, regardless of the original shape of the data distribution, when we repeatedly draw samples of sufficient size from that data and calculate their means, those sample means will follow a normal distribution. This is a powerful idea because it allows us to make certain assumptions and conduct statistical analyses even when we don’t know the shape of the population’s distribution.\n\n\n\nIn this scenario, the distribution of means spans from $109,374 to $132,064, resulting in an error range of -$11,576 to $11,114 relative to the population mean. This range is significantly tighter compared to the error range obtained from samples of 40, where the deviation ranged from -$20,266 to $24,593."
  },
  {
    "objectID": "posts/2021-12-28-statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "href": "posts/2021-12-28-statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "title": "Statistics Foundations: Sampling error",
    "section": "Measuring the sampling error",
    "text": "Measuring the sampling error\nThrough the repetitive extraction of samples of consistent size from a given population, as demonstrated in our previous examples (with both 40 and 150-sized samples), we gain valuable insights into the potential magnitude of errors associated with samples of a particular size.\nAs we’ve witnessed, the average of multiple means calculated from samples of identical size closely aligns with, or is essentially identical to, the true mean of the population. Consequently, the spread or dispersion of these computed means from this point provides a measure of the magnitude of the sampling error. Put simply, the variability in the mean derived from multiple samples of equal size offers a quantifiable measure of the magnitude of the sampling error for samples of that particular size. This measure is commonly known as the standard error of the mean (which we will abbreviate as SEM).\nMathematically, we can express this as the standard error of the mean being equal to the standard deviation of the means of the different samples. Specifically, we prefer using the standard deviation rather than the variance because the former has the same units as the mean, while the latter has squared units. For instance, in the case of annual income, the units for variance would be in dollars squared (\\(\\$^2\\)), while for the standard error, it’s just in dollars ($). In other words:\n\\(\\sqrt{Var(\\bar{X})} = SEM\\)\nThis expression can be translated into the following form:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\nHere, \\(σ\\) represents the population standard deviation and \\(n\\) is the sample size. Establishing an inverse relationship between the standard error and the sample size: as the sample size increases, the standard error decreases. This principle aligns with our intuitive understanding, as seen in the previous post for the statistics foundations series, and as visually depicted in Figure 5.\n\n\n\n\n\n\nStandard Error Derivation\n\n\n\n\n\nLet’s recall that the mean of any variable is equal to the sum of the values of each observation of that variable divided by the total number of observations (which equals our sample size): \\(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\). Therefore, we can rewrite the previous formula as follows:\n\\(\\sqrt{\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nWe also know that the variance of a random variable multiplied by a constant “a” is equal to the variance of that variable multiplied by the square of that constant, i.e., \\(Var(aR) = a^2Var(R)\\), where \\(a\\) is a constant, and \\(R\\) is a random variable. This means that:\n\\(\\sqrt{\\frac{1}{n^2}\\text{Var}\\left(\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nAdditionally, when dealing with a set of pairwise independent random variables (where the variability in one doesn’t depend on the others, as in our case), the variance of their sum is equal to the sum of their individual variances, i.e., \\(\\text{Var}[R_1 + R_2 + \\cdots + R_n]\\) \\(=\\) \\(\\text{Var}[R_1] + \\text{Var}[R_2] + \\cdots + \\text{Var}[R_n]\\), where \\(R_1, R_2,…, R_n\\) are pairwise independent random variables. This allows us to rewrite the formula for SEM as:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}Var(X_i)} = SEM\\)\nLet’s remember that our individual variables, denoted as \\(X_i\\), come from a population with variance equal to \\(\\sigma^2\\). So, our formula becomes:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}\\sigma^2} = SEM\\)\nSince we’re summing up \\(n\\) identical values, we can simplify further:\n\\(\\sqrt{\\frac{1}{n^{2}}n\\sigma^2} = SEM\\)\nUltimately, this can be further simplified to:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\n\n\n\nApplying these formulas, we can now calculate the Standard Error of the mean for sample sizes of 40 and 150. When we compute the standard deviation of the means obtained from multiple samples of 40 customers, we obtain a value of 5.86. Conversely, for samples of 150 customers, we obtained a value of 3.01, which is approximately two times smaller in terms of standard error.\nAlternatively, we can utilize the formula that links the standard error to the population’s standard deviation and the sample size, represented as \\(\\frac{\\sigma}{\\sqrt{n}}\\). Given our population’s standard deviation for annual income is 38.11, dividing this by \\(\\sqrt{40}\\) yields a value of 6.03 for a sample size of 40, while dividing it by \\(\\sqrt{150}\\) results in a value of 3.11 for a sample size of 150.\nHowever, it’s important to note that there are disparities between the results obtained from these two approaches. This is because the first formula relies on a finite number of samples (10,000 in this case), and to obtain equivalent values, the number of samples would need to tend towards infinity, meaning a significantly larger amount of samples.\n\nEstimating the standard error\nUp until now, we have seen that we can compute the standard error of the mean by taking multiple samples of the same size from the population, calculating their means, and extracting the variability of such means.\nYet, let’s face it—in the real world, resources are finite, and repeatedly plucking samples from the population to estimate the standard error can be an extravagant expenditure of these precious assets. Instead, it’s often a more judicious allocation of resources to channel our efforts into amassing a larger sample. As we’ve come to appreciate, a larger sample which better captures the nuances of the population, decreasing the sampling error.\nIn addition to the aforementioned method, we’ve also explored an alternative approach for calculating the standard error—one that bypasses the need to repeatedly extract multiple samples from the population. This alternative method involves dividing the population’s standard deviation by the square root of the sample size (\\(\\frac{\\sigma}{\\sqrt{n}}\\)). However, it’s essential to note that this formula hinges on having access to information about the entire population. This requirement underscores the very reason why we find ourselves seeking to compute the standard error in the first place—a challenge born out of the impracticality of obtaining data for the entire population, compelling us to work with samples and consequently introducing the sampling error.\nNonetheless, it’s crucial to remind ourselves that the core objective when working with a sample is to glean insights into the larger population and derive meaningful conclusions from it. Within this context, it’s reasonable to assume that the standard deviation we observe within our sample (\\(s\\)) can serve as a dependable proxy for the population’s standard deviation (\\(\\sigma\\)). This assumption empowers us to substitute the population standard deviation in the traditional formula (\\(\\frac{\\sigma}{\\sqrt{n}}\\)) with the sample standard deviation (\\(s\\)) derived from that very population:\n\\(SE \\approx \\frac{s}{\\sqrt{n}}\\)\nBy making this substitution, we arrive at an approximation for the standard error. It provides us with a measure of the potential error we may encounter when drawing conclusions from a sample, all without the need for complete information about the population."
  },
  {
    "objectID": "posts/2021-12-28-statistics-foundations-sample-error/index.html#a-final-note",
    "href": "posts/2021-12-28-statistics-foundations-sample-error/index.html#a-final-note",
    "title": "Statistics Foundations: Sampling error",
    "section": "A final note",
    "text": "A final note\nThroughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. Throughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. When we compute these statistics from a sample, the values we obtain can deviate from those of the population, consequently introducing an element of error into our analyses.\nHowever, it’s crucial to acknowledge that the formulas we’ve previously derived for calculating the standard error aren’t universally applicable to all statistics. These formulas have been derived with the mean as their reference point. Nevertheless, it’s worth noting that the fundamental concept behind deriving formulas for computing the standard error for other statistics remains consistent: It is based on the variability of a specific statistic obtained from various samples of the same size."
  },
  {
    "objectID": "posts/2021-12-28-statistics-foundations-sample-error/index.html#summary",
    "href": "posts/2021-12-28-statistics-foundations-sample-error/index.html#summary",
    "title": "Statistics Foundations: Sampling error",
    "section": "Summary",
    "text": "Summary\n\nSampling is necessary because obtaining data for the entire population is often impractical.\nThe use of samples introduces the challenge of sampling errors.\nSampling errors arise because small samples cannot capture all the nuances present in the population.\nThis leads to variations in common measures like the mean between the sample and the population.\nTo illustrate this, we simulated the extraction of 10,000 samples of the same size from our exemplary population and observed that:\n\nThe mean from different samples exhibits variability.\nDespite this variability, the average of mean values from various samples tends to closely align with the population average.\nA normal distribution pattern is evident in the distribution of annual income means from different samples, indicating that samples with means close to the population mean are more likely.\nLarger sample sizes reduce the standard error and yield more accurate estimates, as they better capture the population’s intricacies.\n\nWhen extracting multiple samples of the same size from a population and calculating their means, the average of these sample means corresponds to the population mean. This allows us to quantify the degree of error associated with that sample size by measuring their variability, i.e., how much they deviate from the population mean.\nStandard error of the mean (SEM) is mathematically linked to the population’s standard deviation and sample size.\nIn practice, we estimate the SEM using our sample’s standard deviation.\nIncreasing the sample size results in a smaller standard error.\nStandard error serves as a valuable tool for quantifying the precision of sample-based estimates and is essential for robust statistical analysis.\n\n\n\nCode\n## Reading the \"population\" and visualizing it\nlibrary(gganimate)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.25, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n  \n## Sampling 40 customers and calculating their annual income mean + visualizing their distribution\n\nset.seed(150)\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40), ]\n\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data_sample$Income)\n\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.15, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n   \n ## Extracting several 40 customers and calculating their annual income mean + visualizing their distribution\n \n customer_data_samples &lt;- customer_data_sample\ncustomer_data_samples$Average_Sample &lt;- \"Sample 1\"\n\nseeds &lt;- seq(300, 5500, length.out = 8)\n\ni &lt;- 2\nfor(seed in seeds) {\n    \n    set.seed(seed)\n    customer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40),]\n    row.names(customer_data_sample) &lt;- NULL\n    average_income &lt;- round(mean(customer_data_sample$Income), 2)\n    customer_data_sample$Average_Sample &lt;- paste0(\"Sample \", i)\n    customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n    i &lt;- i + 1\n}\n\n\ndata_income_average &lt;- customer_data_samples %&gt;%\n  summarise(meanIncome = mean(Income), .by = Average_Sample)\n\n\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Average_Sample)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = meanIncome * 1.25, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Average_Sample, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n## Extracting 10,000 samples of 40 observations and calculate their mean + visualize the distribution of the means\nset.seed(3)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 9, label = paste0(\"Mean: \", round(average_income, 2))) \n\n## Extracting 10,000 samples of 150 observations and calculate their mean + visualize the distribution of the means\n\nset.seed(1500)\n# Create an empty numeric vector of length 10000 named 'sample_means'\nsample_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 150 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 150)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample_means), 2)\nsample_means_40 &lt;- data.frame(Income = sample40_means)\nsample_means_40$Observations &lt;- \"Samples of 40 Observations\"\nsample_means_150 &lt;- data.frame(Income = sample_means)\nsample_means_150$Observations &lt;- \"Samples of 150 Observations\"\nsample_means &lt;- rbind(sample_means_40, sample_means_150)\n\n\ndata_income_average &lt;- sample_means %&gt;%\n  summarise(meanIncome = mean(Income), .by = Observations)\n\n\np &lt;- ggplot(sample_means, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = average_income * 1.08, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 10)\nanim"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\nFigure 1. S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\nFigure 1. S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A time of low interest rates",
    "text": "A time of low interest rates\nMoreover, these economic disturbances led governments to act severely. Keynesian policies for the activation of the economy began to play a fundamental role. To encourage consumption, interest rates were lowered to near historic lows. Figure 2 displays the US 10 year note bond yield between 1920 and 2020. In this figure, we can see how the dotcom crash pushed down this yield. However, after that it started recovering until July 2007, when the financial crisis started, after that, that yield continued a downward trend.\n\n\n\nFigure 2. US 10 Year Note Bond Yield"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Stocks as the only attractive investment vehicle",
    "text": "Stocks as the only attractive investment vehicle\nSuch a long period of low interest rates inflated stock prices. Figure 3, shows how cyclically adjusted S&P 500 price-to-earnings ratio has been rising during the last decade, being quite high in comparison to other periods of time (specially before the dot-com bubble). Stock prices started trading at a premium, because there were no attractive alternative investments, this channelled much of the liquidity into equities (bonds with almost no interest or even negative interests were not an attractive investment anymore). Moreover, this low interest rate setting has prompted greater investor leverage, due to its low cost. Hence, low interest rates justify high stock prices, since stocks are highly attractive relative to bonds and debt is stimulated due to its reduced cost.\n\n\n\nFigure 3. Cyclically adjusted S&P 500 price-to-earnings rations"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "An unexpected event: A global pandemic",
    "text": "An unexpected event: A global pandemic\n2020 was not a good year, this year will always be remembered as the year of the COVID. COVID brought many changes in our lives, which undoubtedly had an impact on the economy. As a result, governments continued to pursue stimulative policies and interest rates remained at very low levels.\nOne of the best illustrations of those stimulative policies is the amount of dollars printed in 2020: 21% of the United States dollar was printed in 2020, as shown in Figure 4. This large injection was used for both direct and indirect assistance in the COVID situation. This, at the same time, indirectly channelled part of this aid to the financial markets, increasing their value.\n\n\n\nFigure 4. Annual Money Stock Growth (Trillions of USD)"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A double-edged sword",
    "text": "A double-edged sword\nInjecting money to the economy is always something controversial. Through the law of supply and demand it is easy to infer that a considerable increase in supply (without a similar increase in demand) will reduce the price of a good. In money, when this happens, we say that the money loses value, i.e. one monetary unit can acquire fewer products. In other words, this is what we call inflation. Even Warren Buffet has shown concern for inflation in the past month:\n\n“We are seeing very substantial inflation […] We are raising prices. People are raising prices to us and it’s being accepted.”\n\nThis raise on prices can be already tracked on several indices such as Bloomberg’s agriculture index, shown in Figure 5 and also in the annual single-family home price, as shown in Figure 6 (Figure 6 Source).\n\n\n\nFigure 5. Bloomberg agriculture index\n\n\n\n\n\nFigure 6. Annual Single-family home price\n\n\nThe appearance of inflation means that interest rates should rise. Something that was already pointed by Janet Yellen at the start of this month:\n\n“It may be that interest rates will have to rise somewhat to make sure that our economy doesn’t overheat”\n\nBut what would happen if inflation persists and interest rates have to be risen? Remember that I previously said that stocks are trading at a premium due to the lack of attractive alternative investments. This would no longer be true and this premium would no longer be a thing. In addition, an increase in interest rates would reduce the attractiveness of leverage, thereby encouraging investor’s deleverage. And, thus, we should expect a decrease in the stock price."
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "The market can not be timed",
    "text": "The market can not be timed\nEven with all these indicators, it is difficult to say whether this will happen in the short to medium term. There are many factors which could deter inflation away and interest rates low. As an example, Berkshire Hathaway has been stacking cash during the last years, as shown in Figure 7, playing a slightly more defensive position. This may be due the fact that they already saw that low interest rates during the last decade were driving the stock market at high prices. However, interest rates are still low and during that time the stock market has continued rising.\n\n\n\nFigure 7. Berkshire’s Cash Holdings"
  },
  {
    "objectID": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "href": "posts/2021-05-19-why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Summary",
    "text": "Summary\n\nThe first decade of the 2000s was characterized by two economic crisis, which defined an economy with very low interest rates\nLow interest rates increased stock market prices, since there were no attractive alternative investments and leverage was cheap. Thus, stock markets traded at a premium.\nThe Covid crisis led to a massive injection of money into the economy.\nThis money injection is a double-edged sword which may have brought an exuberance illusion awakening a ghost that has been dormant in recent years, inflation.\nThe emergence of inflation would imply an increase in interest rates.\nAn increase in interest rates would mean that the premium paid for stocks would be lost.\nDespite all these facts, timing the market is no easy task. And thus, other factors could keep inflation away and interest rates away, prolonging this situation."
  },
  {
    "objectID": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html",
    "href": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "href": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "href": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "The Ideal of Population and the Reality of Sampling",
    "text": "The Ideal of Population and the Reality of Sampling\nTo comprehensively explore and enhance our understanding of a given issue, it is ideal to possess data pertaining to all entities impacted by that issue. For instance, in our pursuit of gaining deeper insights into our customer base, the ideal scenario involves having access to data for every single one of our customers. In this comprehensive dataset, we would find detailed information regarding each customer’s age, income, purchasing history, and other relevant attributes. Such a dataset would empower us to attain a holistic understanding of our customer base’s profile. In statistical terms, when we allude to data encompassing all entities affected by the issue of interest, we are referring to the population. Naturally, this population is contingent upon the specific focus of interest. For instance, if our objective shifted from understanding the profile of our overall customer base to that of our Swiss customers specifically, our population would comprise solely our Swiss customer subset.\nNevertheless, acquiring data for every single entity influenced by the matter of interest may often prove to be impractical, primarily due to the substantial expenses associated with such an undertaking or its constantly changing nature. Therefore, in the field of statistics, we operate with subsets derived from this population. Ideally, these subsets are constructed in such a way that each entity within them has an equal probability of being selected. Consequently, the resultant subset, known as a sample, is smaller in scale and serves as a reliable and representative image of the broader population. The fundamental concept underlying this approach is that through the observation and analysis of this sample, we can draw meaningful conclusions about the entire population, a process known as inference.\n\nLimitations and Potential Bias in Sampling\nNonetheless, utilizing samples entails operating with an imperfect representation of the population—a mere approximation that may deviate from the true population characteristics. Even when each individual possesses an equal probability of selection, the random nature of the process can result in the overrepresentation or underrepresentation of certain specific types or groups of entities within our sample, thereby potentially leading to skewed findings and conclusions."
  },
  {
    "objectID": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "href": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "Sampling in Practice: Exploring Our Consumer Base Annual Income",
    "text": "Sampling in Practice: Exploring Our Consumer Base Annual Income\nTo illustrate these concepts, let’s delve into an example using a dataset from Kaggle. This dataset comprises data about 2,000 supermarket customers, encompassing a range of characteristics such as age, annual income, and education level. For the sake of this demonstration, let’s envision that this dataset encapsulates information about every single one of our customers, effectively serving as a representation of our customer population. Now, let’s suppose our objective is to gain a more profound insight into the annual income distribution among our customers.\n\nAnnual Income Distribution for Our Population\nHence, we can promptly delve into the examination of the data distribution, which we can observe through a histogram, depicted in Figure 1. This analysis unveils that the distribution of customer incomes exhibits an approximate normality, featuring an average annual income of $120,950 with a moderate degree of variability (Standard Deviation = $38,110). This implies that a substantial proportion of customers have incomes that closely align with the mean value, while fewer customers fall within the income extremes. Additionally, it is worth noting a slight rightward skew in the plot, indicating a minority of individuals with substantially higher incomes compared to the majority.\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\n\n“Collecting” a Small Customer Sample and Exploring their Annual Income\nHaving briefly explored the distribution of our customers’ annual income population, we must acknowledge the practical challenge of obtaining data from the entire population. Therefore, we opt to acquire a representative sample, a feasible alternative. In this scenario, we decided to select an easily obtainable sample of 20 customers, each having an equal probability of being included in the sample. Following the acquisition of this sample, we analyze their annual income distribution, which can be observed in Figure 2, revealing several noteworthy disparities.\nFirstly, we observe that certain income brackets, present in the population data, remain absent in our sample. Additionally, we notice a higher proportion of high-income customers in comparison to the population. These disparities culminate in a higher average customer annual income (Mean = $137,320) and increased variability (Standard Deviation = $47,770) within the sample. Consequently, if we were to draw inferences based on this data, our conclusions would suggest that individuals in the sample exhibit a higher average income and greater income variability compared to the population, thus drawing into error.\n\n\n\n\n\nFigure 2: Customer annual income distribution for our sample with 20 observations (thousands, $)\n\n\n\n\nDespite these disparities, our sample offers us an initial glimpse into the broader income distribution of the entire population. While our sample may not perfectly mirror the population due to its size and inherent limitations, it serves as a foundational reference point for comprehending income patterns within the larger group. It grants us a preliminary understanding of the income ranges, tendencies, and variations we can anticipate when considering the overall population’s income distribution. However, it’s essential to acknowledge the presence of these disparities and recognize that they may impact the conclusions we can draw regarding the population.\n\n\nCapturing Nuances Better with Increased Sample Size\nThese observed disparities are unsurprising, given the inherent limitations of capturing the intricacies of our data with a small sample of just 20 consumers. Consequently, we observe the absence of individuals in various income brackets and a skewed composition compared to our population (which, in this hypothetical case, we have knowledge of, but in practice, we might not).\nOne might naturally question whether increasing the sample size could enhance the richness of our sample and consequently enable us to better capture the nuances present in the population. This would ultimately result in a more faithful representation. To investigate this, we will generate samples of varying sizes, specifically seven additional samples consisting of 40, 80, 120, 150, 300, 500, and 1000 randomly selected consumers from our population. Figure 3 visually illustrates, through histograms, how the distribution changes for each of these sample sizes, including the initially created one with 20 consumers.\n\n\n\n\n\nFigure 3: Customer annual income distribution for varying sample sizes (thousands, $)\n\n\n\n\nIn this Figure, we can discern that larger sample sizes excel in capturing the subtleties inherent in the population’s data distribution. Notably, the distribution of bigger samples closely mirrors that of the population, with fewer missing income ranges and diminished disparities.\nWhen we work with larger samples, we effectively broaden our scope of observation, incorporating a more diverse range of data points. This expanded sample size minimizes the influence of random variation and offers a more robust representation of the population. In essence, larger samples provide a more comprehensive cross-section of the population, enhancing our ability to accurately capture the underlying patterns, variations, and nuances present in the data.\nNow, you might be wondering: what constitutes the ideal sample size? There is not a one-size-fits-all answer; instead, the sample size should strike a balance. It must be large enough to capture the nuances of the population while still being feasible to acquire within budget and time constraints. It is crucial to recognize that a sample will inherently exhibit differences from the population, a fundamental aspect of statistical analysis."
  },
  {
    "objectID": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#summary",
    "href": "posts/2021-09-25-statistics-foundations-population-and-sample/index.html#summary",
    "title": "Statistics Foundations: Populations and Samples",
    "section": "Summary",
    "text": "Summary\n\nStatistics empowers us to glean valuable insights from data, enriching our comprehension of issues of interest.\nIdeally, a complete understanding of any issue necessitates data from every entity involved, referred to as the population.\nPractical constraints often require us to work with smaller, representative subsets, known as samples, where each entity in the population has an equal chance of inclusion.\nSamples, while essential for making inferences about the population, have inherent limitations due to their size, as they can’t fully capture the population’s intricacies, introducing errors into our inferences.\nLarger samples excel at capturing population nuances, offering a more faithful representation of its characteristics.\n\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gganimate)\nlibrary(scales)\n\n# Read customer data from CSV file and adjust income values\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income / 1000\n\n# Calculate the average and standard deviation of the income for the entire population\naverage_income_population &lt;- mean(customer_data$Income)\nstd_deviation_population &lt;- sd(customer_data$Income)\n\n# Create and display a histogram for the entire population's income distribution\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, \n           label = paste0(\"Mean: \", round(average_income_population, 2), \"\\nSD: \", round(std_deviation_population, 2))) \n\n# Set a random seed for reproducibility\nset.seed(150)\n\n# Create a random sample of 20 observations from the population\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 20), ]\n\n# Calculate the average and standard deviation of the income for the sample\naverage_income_sample &lt;- mean(customer_data_sample$Income)\nstd_deviation_sample &lt;- sd(customer_data_sample$Income)\n\n# Create and display a histogram for the sample's income distribution\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data_sample$Income, na.rm = TRUE) * 0.95, y = 13, \n           label = paste0(\"Mean: \", round(average_income_sample, 2), \"\\nSD: \", round(std_deviation_sample, 2))) \n\n\n\n\n\n## Create samples of different sizes and create animation with their distribution\n\n# Create an empty data frame to store the samples and add a column for observations\ncustomer_data_samples &lt;- data.frame()\ncustomer_data_samples$Observations &lt;- character(0)\n\n# Define the sample sizes\nsample_sizes &lt;- c(20, 40, 80, 120, 150, 300, 500, 1000)\n\n# Set a random seed for reproducibility\nset.seed(350)\n\n# Loop through each sample size\nfor (sample_size in sample_sizes) {\n  # Create a random sample of the specified size\n  customer_data_sample &lt;- customer_data[sample(nrow(customer_data), sample_size), ]\n  \n  # Create a label for the observations indicating the sample size\n  customer_data_sample$Observations &lt;- paste0(sample_size, \" Observations\")\n  \n  # Append the sample to the data frame\n  customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n}\n\n# Add a label for the population\ncustomer_data$Observations &lt;- \"Population\"\n\n# Append the population data to the data frame\ncustomer_data_samples &lt;- rbind(customer_data_samples, customer_data)\n\n# Convert the \"Observations\" column to a factor with custom labels\ncustomer_data_samples$Observations &lt;- factor(\n  customer_data_samples$Observations,\n  levels = c(\"20 Observations\", \"40 Observations\", \"80 Observations\", \"120 Observations\", \"150 Observations\", \"300 Observations\", \"500 Observations\", \"1000 Observations\", \"Population\")\n)\n\n# Loop through the levels of the \"Observations\" factor and update labels\nfor (i in seq_along(levels(customer_data_samples$Observations))) {\n  sample_type &lt;- levels(customer_data_samples$Observations)[i]\n  subset &lt;- customer_data_samples[customer_data_samples$Observations == sample_type, \"Income\"]\n  average_income &lt;- round(mean(subset), 2)\n  std_deviation_income &lt;- round(sd(subset), 2)\n  \n  # Update labels with mean and standard deviation information\n  if (i &lt;= 8) {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      levels(customer_data_samples$Observations)[i],\n      \"Sample\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  } else {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      \"Population\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  }\n}\n\n#Create and animate the plot\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = percent_format()) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 150, duration = 15)\nanim"
  },
  {
    "objectID": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html",
    "href": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "",
    "text": "In the previous post of the Statistics Foundations series, we explored the inherent errors associated with working with samples instead of the entire population. These errors stem from the limitations of samples in capturing the full spectrum of population nuances.\nWe delved into quantifying this error, commonly known as the standard error, by assessing the variability of a statistic derived from different samples of the same size drawn from the same population. To further explore this concept, we focused on the mean. We used the standard error definition to derive a formula that directly computes the standard error of the mean, dividing the population’s standard deviation by the square root of the sample size. However, we recognized that applying such a formula in practical scenarios can be unfeasible, as it hinges on possessing information about the entire population—information that is typically unavailable, underscoring the very reason why we employ samples in the first place.\nNevertheless, we uncovered a practical workaround by assuming that the standard deviation of our sample serves as an estimator for the population’s standard deviation. This substitution effectively transformed the formula into one that employs the sample’s standard deviation divided by the square root of its size, allowing us to estimate the standard error.\nSo far, our primary emphasis has been on understanding and quantifying the sampling error. But let’s take it a step further and connect this concept to something more practical and tangible. Imagine if we could use this knowledge to pinpoint a range within which our sample statistics are likely to fall and, by extension, where the true population parameters may lie. That’s where the notion of confidence intervals comes into the spotlight, a topic we’ll explore in this blog, with a primary focus on the mean.\nOnce more, we’ll be employing the same dataset from our previous posts, encompassing data from 2,000 supermarket customers, including details about their age, annual income, and educational level. For the purpose of this analysis, we’ll assume that this dataset accurately represents our entire customer population. As in our prior discussions, we’ll operate under the assumption that this dataset comprehensively captures information about all our customers, effectively representing our entire population."
  },
  {
    "objectID": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#distributions",
    "href": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#distributions",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Distributions",
    "text": "Distributions\nIn our previous discussions, we’ve touched upon the concept of distribution without providing a formal definition. In common parlance, this term is commonly used and readily understood, as demonstrated by phrases like “concentrated urban population distribution” or “disparities in wealth distribution within a country.” In these examples, “distribution” simply denotes how individuals or assets are spread across a specific area.\nIn statistics, the concept of distribution remains conceptually consistent. A variable’s distribution illustrates how the different values of that variable spread out and which are more prevalent. Our exploration of distributions has primarily involved the examination of a variable’s histogram — a visual representation that conveys the frequency of values within predefined intervals, commonly known as “bins”. This approach provides a direct and intuitive means of discerning the spread of values and identifying those that occur more and less frequently. For instance, let’s revisit the annual income distribution of our “population”.\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\nFigure 1 offers a visual representation of our population’s annual income distribution. This graphic, for instance, reveals that a substantial portion of our customers falls within the income range of $80,000 to $150,000, specifically encompassing 1,427 individuals, equivalent to 71.35% of our customer base.\nNow, consider a scenario where we randomly select a customer and, before checking any of their information, we make an educated guess about their income. In this situation, it’s reasonable to infer that their income is highly likely to fall within the range of $80,000 to $150,000, as the majority of our customers are concentrated in this income range.\nConsequently, distributions provide us with a framework to describe variables using the language of probability. This is why, in statistics, we often refer to them as probability distributions. To illustrate, in the previous example, we could state that the probability of a randomly chosen customer having an income between $80,000 and $150,000 is 71.35%. The connection between a variable’s values and their associated probabilities can be mathematically expressed through a function, which directly relate the variable’s values to their respective probabilities.\n\n\n\n\n\n\nDiscrete and Continuous Distributions\n\n\n\n\n\nWhen discussing probability distributions, it is crucial to distinguish between two fundamental types: continuous and discrete distributions.\nDiscrete distributions are characterized by elements that can only assume a finite number of values within a defined range. Examples of such distributions include the number of children in a family or the count of customers in a shop on a given day. These variables take on specific, countable values.\nOn the other hand, continuous distributions consist of elements that can take any value within a specified interval. While our everyday thinking and calculations often involve finite numbers, consider scenarios where precise measurements are vital, such as in pharmaceutical drug development, where even minuscule differences in weight can have significant implications. In this context, the weight of substances is treated as a continuous variable.\nIn continuous distributions, owing to their infinite range of potential values, it is not possible to precisely calculate the probability associated with a specific value. Conversely, in the case of discrete distributions, where a finite and countable set of values exists, we can accurately determine the probability associated with each individual value. In practical terms, this means that for continuous variables, we can only compute the probability of a variable falling within a certain range, while for discrete variables, we can calculate the probability of it assuming a specific value or falling within a particular range.\n\n\n\nMany probability distributions are frequently encountered and have earned distinctive names due to their importance. One such example is the uniform distribution, where every potential value of a variable is equally likely to occur. Another prominent distribution is the normal distribution, recognizable by its bell-shaped curve.\nWhile there are undeniably several other frequent probability distributions, for the purpose of this discussion, we will concentrate on the normal distribution. This emphasis is justified by its pivotal role in facilitating the translation of sample mean and sampling error, quantified by the standard error, into intervals within which we can confidently predict the likely range of the population mean. These intervals are commonly known as confidence intervals.\n\nThe Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is one of the most valuable continuous distributions, primarily because many statistics are normally distributed in their sampling distribution (as we saw in the previous post for the case of the mean).\nThe normal distribution is easily recognizable by its classic bell-shaped curve, as depicted in Figure 2. This curve resembles a perfectly symmetrical hill with a clear peak at its center, which represents the distribution’s average. As you move away from this peak, the curve gradually slopes downward and then gently turns outward. This smooth descent and outward turn reveal a pattern of how data spreads — the likelihood of observing values becomes lower as you move further away from the average, making values closer to the average more probable.\nAdditionally, the symmetry of the normal distribution means that the probabilities of finding values above and below the average are identical. In simpler terms, it implies that the chances of observing values on one side of the peak are the same as on the other side.\n\n\n\n\n\nFigure 2: Exemplary shape of a normal distribution\n\n\n\n\n\nMean and Standard Deviation: Shaping the Normal Distribution\nThe entire shape of a normal distribution can be effectively described using just two key parameters: the mean and the standard deviation.\n\n\n\n\n\n\nNotation\n\n\n\n\n\nGiven that the mean and the standard deviation effectively describe the entire shape of a normal distribution, we often refer to it is as \\(N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the variable’s mean and \\(\\sigma\\) is the variable’s standard deviation.\nTherefore, we can denote that a random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as:\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\n\n\nThe mean, as mentioned earlier, represents the distribution’s center and the location of its peak. On the other hand, the standard deviation characterizes the curve’s shape. It indicates whether the curve is relatively flat or sharply peaked.\nFigure 3 offers an interactive visual representation showcasing how tweaking the mean and the standard deviation influences a normal distribution. When we adjust the mean, while keeping the standard deviation constant, we observe a fascinating phenomenon: the entire distribution shifts. An increase in the mean nudges it to the right, while a decrease causes it to veer to the left. Conversely, changing the standard deviation while maintaining the mean constant is like stretching or compressing the data. A smaller standard deviation suggests that most data points group closer to the mean, yielding a tall, slender curve. On the contrary, a larger standard deviation indicates that data points are more dispersed, resulting in a shorter, broader curve.\n\nviewof current_sd = Inputs.range(\n  [1, 5],\n  {value: 1, step: 1, label: \"Standard Deviation\"}\n)\n\nviewof current_mean = Inputs.range(\n  [0, 20],\n  {value: 0, step: 5, label: \"Mean\"}\n)\n\nfiltered = transpose(data).filter(function(normal_distribution) {\n  return current_sd === normal_distribution.sd_value &&\n    current_mean === normal_distribution.mean_value;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered,\n  Plot.binX(\n    {y: \"proportion\"},\n    {x: \"values\", \n     curve: \"natural\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n     interval : 0.75\n    }\n  )\n  \n    \n).plot({x: {domain: [-20, 40], grid: true}, y: {domain: [0, 0.3]}})\n\n\n\n\n\nFigure 3: Exemplary normal distributions with varying mean and standard deviation\n\n\n\n\n\nThe Normal Probability Density Function\nAs we’ve just observed and articulated, the core characteristics of a normal distribution revolve around its mean and standard deviation. Mathematically, the shape of a normal distribution can be portrayed through a functional relationship between the values of a normally distributed variable \\(X\\), characterized by a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\), and their probability density, known as the probability density function:\n\\(f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\)\nThe pivotal element within this formula is the exponential term \\(\\left({ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\right)\\). This term effectively communicates the rate at which the probability density diminishes as we distance ourselves from the mean (\\(\\mu\\)) — as \\(x\\) moves farther from \\(\\mu\\), the lower the probability density. It’s essential to note that a larger standard deviation (\\(\\sigma\\)) results in a reduction of the magnitude of this expression, which, in turn, moderates the pace of the probability density decay.\n\n\n\n\n\n\nProbability Densities\n\n\n\n\n\nIt is essential to differentiate between probability densities and probabilities, as these two concepts fundamentally diverge. As we have underscored previously, computing the exact probability of a continuous variable taking a specific value is unfeasible, given that continuous variables can theoretically encompass an infinite range of values.\nTo illustrate this, consider measuring an individual’s height, which may be reported as 175cm. However, if we possessed an incredibly precise measuring instrument, it might record the height as 174.9999945 cm. In practice, we typically round such measurements to a more practical form, like 175 cm, instead of expressing them as infinite decimals.\nWhen we talk about probability densities, we essentially employ a similar principle — grouping values near one another. This grouping enables us to represent the likelihood of a value falling near a specific point, such as 175cm, without claiming it is precisely 175cm. It’s important to note that probability densities, in isolation, lack a direct interpretation as probabilities. However, they are ingeniously constructed to ensure that the area beneath the density curve always maintains its interpretability as genuine probabilities.\n\n\n\n\n\nThe 68-95-99.7 Rule\nSince the “probability” of specific values in a normal distribution is dictated by the mean and the standard deviation, we can directly associate the likelihood of specific events with these two parameters, particularly concerning how many standard deviations we deviate from the mean. This relationship gives rise to a widely recognized rule, commonly known as the 68-95-99.7 rule. According to this rule, there’s an approximate probability of 68% that a particular observation falls within one standard deviation from the mean (i.e., between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\)), roughly 95% within two standard deviations from the mean (i.e., between \\(\\mu - 2\\sigma\\) and \\(\\mu + 2\\sigma\\)), and approximately 99.7% within three standard deviations from the mean (i.e., between \\(\\mu - 3\\sigma\\) and \\(\\mu + 3\\sigma\\)). In this context, the values 1, 2, and 3, representing the number of standard deviations from the mean, are often referred to as critical values, which help define specific regions in the distribution.\nIn simpler terms, this rule tells us that for a normally distributed variable, roughly 68% of observations are within one standard deviation of the mean, about 95% are within two standard deviations, and approximately 99.7% are within three standard deviations. Figure 4 provides a visual representation of this rule.\n\n\n\n\n\nFigure 4: Visual representation of the 68-95-99.7 rule"
  },
  {
    "objectID": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "href": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Sample means and the normal distribution",
    "text": "Sample means and the normal distribution\nNow that we have a better understanding of probability distributions and how they help us assess where most data points are likely to cluster, as well as to assess the probability of an unknown data point falling within a specific range, let’s revisit our example involving supermarket customers. In the previous post, we uncovered an essential concept: sample means drawn from the same population and of the same size conform to a normal distribution with its center—the distribution’s mean—aligning with the population’s mean.\n\nVisualizing the Concept\nTo provide a visual representation of this concept, Figure 5 displays a histogram that provides an overview of the means obtained from 10,000 samples, with each sample containing 40 randomly selected customers from our population. In essence, this histogram visualizes the mean sampling distribution for samples of 40 observations drawn from our population.\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\nAs evident from the figure, these sample means display a characteristic bell-shaped distribution, i.e., following a normal distribution, with the distribution mean precisely aligning with the population mean, which, in this case, is $120,950. Additionally, the standard deviation of this distribution, which is equivalent to the standard error of the mean, is $5,862.\n\n\nApplying the 68-95-99.7 Rule\nBy applying the principles of the 68-95-99.7 rule, as explained earlier, we can infer that approximately 68% of samples drawn from the population will fall within one standard deviation from the mean, about 95% within two standard deviations, and nearly 99.7% within three. Notably, the standard deviation of sample means, computed from various samples of the same size and from the same population, corresponds to their standard error, whereas the mean aligns with the population mean. Consequently, we can rephrase this as follows: about 68% of sample means will be within one standard error of the population’s mean, approximately 95% within two standard errors, and nearly 99.7% within three standard errors of the population’s mean.\n\n\nPractical Example and Intuition\nNow, let’s apply this understanding in practice. Consider a scenario where we know the standard error of the mean (SEM = $5,862), but the population mean remains uncertain. However, we want to have an idea of value the population mean could take. To do so, we select a sample of 40 customers from our population and calculate the mean of their annual income, which turns out to be $134,320.\nThis process of taking a sample and computing its mean is akin to randomly selecting a value from the distribution we discussed earlier, the mean sampling distribution. Therefore, it is highly likely that the value we obtain from this sample will be found within three standard errors of the population’s mean, as approximately 99.7% of sample means would be found within this range. This potential difference that we could have from the population mean is commonly known as the margin of error.\nConsequently, we can reverse the previous statement, affirming that the population mean is very likely to fall within three standard errors of the current sample mean. Therefore, we can say that our population annual income mean will be found with a 99.7% confidence within $116,734 (\\(134,320-3\\times5,862\\)) and $151,906 (\\(134,320+3\\times5,862\\)). We talk about 99.7% confidence because there’s a small chance (0.03%) that the population mean could be farther away than 3 standard errors. That’s why we refer to these intervals as confidence intervals, as they give us with some level of confidence, an interval in which the population mean will be found.\n\n\nVisualizing the intuition behind confidence intervals\nFigure 6 visually illustrates the intuition behind our reasoning. We assume that the mean obtained from our sample could belong to any mean sampling distribution, which center, i.e., distribution mean and, in turn, population mean, is found within this ±3SEM area, highlighted with a slightly grayer shade. Even the mean we obtained could be the center of the distribution, i.e., the population mean.\n\n\n\n\n\nFigure 6: Exemplary possible sample means distributions\n\n\n\n\n\n\nDirectly measuring errors\nIn our previous example, we gained an understanding of confidence intervals by examining the distribution of sample means. However, we can take this a step further by directly translating the distribution of sample means into an error distribution.\nError, in this context, refers to the difference from the population’s mean. To calculate it, we simply subtract the population mean from each individual sample. Non-zero values indicate a disparity between the sample mean and the population mean, with the magnitude of the value signifying the extent of this difference.\nWhen we subtract a consistent value from every observation of a variable, we effectively shift the variable’s mean by the same amount. Given that the mean of the sampling distribution aligns with the population mean, subtracting this value from the mean calculated for each sample effectively centers the distribution around 0.\nMoreover, as we’ve discussed before, altering the mean of a normal distribution corresponds to shifting the distribution horizontally, repositioning its central point where it is symmetrical. Therefore, this subtraction effectively repositions the distribution’s center to zero.\nLet’s revisit the previous scenario where we calculated the mean annual income from 10,000 different samples, each containing 40 customers drawn from our population. We visualized the distribution of the computed means in Figure 5. We now proceed to subtract the population mean from every computed individual sample mean, obtaining the errors incurred. We then plot the error distribution using a histogram, as depicted in Figure 7. As shown in the figure, it retains the same shape as Figure 5, yet it is now centered at zero, i.e. the point which signifies absence of error.\n\n\n\n\n\nFigure 7: Error Distribution of the Means for 10,000 Samples of 40 Customers (thousands, $)\n\n\n\n\nSo, rather than focusing on the distribution of various sample means, we are now examining how errors, the differences between sample means and population means, are distributed. Notably, this error is expressed in the same units as our variable, i.e., in thousands of dollars.\nIdeally, we aim to obtain errors that remain invariant across different measurement scales, enabling us to compare error distributions across different variables, even when they use distinct units of measurement. Achieving this requires us to utilize a common measurement unit, with the standard deviation commonly being the preferred metric for this purpose.\nConsequently, we proceed by dividing each value of our variable by its standard deviation, effectively transforming measurement units into standard deviation units. This process of dividing each observation by the standard deviation is essentially equivalent to dividing the standard deviation by itself, resulting in a standard deviation of 1.\nKeep in mind that the standard deviation of the sampling distribution of the mean is identical to the standard error of the mean. Therefore, our process involves dividing the diverse errors by the standard error, resulting in a error measured in standard errors of the mean distribution with a mean of 0 and a standard deviation of 1, as depicted in Figure 8. As we can see, the distribution is still normal, we only shifted its mean to 0 and converted its standard deviation to 1.\n\n\n\n\n\nFigure 8: Sampling mean error measured in standard errors distribution for 10,000 Samples of 40 Customers\n\n\n\n\nIn summary, our process involved subtracting the mean of the sampling distribution of the mean, i.e., the population mean from each computed sample mean, essentially transforming the sample means into errors—difference between the computed mean and the population mean. Following this, we divided these errors by the standard deviation of the sampling distribution of the mean, i.e. the standard error of the mean, thereby representing them in a consistent unit of measurement. The outcome is a distribution with a mean of 0 and a standard deviation of 1, allowing us to compare these standardized errors across various variables effectively. Hence the whole process can be represented through the following formula:\n\\[\n\\frac{\\bar{x}-\\mu}{SE} = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nThis is what we call the standardized version of the sample mean."
  },
  {
    "objectID": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "href": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Dealing with uncertainty: When standard error information is lacking",
    "text": "Dealing with uncertainty: When standard error information is lacking\nOur idealized assumption of knowing the exact standard error of the mean is often impractical in real-world scenarios. As we discussed in the previous post, calculating the standard error usually necessitates either drawing numerous samples of the same size from the same population, calculating the mean for each sample, and then determining the standard deviation of those sample means, or dividing the population’s standard deviation by the square root of our sample size. Unfortunately, both of these methods are frequently unfeasible. To overcome this challenge, we resort to estimating the standard error by dividing the sample’s standard deviation by the square root of the sample size, thereby introducing an additional layer of uncertainty into our calculations.This can be easily visualized by replacing the population’s standard deviation to the sample’s standard deviation in the previous formula:\n\\[  \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}} \\]\nAs previously shown, when we have exact knowledge of the standard error, the distribution of the standardized version of the sample mean follows a normal distribution. However, when we lack precise knowledge of the standard error and instead use an estimate, does the distribution of the standardized version of the sample mean still follow a normal distribution? To verify this, we proceed to visualize the distribution of the standardized version of the sample mean when we don’t know the standard error precisely and estimate it by dividing the sample’s standard deviation by the square root of the sample size. Given that the standard error depends on the sample size, we repeat this process for various sample sizes. For each size, we extract 100,000 samples and compute the standardized version of the sample mean with the estimated standard error. These resulting distributions are illustrated in Figure 9.\n\n\n\n\n\nFigure 9: Sampling mean error measured in estimated standard errors distribution for 100,000 Samples of varying number of Customers\n\n\n\n\nAs evident in Figure 9, these errors exhibit a distribution that closely resembles the normal distribution, particularly when dealing with larger sample sizes. However, for smaller sample sizes, the distribution exhibits broader tails compared to the typical normal distribution. In reality, this altered distribution is known as the Student’s t-distribution, commonly referred to as the t-distribution for simplicity.\n\nEmbracing the t-Distribution\nThe t-distribution is not a single function but rather a family of functions. Similar to the normal curve, each t-distribution is symmetric, with its mean positioned at the center. However, unlike the normal distribution, in the case of the t-distribution, the mean is always fixed at 0. This means that the t-distribution is centered around 0 by default, and its shape and spread are determined by a parameter known as the degrees of freedom. The concept of degrees of freedom, in a sense, varies from application to application, but in this domain we can understand it as the number of independent pieces of information to calculate a statistic, i.e. the mean for us. For the mean the degrees of freedom are equivalent to the number of observations minus one (\\(n-1\\)).\nThe intuition behind the appearance of t-distribution when using the estimated standard error arises from the added uncertainty because of the use of this estimation. It provides a more appropriate and conservative model for the variability of sample means when the standard error is unknown. Unlike the normal distribution, the t-distribution takes sample size into account, presenting wider tails that aptly accommodate the increased uncertainty and variability associated with estimating the standard error from a sample.\nFigure 10 illustrates how the t-distribution transforms with varying degrees of freedom.As degrees of freedom increase, the distribution’s tails gradually become narrower, nearing a state that closely resembles the normal distribution. This transformation is a consequence of larger sample sizes, which enable a more refined depiction of the underlying population. Consequently, it leads to reduced sampling errors and, consequently, enhances the precision of our estimations of the standard error, reducing uncertainty.\n\nviewof current_df = Inputs.range(\n  [1, 50],\n  {value: 1, step: 1, label: \"Degrees of freedom\"}\n)\n\n\n\nfiltered2 = transpose(data2).filter(function(df_distribution) {\n  return current_df === df_distribution.df;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered2,\n  { x: \"x\",\n    y: \"values\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n    }\n  ).plot({x: {domain: [-6, 6], grid: true}, y: {domain: [0, 0.45]}})\n\n\n\n\n\nFigure 10: Exemplary t distributions with varying degrees of freedom\n\n\n\nThe t-distribution, in contrast to the normal distribution, exhibits broader tails, making it unsuitable for applying the 68-95-99.7 rule we discussed earlier. With these wider tails, our expectations change: we can no longer anticipate approximately 68% of potential mean values falling within 1 standard error of the mean, 95% within 2, and 99.7% within 3; these proportions are now reduced.\nAs the characteristics of the t-distribution are contingent upon degrees of freedom, we must consider them when determining the number of standard errors from the mean required to encompass a specific proportion of values. These precise proportions will vary as degrees of freedom change. Nevertheless, as degrees of freedom increase, the distribution of standardized sample means approximates a normal distribution, allowing us to eventually employ the 68-95-99.7 rule.\nTable 1 provides the critical values for t-distributions, which are usually referred as t, across a range of degrees of freedom and confidence levels. Notably, as the sample size becomes sufficiently large, the critical values for the t-distribution closely mirror those of a (standardized) normal distribution. In fact, when dealing with an infinite number of degrees of freedom, the critical values for the t-distribution converge to those of a normal distribution. Consequently, for sufficiently large sample sizes, it’s entirely justified to work directly with a normal distribution due to its close approximation to the t-distribution in such cases.\n\n\nDisplay Table 1\n\n\n\nTable 1: T-distribution critical values\n\n\n\n\n\n\n\n\nDegrees of Freedom (df)\n68% Critical Value\n95% Critical Value\n99.7% Critical Value\n\n\n\n\n1\n1.819\n12.706\n212.205\n\n\n2\n1.312\n4.303\n18.216\n\n\n3\n1.189\n3.182\n8.891\n\n\n4\n1.134\n2.776\n6.435\n\n\n5\n1.104\n2.571\n5.376\n\n\n6\n1.084\n2.447\n4.800\n\n\n7\n1.070\n2.365\n4.442\n\n\n8\n1.060\n2.306\n4.199\n\n\n9\n1.053\n2.262\n4.024\n\n\n10\n1.046\n2.228\n3.892\n\n\n20\n1.020\n2.086\n3.376\n\n\n30\n1.011\n2.042\n3.230\n\n\n40\n1.007\n2.021\n3.160\n\n\n50\n1.004\n2.009\n3.120\n\n\n60\n1.003\n2.000\n3.094\n\n\n70\n1.002\n1.994\n3.075\n\n\n80\n1.001\n1.990\n3.061\n\n\n90\n1.000\n1.987\n3.051\n\n\n100\n0.999\n1.984\n3.042\n\n\n150\n0.998\n1.976\n3.017\n\n\nInfinity\n0.994\n1.960\n2.968\n\n\n\n\n\n\n\nEstimating confidence intervals for our exemplary sample of 40 customers\nHaving established that estimating, rather than precisely knowing the standard error, introduces an additional layer of uncertainty, we must adapt our approach when calculating margins of error and confidence intervals. Instead of relying on the normal distribution, we turn to the t-distribution, a distribution whose shape varies with the sample size. For smaller samples, it features wider tails, transitioning towards a normal distribution as the sample size increases. These wider tails account for the added uncertainty, resulting in more conservative estimates.\nReturning to our example, where we examined a sample of 40 customers with a mean of $134,320 the first step is to estimate the standard deviation for this sample, which amounts to $70,439. Therefore, we estimate the standard error by dividing this standard deviation by the square root of 40, resulting in an estimated standard error of $11,137.38.\nFor a desired confidence level of 99.7%, we should use a critical value of 3.166. This value corresponds to the critical value for a confidence level of 99.7% and 39 degrees of freedom (40-1). Moreover, note that this critical value is larger than the value of 3, which we used when we knew the standard error and, thus, had a sampling distribution of the mean that followed a normal distribution. Consequently, we can calculate the margin of error by multiplying the critical value for the chosen confidence level by the estimated standard error, yielding a margin of error of $35,260.95 (\\(3.166 \\times \\$11,137.38\\) = \\(\\$35,260.95\\)). With 99.7% confidence, the population mean is estimated to fall between (\\(\\$134,320\\pm\\$35,260.95\\)) $99,059.05 and $169,581.\nThese confidence intervals, which are wider than the previous ones obtained when we knew the exact standard error, provide a range of $99,059.05 to $169,581, as opposed to the narrower intervals of $116,734 to $151,906. This increase in width reflects the additional caution necessitated by the t-distribution’s wider tails and the inherent uncertainty associated with estimating the standard error.\n\n\n\n\n\n\n95% confidence as a standard\n\n\n\n\n\nIn the examples provided thus far, we’ve been working with a relatively high confidence level of 99.7%. While such a level of confidence is valuable in certain contexts, it’s worth highlighting that in everyday practical applications, a 95% confidence level is the more prevalent choice.\nA 95% confidence level offers a balanced compromise between precision and practicality. This means that we are willing to tolerate a 5% chance of not capturing the population mean within the defined intervals, in return for the benefits of having narrower confidence intervals."
  },
  {
    "objectID": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#a-final-note",
    "href": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#a-final-note",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "A final note",
    "text": "A final note\nIn the preceding sections, we saw that we can define confidence intervals by leveraging the shape of the sampling distribution. This sampling distribution can be converted into an error distribution measured in standard errors (or estimated standard errors). And from such distribution we can find critical values that define the points within which the majority of errors will be found, in a way that we can calculate confidence intervals. Thus, given that such a distribution for other statistics follows a symmetrical as the previously seen, we can extract some simple formula for the computation of the confidence interval of such statistics.\nA confidence interval consists of two limits, defining the lower and upper bounds of the interval, where the point estimate lies at the center. The confidence interval (CI) for an estimator of a parameter \\(\\theta\\) can be expressed in the following way:\n\\[CI_{\\theta} = \\hat{\\theta}\\pm MOE\\]\nmeaning that the upper confidence interval for a parameter \\(\\theta\\) is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)), i.e., the value obtained in the sample for that parameter, plus the margin of error (MOE). While the lower confidence interval is equal to the the estimator for that parameter (\\(\\hat{\\theta}\\)) minus the margin of error.\nThe margin of error corresponds to half the width of the interval and is given by:\n\\[MOE = \\Phi^{-1}_{1-\\frac{\\alpha}{2}} \\times \\hat{\\sigma}_{\\theta} \\]\nHere, \\(\\hat{\\sigma}_{\\theta}\\) represents the estimated standard error of \\(\\theta\\) and \\(\\Phi^{-1}_{1-\\frac{\\alpha}{2}}\\) denotes the critical value. More specifically it refers to the inverse quantile function, which includes a confidence level equal to \\(1-\\alpha\\), i.e., a function which tells us the point in which that proportion of the distribution is found.\nWhile the exact sampling distribution for some statistics can be unknown, as well as their standard error, the central limit theorem often provides a justification for using a normal approximation. For this reason, for most statistics we tend to assume that their sampling distribution follows a normal distribution (given that the sample is large enough. An example is the t-distribution which for large enough samples it approximates the normal distribution). This approximation tends to be accurate, but even in cases in which it’s not that accurate, it’s better to have an approximate confidence interval than a solely point estimate."
  },
  {
    "objectID": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#summary",
    "href": "posts/2022-04-07-statistics-foundations-confidence-intervals/index.html#summary",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Summary",
    "text": "Summary\n\nSampling error arises from using samples rather than the entire population for analysis, as samples may not capture all population nuances.\nThe sampling error can be quantified using the standard error, which measures the variability of a statistic calculated from different samples.\nThe standard error helps define a range within which sample statistics are likely to fall, providing insights into potential population parameters.\nThe sampling distribution of the mean follows a normal distribution, with its mean equal to the population mean and standard deviation equal to the standard error.\nThe 68-95-99.7 rule provides a shorthand for understanding the distribution: approximately 68% of values are within 1 standard deviation from the mean, 95% within two standard deviations, and 99.7% within three standard deviations. These values defining specific regions are known as critical values.\nTranslating this to standard errors, 68% of values are about 1 standard error from the population mean, 95% about two standard errors, and 99.7% about three standard errors.\nA sample, with 95% confidence, is expected to be about 2 standard errors from the mean, and with 99.7% confidence, about 3 standard errors.\nThis argument can be reversed: with 95% confidence, a sample is about 2 standard errors from the population mean, and with 99.7% confidence, about 3 standard errors.\nThe margin of error, which creates a likely population range (confidence interval), is determined by multiplying the critical value by the standard error.\nThe confidence interval is obtained by adding the margin of error to the sample mean (upper interval) and subtracting it from the sample mean (lower interval).\nIn most cases, the standard error is unknown and needs to be estimated by dividing the sample’s standard deviation by the square root of its size.\nWhen estimating the standard error, the assumption that the sampling distribution of the mean follows a normal distribution no longer holds; it follows a t-Student distribution.\nThe t-Student distribution (t-distribution) varies based on degrees of freedom, resembling the normal distribution but having wider tails with smaller sample sizes (degrees of freedom).\nThe wider tails of the t-distribution result in larger magnitude critical values than the normal distribution, leading to wider confidence intervals."
  }
]