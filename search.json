[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge\nI am a technophile and data enthusiast, passionate about harnessing the potential of data to analyze competitive landscapes and derive strategic market insights, which provide actionable guidance, ultimately empowering organizations to gain a competitive advantage. Equally important to me is the art of effectively communicating these insights, no matter how intricate they may be.\nI possess 7 years of academic research experience, during which I explored how to leverage state-of-the-art data-driven methods to gain a deeper understanding of consumers and markets. Throughout this period, I have also assumed leadership roles and actively contributed to several multifaceted industry consultancy endeavors, primarily in the finance and tech sectors, in which I helped organizations enhance their understanding of consumer purchasing behavior and the complex dynamics that shape their markets."
  },
  {
    "objectID": "index.html#fran",
    "href": "index.html#fran",
    "title": "Francesc Busquet",
    "section": "",
    "text": "Data-Driven Market Intelligence\nEmpowering Organizations for Competitive Edge"
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "",
    "text": "In the previous post of the Statistics Foundations series, we explored the inherent errors associated with working with samples instead of the entire population. These errors stem from the limitations of samples in capturing the full spectrum of population nuances.\nWe delved into quantifying this error, commonly known as the standard error, by assessing the variability of a statistic derived from different samples of the same size drawn from the same population. To further explore this concept, we focused on the mean. We used the standard error definition to derive a formula that directly computes the standard error of the mean, dividing the population’s standard deviation by the square root of the sample size. However, we recognized that applying such a formula in practical scenarios can be unfeasible, as it hinges on possessing information about the entire population—information that is typically unavailable, underscoring the very reason why we employ samples in the first place.\nNevertheless, we uncovered a practical workaround by assuming that the standard deviation of our sample serves as an estimator for the population’s standard deviation. This substitution effectively transformed the formula into one that employs the sample’s standard deviation divided by the square root of its size, allowing us to estimate the standard error.\nSo far, our primary emphasis has been on understanding and quantifying the sampling error. But let’s take it a step further and connect this concept to something more practical and tangible. Imagine if we could use this knowledge to pinpoint a range within which our sample statistics are likely to fall and, by extension, where the true population parameters may lie. That’s where the notion of confidence intervals comes into the spotlight, a topic we’ll explore in this blog, with a primary focus on the mean.\nOnce more, we’ll be employing the same dataset from our previous posts, encompassing data from 2,000 supermarket customers, including details about their age, annual income, and educational level. As in our prior discussions, we’ll operate under the assumption that this dataset comprehensively captures information about all our customers, effectively representing our entire population."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#distributions",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#distributions",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Distributions",
    "text": "Distributions\nIn our previous discussions, we’ve touched upon the concept of distribution without providing a formal definition. In common parlance, this term is commonly used and readily understood, as demonstrated by phrases like “concentrated urban population distribution” or “disparities in wealth distribution within a country.” In these examples, “distribution” simply denotes how individuals or assets are spread across a specific area.\nIn statistics, the concept of distribution remains conceptually consistent. A variable’s distribution illustrates how the different values of that variable spread out and which are more prevalent. Our exploration of distributions has primarily involved the examination of a variable’s histogram—a visual representation that conveys the frequency of values within predefined intervals, commonly known as “bins”. This approach provides a direct and intuitive means of discerning the spread of values and identifying those that occur more and less frequently. For instance, let’s revisit the annual income distribution of our “population”.\n\n\n\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\nFigure 1 offers a visual representation of our population’s annual income distribution. This graphic, for instance, reveals that a substantial portion of our customers falls within the income range of $80,000 to $150,000, specifically encompassing 1,427 individuals, equivalent to 71.35% of our customer base.\nNow, consider a scenario where we randomly select a customer and, before checking any of their information, we make an educated guess about their income. In this situation, it’s reasonable to infer that their income is highly likely to fall within the range of $80,000 to $150,000, as the majority of our customers are concentrated in this income range.\nConsequently, distributions provide us with a framework to describe variables using the language of probability. This is why, in statistics, we often refer to them as probability distributions. To illustrate, in the previous example, we could state that the probability of a randomly chosen customer having an income between $80,000 and $150,000 is 71.35%. The connection between a variable’s values and their associated probabilities can be mathematically expressed through a function, which directly relates the variable’s values to their respective probabilities.\n\n\n\n\n\n\nDiscrete and continuous distributions\n\n\n\n\n\nWhen discussing probability distributions, it is crucial to distinguish between two fundamental types: continuous and discrete distributions.\nDiscrete distributions are characterized by elements that can only assume a finite number of values within a defined range. Examples of such distributions include the number of children in a family or the count of customers in a shop on a given day. These variables take on specific, countable values.\nOn the other hand, continuous distributions consist of elements that can take any value within a specified interval. While our everyday thinking and calculations often involve finite numbers, consider scenarios where precise measurements are vital, such as in pharmaceutical drug development, where even minuscule differences in weight can have significant implications. In this context, the weight of substances is treated as a continuous variable.\nIn continuous distributions, owing to their infinite range of potential values, it is not possible to precisely calculate the probability associated with a specific value. Conversely, in the case of discrete distributions, where a finite and countable set of values exists, we can accurately determine the probability associated with each individual value. In practical terms, this means that for continuous variables, we can only compute the probability of a variable falling within a certain range. While for discrete variables, we can calculate the probability of it assuming a specific value or falling within a particular range.\n\n\n\nMany probability distributions are frequently encountered and have earned distinctive names due to their importance. One such example is the uniform distribution, where every potential value of a variable is equally likely to occur. Another prominent distribution is the normal distribution, recognizable by its bell-shaped curve.\nWhile there are undeniably several other frequent probability distributions, for the purpose of this discussion, we will concentrate on the normal distribution. This emphasis is justified by its pivotal role in facilitating the translation of sample mean and sampling error, quantified by the standard error, into intervals within which we can confidently predict the likely range of the population mean. These intervals are commonly known as confidence intervals.\n\nThe normal distribution\nThe normal distribution, also known as the Gaussian distribution, is one of the most valuable continuous distributions, primarily because many statistics are normally distributed in their sampling distribution (as we saw in the previous post for the case of the mean).\nThe normal distribution is easily recognizable by its classic bell-shaped curve, as depicted in Figure 2. This curve resembles a perfectly symmetrical hill with a clear peak at its center, which represents the distribution’s mean. As you move away from this peak, the curve gradually slopes downward and then gently turns outward. This smooth descent and outward turn reveal a pattern of how data spreads—the likelihood of observing values becomes lower as you move further away from the mean, making values closer to the mean more probable.\nAdditionally, the symmetry of the normal distribution means that the probabilities of finding values above and below the mean are identical. In simpler terms, it implies that the chances of observing values on one side of the peak are the same as on the other side.\n\n\n\n\n\n\n\n\nFigure 2: Exemplary shape of a normal distribution\n\n\n\n\n\n\nMean and standard deviation: shaping the normal distribution\nThe entire shape of a normal distribution can be effectively described using just two key parameters: the mean and the standard deviation.\n\n\n\n\n\n\nNotation\n\n\n\n\n\nGiven that the mean and the standard deviation effectively describe the entire shape of a normal distribution. As such, we typically employ the following notation to succinctly represent a normal distribution: \\(N(\\mu, \\sigma^2)\\), where \\(\\mu\\) is the variable’s mean and \\(\\sigma\\) is the variable’s standard de\nTherefore, we can denote that a random variable \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) as:\n\\(X \\sim N(\\mu, \\sigma^2)\\)\n\n\n\nThe mean, as mentioned earlier, represents the distribution’s center and the location of its peak. On the other hand, the standard deviation characterizes the curve’s shape. It indicates whether the curve is relatively flat or sharply peaked.\nFigure 3 offers an interactive visual representation showcasing how tweaking the mean and the standard deviation influences a normal distribution. When we adjust the mean, while keeping the standard deviation constant, we how the entire distribution shifts. An increase in the mean nudges it to the right, while a decrease causes it to veer to the left. Conversely, changing the standard deviation while maintaining the mean constant is like stretching or compressing the data. A smaller standard deviation suggests that most data points group closer to the mean, yielding a tall, slender curve. On the contrary, a larger standard deviation indicates that data points are more dispersed, resulting in a shorter, broader curve.\n\nviewof current_sd = Inputs.range(\n  [1, 5],\n  {value: 1, step: 1, label: \"Standard Deviation\"}\n)\n\nviewof current_mean = Inputs.range(\n  [0, 20],\n  {value: 0, step: 5, label: \"Mean\"}\n)\n\nfiltered = transpose(data).filter(function(normal_distribution) {\n  return current_sd === normal_distribution.sd_value &&\n    current_mean === normal_distribution.mean_value;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered,\n  Plot.binX(\n    {y: \"proportion\"},\n    {x: \"values\", \n     curve: \"natural\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n     interval : 0.75\n    }\n  )\n  \n    \n).plot({x: {domain: [-20, 40], grid: true}, y: {domain: [0, 0.3]}})\n\n\n\n\n\n\n\n\nFigure 3: Exemplary normal distributions with varying mean and standard deviation\n\n\n\n\n\n\nThe normal probability density function\nAs we’ve just observed and articulated, the core characteristics of a normal distribution revolve around its mean and standard deviation. Mathematically, the shape of a normal distribution can be portrayed through a functional relationship between the values of a normally distributed variable \\(X\\), characterized by a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\), and their probability density, known as the probability density function:\n\\(f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\)\nThe pivotal element within this formula is the exponential term \\(\\left({ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}}\\right)\\). This term effectively communicates the rate at which the probability density diminishes as we distance ourselves from the mean (\\(\\mu\\))—as \\(x\\) moves farther from \\(\\mu\\), the lower the probability density. It’s essential to note that a larger standard deviation (\\(\\sigma\\)) results in a reduction of the magnitude of this expression, which, in turn, moderates the pace of the probability density decay.\n\n\n\n\n\n\nProbability densities\n\n\n\n\n\nIt is essential to differentiate between probability densities and probabilities, as these two concepts fundamentally diverge. As we have underscored previously, computing the exact probability of a continuous variable taking a specific value is unfeasible, given that continuous variables can theoretically encompass an infinite range of values.\nTo illustrate this, consider measuring an individual’s height, which may be reported as 175cm. However, if we possessed an incredibly precise measuring instrument, it might record the height as 174.9999945 cm. In practice, we typically round such measurements to a more practical form, like 175 cm, instead of expressing them as infinite decimals.\nWhen we talk about probability densities, we essentially employ a similar principle — grouping values near one another. This grouping enables us to represent the likelihood of a value falling near a specific point, such as 175cm, without claiming it is precisely 175cm. It’s important to note that probability densities, in isolation, lack a direct interpretation as probabilities. However, they are ingeniously constructed to ensure that the area beneath the density curve always maintains its interpretability as genuine probabilities.\n\n\n\n\n\nThe 68-95-99.7 rule\nSince the “probability” of specific values in a normal distribution is dictated by the mean and the standard deviation, we can directly associate the likelihood of specific events with these two parameters, particularly concerning how many standard deviations we deviate from the mean. This relationship gives rise to a widely recognized rule, commonly known as the 68-95-99.7 rule. According to this rule, there’s an approximate probability of 68% that a particular observation falls within one standard deviation from the mean (i.e., between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\)), roughly 95% within two standard deviations from the mean (i.e., between \\(\\mu - 2\\sigma\\) and \\(\\mu + 2\\sigma\\)), and approximately 99.7% within three standard deviations from the mean (i.e., between \\(\\mu - 3\\sigma\\) and \\(\\mu + 3\\sigma\\)). In this context, the values 1, 2, and 3, representing the number of standard deviations from the mean, are often referred to as critical values. These values help to define specific regions in the distribution.\nIn simpler terms, this rule tells us that for a normally distributed variable, roughly 68% of observations are within one standard deviation of the mean, about 95% are within two standard deviations, and approximately 99.7% are within three standard deviations. Figure 4 provides a visual representation of this rule.\n\n\n\n\n\n\n\n\nFigure 4: Visual representation of the 68-95-99.7 rule"
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#sample-means-and-the-normal-distribution",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Sample means and the normal distribution",
    "text": "Sample means and the normal distribution\nNow that we have a better understanding of probability distributions and how they help us assess where most data points are likely to cluster, as well as to assess the probability of an unknown data point falling within a specific range, let’s revisit our example involving supermarket customers. In the previous post, we uncovered an essential concept: sample means drawn from the same population and of the same size conform to a normal distribution with its center—the distribution’s mean—aligning with the population’s mean.\n\nVisualizing the concept\nTo provide a visual representation of this concept, Figure 5 displays a histogram that provides an overview of the means obtained from 10,000 samples, with each sample containing 40 randomly selected customers from our population. In essence, this histogram visualizes the mean sampling distribution for samples of 40 observations drawn from our population.\n\n\n\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nAs evident from the figure, these sample means display a characteristic bell-shaped distribution, i.e., following a normal distribution, with the distribution mean precisely aligning with the population mean, which, in this case, is $120,950. Additionally, the standard deviation of this distribution, which is equivalent to the standard error of the mean, is $5,862.\n\n\nApplying the 68-95-99.7 rule\nBy applying the principles of the 68-95-99.7 rule, as explained earlier, we can infer that approximately 68% of samples drawn from the population will fall within one standard deviation from the mean, about 95% within two standard deviations, and nearly 99.7% within three. Notably, the standard deviation of sample means, computed from various samples of the same size and from the same population, corresponds to their standard error, whereas the mean aligns with the population mean. Consequently, we can rephrase this as follows: about 68% of sample means will be within one standard error of the population’s mean, approximately 95% within two standard errors, and nearly 99.7% within three standard errors of the population’s mean.\n\n\nPractical example and intuition\nNow, let’s apply this understanding in practice. Consider a scenario where we know the standard error of the mean (SEM = $5,862), but the population mean remains uncertain. However, we want to have an idea of the value the population mean could take. To do so, we select a sample of 40 customers from our population and calculate the mean of their annual income, which turns out to be $134,320.\nThis process of taking a sample and computing its mean is akin to randomly selecting a value from the distribution we discussed earlier, the mean sampling distribution. Therefore, it is highly likely that the value we obtain from this sample will be found within three standard errors of the population’s mean, as approximately 99.7% of sample means would be found within this range. This potential difference that we could have from the population mean is commonly known as the margin of error.\nConsequently, we can reverse the previous statement, affirming that the population mean is very likely to fall within three standard errors of the current sample mean. Therefore, we can say that our population’s annual income mean will be found with a 99.7% confidence within $116,734 (\\(134,320-3\\times5,862\\)) and $151,906 (\\(134,320+3\\times5,862\\)). We talk about 99.7% confidence because there’s a small chance (0.03%) that the population mean could be farther away than 3 standard errors. That’s why we refer to these intervals as confidence intervals, as they give us with some level of confidence, an interval in which the population mean will be found.\n\n\nVisualizing the intuition behind confidence intervals\nFigure 6 visually illustrates the intuition behind our reasoning. We assume that the mean obtained from our sample could belong to any mean sampling distribution, which center, i.e., distribution mean and, in turn, population mean, is found within this ±3SEM area, highlighted with a slightly grayer shade. Even the mean we obtained could be the center of the distribution, i.e., the population mean.\n\n\n\n\n\n\n\n\nFigure 6: Exemplary possible sample means distributions\n\n\n\n\n\n\n\nDirectly measuring errors\nIn our previous example, we gained an understanding of confidence intervals by examining the distribution of sample means. However, we can take this a step further by directly translating the distribution of sample means into an error distribution.\nError, in this context, refers to the difference from the population’s mean. To calculate it, we simply subtract the population mean from each individual sample. Non-zero values indicate a disparity between the sample mean and the population mean, with the magnitude of the value signifying the extent of this difference.\nWhen we subtract a consistent value from every observation of a variable, we effectively shift the variable’s mean by the same amount. Given that the mean of the sampling distribution aligns with the population mean, subtracting this value from the mean calculated for each sample effectively centers the distribution around 0.\nMoreover, as we’ve discussed before, altering the mean of a normal distribution corresponds to shifting the distribution horizontally, repositioning its central point where it is symmetrical. Therefore, this subtraction effectively repositions the distribution’s center to zero.\nLet’s revisit the previous scenario where we calculated the mean annual income from 10,000 different samples, each containing 40 customers drawn from our population. We visualized the distribution of the computed means in Figure 5. We now proceed to subtract the population mean from every computed individual sample mean, obtaining the errors incurred. We then plot the error distribution using a histogram, as depicted in Figure 7. As shown in the figure, it retains the same shape as Figure 5, yet it is now centered at zero, i.e. the point that signifies the absence of error.\n\n\n\n\n\n\n\n\nFigure 7: Error distribution of the means for 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nSo, rather than focusing on the distribution of various sample means, we are now examining how errors, the differences between sample means and population means, are distributed. Notably, this error is expressed in the same units as our variable, i.e., in thousands of dollars.\nIdeally, we aim to obtain errors that remain invariant across different measurement scales, enabling us to compare error distributions across different variables, even when they use distinct units of measurement. Achieving this requires us to utilize a common measurement unit, with the standard deviation commonly being the preferred metric for this purpose.\nConsequently, we proceed by dividing each value of our variable by its standard deviation, effectively transforming measurement units into standard deviation units. This process of dividing each observation by the standard deviation is essentially equivalent to dividing the standard deviation by itself, resulting in a standard deviation of 1.\nKeep in mind that the standard deviation of the sampling distribution of the mean is identical to the standard error of the mean. Therefore, our process involves dividing the diverse errors by the standard error, resulting in a error measured in standard errors of the mean distribution with a mean of 0 and a standard deviation of 1, as depicted in Figure 8. As we can see, the distribution is still normal, we only shifted its mean to 0 and converted its standard deviation to 1.\n\n\n\n\n\n\n\n\nFigure 8: Sampling error of the mean measured in standard errors distribution for 10,000 samples of 40 customers\n\n\n\n\n\nIn summary, our process involved subtracting the mean of the sampling distribution of the mean, i.e., the population mean from each computed sample mean, essentially transforming the sample means into errors—difference between the computed mean and the population mean. Following this, we divided these errors by the standard deviation of the sampling distribution of the mean, i.e. the standard error of the mean, thereby representing them in a consistent unit of measurement. The outcome is a distribution with a mean of 0 and a standard deviation of 1, allowing us to compare these standardized errors across various variables effectively. Hence the whole process can be represented through the following formula:\n\\[\n\\frac{\\bar{x}-\\mu}{SE} = \\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nThis is what we call the standardized version of the sample mean."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#dealing-with-uncertainty-when-standard-error-information-is-lacking",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Dealing with uncertainty: When standard error information is lacking",
    "text": "Dealing with uncertainty: When standard error information is lacking\nOur idealized assumption of knowing the exact standard error of the mean is often impractical in real-world scenarios. As we discussed in the previous post, calculating the standard error usually necessitates either drawing numerous samples of the same size from the same population, calculating the mean for each sample, and then determining the standard deviation of those sample means, or dividing the population’s standard deviation by the square root of our sample size. Unfortunately, both of these methods are frequently unfeasible. To overcome this challenge, we resort to estimating the standard error by dividing the sample’s standard deviation by the square root of the sample size, thereby introducing an additional layer of uncertainty into our calculations.This can be easily visualized by replacing the population’s standard deviation to the sample’s standard deviation in the previous formula:\n\\[  \\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{n}}} \\]\nAs previously shown, when we have exact knowledge of the standard error, the distribution of the standardized version of the sample mean follows a normal distribution. However, when we lack precise knowledge of the standard error and instead use an estimate, does the distribution of the standardized version of the sample mean still follow a normal distribution? To verify this, we proceed to visualize the distribution of the standardized version of the sample mean when we don’t know the standard error precisely and estimate it by dividing the sample’s standard deviation by the square root of the sample size. Given that the standard error depends on the sample size, we repeat this process for various sample sizes. For each size, we extract 100,000 samples and compute the standardized version of the sample mean with the estimated standard error. These resulting distributions are illustrated in Figure 9.\n\n\n\n\n\n\n\n\nFigure 9: Sampling error of the mean measured in estimated standard errors distribution for 100,000 samples of varying number of customers\n\n\n\n\n\nAs evident in Figure 9, these errors exhibit a distribution that closely resembles the normal distribution, particularly when dealing with larger sample sizes. However, for smaller sample sizes, the distribution exhibits broader tails compared to the typical normal distribution. In reality, this altered distribution is known as the Student’s t-distribution, commonly referred to as the t-distribution for simplicity.\n\nEmbracing the t-distribution\nThe t-distribution is not a single function but rather a family of functions. Similar to the normal curve, each t-distribution is symmetric, with its mean positioned at the center. However, unlike the normal distribution, in the case of the t-distribution, the mean is always fixed at 0. This means that the t-distribution is centered around 0 by default, and its shape and spread are determined by a parameter known as the degrees of freedom. The concept of degrees of freedom, in a sense, varies from application to application, but in this domain we can understand it as the number of independent pieces of information to calculate a statistic, i.e. the mean for us. For the mean the degrees of freedom are equivalent to the number of observations minus one (\\(n-1\\)).\nThe intuition behind the appearance of t-distribution when using the estimated standard error arises from the added uncertainty because of the use of this estimation. It provides a more appropriate and conservative model for the variability of sample means when the standard error is unknown. Unlike the normal distribution, the t-distribution takes sample size into account, presenting wider tails that aptly accommodate the increased uncertainty and variability associated with estimating the standard error from a sample.\nFigure 10 illustrates how the t-distribution transforms with varying degrees of freedom.As degrees of freedom increase, the distribution’s tails gradually become narrower, nearing a state that closely resembles the normal distribution. This transformation is a consequence of larger sample sizes, which enable a more refined depiction of the underlying population. Consequently, it leads to reduced sampling errors and, consequently, enhances the precision of our estimations of the standard error, reducing uncertainty.\n\nviewof current_df = Inputs.range(\n  [1, 50],\n  {value: 1, step: 1, label: \"Degrees of freedom\"}\n)\n\n\n\nfiltered2 = transpose(data2).filter(function(df_distribution) {\n  return current_df === df_distribution.df;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.areaY(filtered2,\n  { x: \"x\",\n    y: \"values\",\n     fill: \"#4682b4\",\n     fillOpacity: 0.5,\n    }\n  ).plot({x: {domain: [-6, 6], grid: true}, y: {domain: [0, 0.45]}})\n\n\n\n\n\n\n\n\nFigure 10: Exemplary t distributions with varying degrees of freedom\n\n\n\n\nThe t-distribution, in contrast to the normal distribution, exhibits broader tails, making it unsuitable for applying the 68-95-99.7 rule we discussed earlier. With these wider tails, our expectations change: we can no longer anticipate approximately 68% of potential mean values falling within 1 standard error of the mean, 95% within 2, and 99.7% within 3; these proportions are now reduced.\nAs the characteristics of the t-distribution are contingent upon degrees of freedom, we must consider them when determining the number of standard errors from the mean required to encompass a specific proportion of values. These precise proportions will vary as degrees of freedom change. Nevertheless, as degrees of freedom increase, the distribution of standardized sample means approximates a normal distribution, allowing us to eventually employ the 68-95-99.7 rule.\nTable 1 provides the critical values for t-distributions across a range of degrees of freedom and confidence levels. Notably, as the sample size becomes sufficiently large, the critical values for the t-distribution closely mirror those of a (standardized) normal distribution. In fact, when dealing with an infinite number of degrees of freedom, the critical values for the t-distribution converge to those of a normal distribution. Consequently, for sufficiently large sample sizes, it’s entirely justified to work directly with a normal distribution due to its close approximation to the t-distribution in such cases.\n\n\nDisplay Table 1\n\n\n\n\nTable 1: T-distribution critical values\n\n\n\n\n\n\n\n\n\n\n\nDegrees of Freedom (df)\n68% Critical Value\n95% Critical Value\n99.7% Critical Value\n\n\n\n\n1\n1.819\n12.706\n212.205\n\n\n2\n1.312\n4.303\n18.216\n\n\n3\n1.189\n3.182\n8.891\n\n\n4\n1.134\n2.776\n6.435\n\n\n5\n1.104\n2.571\n5.376\n\n\n6\n1.084\n2.447\n4.800\n\n\n7\n1.070\n2.365\n4.442\n\n\n8\n1.060\n2.306\n4.199\n\n\n9\n1.053\n2.262\n4.024\n\n\n10\n1.046\n2.228\n3.892\n\n\n20\n1.020\n2.086\n3.376\n\n\n30\n1.011\n2.042\n3.230\n\n\n40\n1.007\n2.021\n3.160\n\n\n50\n1.004\n2.009\n3.120\n\n\n60\n1.003\n2.000\n3.094\n\n\n70\n1.002\n1.994\n3.075\n\n\n80\n1.001\n1.990\n3.061\n\n\n90\n1.000\n1.987\n3.051\n\n\n100\n0.999\n1.984\n3.042\n\n\n150\n0.998\n1.976\n3.017\n\n\nInfinity\n0.994\n1.960\n2.968\n\n\n\n\n\n\n\n\n\nEstimating confidence intervals for our exemplary sample of 40 Customers\nHaving established that estimating, rather than precisely knowing the standard error, introduces an additional layer of uncertainty, we must adapt our approach when calculating margins of error and confidence intervals. Instead of relying on the normal distribution, we turn to the t-distribution, a distribution whose shape varies with the sample size. For smaller samples, it features wider tails, transitioning towards a normal distribution as the sample size increases. These wider tails account for the added uncertainty, resulting in more conservative estimates.\nReturning to our example, where we examined a sample of 40 customers with a mean of $134,320 the first step is to estimate the standard deviation for this sample, which amounts to $70,439. Therefore, we estimate the standard error by dividing this standard deviation by the square root of 40, resulting in an estimated standard error of $11,137.38.\nFor a desired confidence level of 99.7%, we should use a critical value of 3.166. This value corresponds to the critical value for a confidence level of 99.7% and 39 degrees of freedom (40-1). Moreover, note that this critical value is larger than the value of 3, which we used when we knew the standard error and, thus, had a sampling distribution of the mean that followed a normal distribution. Consequently, we can calculate the margin of error by multiplying the critical value for the chosen confidence level by the estimated standard error, yielding a margin of error of $35,260.95 (\\(3.166 \\times \\$11,137.38\\) = \\(\\$35,260.95\\)). With 99.7% confidence, the population mean is estimated to fall between (\\(\\$134,320\\pm\\$35,260.95\\)) $99,059.05 and $169,581.\nThese confidence intervals, which are wider than the previous ones obtained when we knew the exact standard error, provide a range of $99,059.05 to $169,581, as opposed to the narrower intervals of $116,734 to $151,906. This increase in width reflects the additional caution necessitated by the t-distribution’s wider tails and the inherent uncertainty associated with estimating the standard error.\n\n\n\n\n\n\n95% confidence as a standard\n\n\n\n\n\nIn the examples provided thus far, we’ve been working with a relatively high confidence level of 99.7%. While such a level of confidence is valuable in certain contexts, it’s worth highlighting that in everyday practical applications, a 95% confidence level is the more prevalent choice.\nA 95% confidence level offers a balanced compromise between precision and practicality. This means that we are willing to tolerate a 5% chance of not capturing the population mean within the defined intervals, in return for the benefits of having narrower confidence intervals."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#a-final-note",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#a-final-note",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "A final note",
    "text": "A final note\nIn the preceding sections, we saw that we can define confidence intervals by leveraging the shape of the sampling distribution. This sampling distribution can be converted into an error distribution measured in standard errors (or estimated standard errors). And from such distribution, we can find critical values that define the points within which the majority of errors will be found, in a way that we can calculate confidence intervals. Thus, given that such a distribution for other statistics follows a symmetrical as previously seen, we can extract some simple formula for the computation of the confidence interval of such statistics.\nA confidence interval consists of two limits, defining the lower and upper bounds of the interval, where the point estimate lies at the center. The confidence interval (CI) for an estimator of a parameter \\(\\theta\\) can be expressed in the following way:\n\\[CI_{\\theta} = \\hat{\\theta}\\pm MOE\\]\nmeaning that the upper confidence interval for a parameter \\(\\theta\\) is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)), i.e., the value obtained in the sample for that parameter, plus the margin of error (MOE). While the lower confidence interval is equal to the estimator for that parameter (\\(\\hat{\\theta}\\)) minus the margin of error.\nThe margin of error corresponds to half the width of the interval and is given by:\n\\[MOE = \\Phi^{-1}_{1-\\frac{\\alpha}{2}} \\times \\hat{\\sigma}_{\\theta} \\]\nHere, \\(\\hat{\\sigma}_{\\theta}\\) represents the estimated standard error of \\(\\theta\\) and \\(\\Phi^{-1}_{1-\\frac{\\alpha}{2}}\\) denotes the critical value. More specifically it refers to the inverse quantile function, which includes a confidence level equal to \\(1-\\alpha\\), i.e., a function that tells us the point in which that proportion of the distribution is found.\nWhile the exact sampling distribution for some statistics can be unknown, as well as their standard error, the central limit theorem often provides a justification for using a normal approximation. For this reason, for most statistics, we tend to assume that their sampling distribution follows a normal distribution (given that the sample is large enough. An example is the t-distribution which for large enough samples approximates the normal distribution). This approximation tends to be accurate, but even in cases in which it’s not that accurate, it’s better to have an approximate confidence interval than a solely-point estimate."
  },
  {
    "objectID": "posts/2022/statistics-foundations-confidence-intervals/index.html#summary",
    "href": "posts/2022/statistics-foundations-confidence-intervals/index.html#summary",
    "title": "Statistics Foundations: Confidence intervals",
    "section": "Summary",
    "text": "Summary\n\nSampling error arises from using samples rather than the entire population for analysis, as samples may not capture all population nuances.\nThe sampling error can be quantified using the standard error, which measures the variability of a statistic calculated from different samples.\nThe standard error helps define a range within which sample statistics are likely to fall, providing insights into potential population parameters.\nThe sampling distribution of the mean follows a normal distribution, with its mean equal to the population mean and standard deviation equal to the standard error.\nThe 68-95-99.7 rule provides a shorthand for understanding the distribution: approximately 68% of values are within 1 standard deviation from the mean, 95% within two standard deviations, and 99.7% within three standard deviations. These values defining specific regions are known as critical values.\nTranslating this to standard errors, 68% of values are about 1 standard error from the population mean, 95% about two standard errors, and 99.7% about three standard errors.\nA sample, with 95% confidence, is expected to be about 2 standard errors from the mean, and with 99.7% confidence, about 3 standard errors.\nThis argument can be reversed: with 95% confidence, a sample is about 2 standard errors from the population mean, and with 99.7% confidence, about 3 standard errors.\nThe margin of error, which creates a likely population range (confidence interval), is determined by multiplying the critical value by the standard error.\nThe confidence interval is obtained by adding the margin of error to the sample mean (upper interval) and subtracting it from the sample mean (lower interval).\nIn most cases, the standard error is unknown and needs to be estimated by dividing the sample’s standard deviation by the square root of its size.\nWhen estimating the standard error, the assumption that the sampling distribution of the mean follows a normal distribution no longer holds; it follows a Student t-distribution.\nThe Student t-distribution (t-distribution) varies based on degrees of freedom, resembling the normal distribution but having wider tails with smaller sample sizes (degrees of freedom).\nThe wider tails of the t-distribution result in larger magnitude critical values than the normal distribution, leading to wider confidence intervals.\n\n\n\nCode\n# Customer annual income distribution (thousands, $) ---------------\n\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\nstd_deviation_income &lt;- sd(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n    scale_x_continuous(breaks = round(seq(30, 320, by = 20))) +\n   annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, label = paste0(\"Mean: \", round(average_income, 2), \"\\nSD: \", round(std_deviation_income, 2))) \n\n# Create exemplary normality plot ---------------\n\nset.seed(150)\n\n#Create a normal distribution\nnormal_distribution &lt;- rnorm(30000)\nnormal_distribution &lt;- data.frame(values = normal_distribution)\n\n\n\n\nggplot(normal_distribution, aes(x = values)) +\n  geom_density(fill = \"steelblue\", color = \"steelblue\", alpha = 0.4, adjust = 2) + \n  labs(\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) \n    \n# Visual representation of the 68-95-99.7 rule ---------------\n\nmu = 0\nsigma = 1\nx &lt;- seq(-5*sigma, 5*sigma, length.out = 1000)\ny &lt;- dnorm(x, mean = mu, sd = sigma)\ndata &lt;- data.frame(x, y)\n\nggplot(data.frame(data), aes(x)) + \n    geom_ribbon(data = subset(data, x &gt;= mu - 3 * sigma & x &lt;= mu + 3 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#c1f5ef\") +\n    geom_ribbon(data = subset(data, x &gt;= mu - 2 * sigma & x &lt;= mu + 2 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#90ebe1\") +\n    geom_ribbon(data = subset(data, x &gt;= mu - 1 * sigma & x &lt;= mu + 1 * sigma),\n                aes(ymax = y), ymin = 0, fill = \"#34d1bf\") +\n  geom_ribbon(data = subset(data, x &gt;= -0.01 & x &lt;= 0.01),\n                aes(ymax = y), ymin = 0, fill = \"black\") +\n    theme_minimal() +\n    geom_vline(xintercept = c(mu - 3 * sigma, mu - 2 * sigma, mu - 1 * sigma, mu + 1 * sigma, mu + 2 * sigma, mu + 3 * sigma), \n               linetype = \"dashed\", color = \"black\", alpha = 0.5) +\n    geom_segment(x = 0 - 1 * sigma +0.05, xend = 0 + 1 * sigma -0.05, y = 0.45, yend = 0.45, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n  annotate(\"text\", x = 0, y = 0.46, label = \"68%\") +\n  geom_segment(x = 0 - 2 * sigma +0.05, xend = 0 + 2 * sigma -0.05, y = 0.5, yend = 0.5, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n   annotate(\"text\", x = 0, y = 0.51, label = \"95%\") +\n  geom_segment(x = 0 - 3 * sigma +0.05, xend = 0 + 3 * sigma -0.05, y = 0.55, yend = 0.55, alpha = 0.4, arrow = arrow(length = unit(0.015, \"npc\"), ends = \"both\")) +\n   annotate(\"text\", x = 0, y = 0.56, label = \"99.7%\") +\n  ylim(c(0, 0.6)) +\n  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), labels = c(expression(mu ~ \"- 3\" ~ sigma), expression(mu ~ \"- 2\" ~ sigma), expression(mu ~ \"-\" ~ sigma), expression(mu), expression(mu ~ \"+\" ~ sigma), expression(mu ~ \"+ 2\" ~ sigma),  expression(mu ~ \"+ 3\" ~ sigma))) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n        axis.title.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank(),\n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(),\n        plot.caption = element_text(hjust = 0)) \n        \n# Average income distribution of 10,000 samples of 40 customers (thousands, $) ---------------\n\nset.seed(0)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 10, label = paste0(\"Mean: \", round(average_income, 2))) \n\n# Exemplary possible sample means distributions ---------------\n\nlibrary(gganimate)\nsample_mean &lt;- 134.320\nstandard_error &lt;- sd(sample40_means)\n\nsample_means_modified &lt;- data.frame(Income = NA, possible_mean_sample = NA)\npossible_mean_sample &lt;- seq(sample_mean - 3 * standard_error, sample_mean + 3 * standard_error, length.out = 7)\npossible_mean_sample[2] &lt;- 120.950\n\nfor (possible_mean in possible_mean_sample) {\n  new_sample_mean &lt;- data.frame(Income = sample_means$Income - mean(sample_means$Income) + possible_mean)\n  new_sample_mean$possible_mean_sample &lt;- possible_mean\n  sample_means_modified &lt;- rbind(sample_means_modified, new_sample_mean)\n}\n\nsample_means_modified &lt;- sample_means_modified[-1,]\nsample_means_modified$possible_mean_sample &lt;- round(sample_means_modified$possible_mean_sample, 2)\nsample_means_modified$title &lt;- paste0(\"Assuming samples extracted from a population \\nwith M = \",sample_means_modified$possible_mean_sample, \" (SEM = \", round(standard_error,2), \")\")\nsample_means_modified$title &lt;- as.factor(sample_means_modified$title)\n\npossible_mean_sample &lt;- data.frame(Mean_value = possible_mean_sample, title = unique(sample_means_modified$title))\n\n\np &lt;- ggplot(sample_means_modified, aes(x = Income)) +\n  annotate(\"rect\", xmin = sample_mean - 3 * standard_error, xmax = sample_mean + 3 * standard_error,  ymin = 0, ymax = 0.075, alpha = .1) +\n  annotate(\"text\", x = sample_mean - 3 * standard_error * 1.05, y = 0.025, label = \"Sample Mean - 3SE\", alpha = 0.3, angle = 90)  +\n  annotate(\"text\", x = sample_mean + 3 * standard_error * 1.05, y = 0.025, label = \"Sample Mean + 3SE\", alpha = 0.3, angle = -90)  +\n  annotate(\"text\", x = sample_mean, y = 0.077, label = \"Sample Mean \", alpha = 0.3)  +\n  geom_vline(xintercept = sample_mean, size = 0.4, alpha = 0.4, linetype = \"dashed\") +\n  geom_density(fill = \"steelblue\", alpha = 0.4) +\n  geom_vline(data = possible_mean_sample, aes(xintercept = Mean_value), color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = possible_mean_sample, aes(x = Mean_value * 1.08, y = 0.05, label = paste0(\"Mean: \", round(Mean_value, 2)))) + \n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Probability Density\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0, size = 12),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    transition_states(title, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n# Error Distribution of the Means for 10,000 Samples of 40 Customers (thousands, $) ---------------\n\nsample_means_error &lt;- sample_means\nsample_means_error$error &lt;- sample_means$Income - mean(sample_means$Income)\n\nsd_sample_means_error &lt;- sd(sample_means_error$error)\n\n\nggplot(sample_means_error, aes(x = error)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Error (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = 3.2, y = 11, label = paste0(\"Mean: 0\\nSD: \", round(sd_sample_means_error, 2))) \n   \n# Sampling error of the mean measured in standard errors distribution for 10,000 Samples of 40 Customers ---------------\n\nsample_means_error$error_div_sd &lt;- sample_means_error$error / sd(sample_means$Income)\n\nsd_sample_means_error &lt;- sd(sample_means_error$error_div_sd)\n\n\n\nggplot(sample_means_error, aes(x = error_div_sd)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Error measured in standard errors\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = 1, y = 11, label = paste0(\"Mean: 0\\nSD: \", round(sd_sample_means_error, 2))) \n\n# Sampling error of the mean measured in estimated standard errors distribution for 100,000 Samples of varying number of Customers ---------------\n\nset.seed(34)\nsample_sizes &lt;- c(seq(2, 10, by = 1), seq(20, 60, by = 20))\nsamples_different_sizes &lt;- data.frame(values = NA, sizes = NA, sd = NA)\nfor(sample_size in sample_sizes){\n  \n  sample_means_diff_size &lt;- numeric(length = 100000)\n\n  for(i in seq(1, 100000)){\n     sample_means_diff_size[i] &lt;- mean(sample(customer_data$Income, sample_size))\n     \n  }\n  samples_different_sizes2 &lt;- data.frame(values = sample_means_diff_size, sizes = sample_size, sd = sd(sample_means_diff_size))\n  samples_different_sizes &lt;- rbind(samples_different_sizes, samples_different_sizes2)\n \n}\n\nsamples_different_sizes &lt;- samples_different_sizes[-1,]\n\n\nsamples_different_sizes$error_divided_by_sd &lt;- (samples_different_sizes$values - mean(customer_data$Income)) /(samples_different_sizes2$sd/sqrt(samples_different_sizes2$sizes))\n\n\nsamples_different_sizes$sizes &lt;- as.factor(samples_different_sizes$sizes)\nlevels(samples_different_sizes$sizes) &lt;- paste(\"Number of observations per sample:\", levels(samples_different_sizes$sizes))\ndata_error_div_sd_average &lt;- samples_different_sizes %&gt;%\n  summarise(mean_error_div_sd = mean(error_divided_by_sd), .by = sizes)\n\np &lt;- ggplot(samples_different_sizes, aes(x = error_divided_by_sd, fill = sizes)) +\n  geom_density() +\n  labs(\n    x = \"Error measured in estimated standard errors\",\n    y = \"Probability Density\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_manual(values = c(\n  \"#006699\", \"#FF9E00\", \"#B5113E\", \"#3B125F\", \"#007F7B\", \"#1A4C3C\",\n  \"#7C0A02\", \"#00567F\", \"#7500A4\", \"#B760DE\", \"#002E3E\", \"#7F7F7F\"\n)) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(sizes) +\n  enter_fade() +\n  view_zoom_manual(xmin = c(-120, -100,  -90,  -75,  -70,  -70,  -60, -60, -60,  -45,  -30,  -30), xmax = -1 * c(-120, -100,  -90,  -75,  -70,  -70,  -60, -60, -60,  -45,  -30,  -30), ymin = rep(0, 12), ymax = c(rep(0.02, 5), rep(0.025, 3), 0.03,  0.035, 0.05, 0.06)) +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 25)\nanim"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html",
    "title": "Statistics Foundations: Populations and samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#the-power-of-statistics",
    "title": "Statistics Foundations: Populations and samples",
    "section": "",
    "text": "Statistics enables us to distill valuable insights from unstructured information, commonly referred to as data. This acquired knowledge empowers us to develop a deeper comprehension of the subject matter at hand, facilitating the exploration of questions that span a wide spectrum, such as:\n\nWhat is the profile of our customer base?\nDo our Swiss consumers exhibit a higher average expenditure compared to their Norwegian counterparts?\nDoes consistent alcohol consumption correlate with an elevated risk of experiencing a heart attack?\n“What magnitude of sales increase can I anticipate for the upcoming year with a 20% boost in advertising expenditure?"
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#the-ideal-of-population-and-the-reality-of-sampling",
    "title": "Statistics Foundations: Populations and samples",
    "section": "The ideal of population and the reality of sampling",
    "text": "The ideal of population and the reality of sampling\nTo comprehensively explore and enhance our understanding of a given issue, it is ideal to possess data pertaining to all entities impacted by that issue. For instance, in our pursuit of gaining deeper insights into our customer base, the ideal scenario involves having access to data for every single one of our customers. In this comprehensive dataset, we would find detailed information regarding each customer’s age, income, purchasing history, and other relevant attributes. Such a dataset would empower us to attain a holistic understanding of our customer base’s profile. In statistical terms, when we allude to data encompassing all entities affected by the issue of interest, we are referring to the population. Naturally, this population is contingent upon the specific focus of interest. For instance, if our objective shifted from understanding the profile of our overall customer base to that of our Swiss customers specifically, our population would comprise solely our Swiss customer subset.\nNevertheless, acquiring data for every single entity influenced by the matter of interest may often prove to be impractical, primarily due to the substantial expenses associated with such an undertaking or its constantly changing nature. Therefore, in the field of statistics, we operate with subsets derived from this population. Ideally, these subsets are constructed in such a way that each entity within them has an equal probability of being selected. Consequently, the resultant subset, known as a sample, is smaller in scale and serves as a reliable and representative image of the broader population. The fundamental concept underlying this approach is that through the observation and analysis of this sample, we can draw meaningful conclusions about the entire population, a process known as inference.\n\nLimitations and potential bias in sampling\nNonetheless, utilizing samples entails operating with an imperfect representation of the population—a mere approximation that may deviate from the true population characteristics. Even when each individual possesses an equal probability of selection, the random nature of the process can result in the overrepresentation or underrepresentation of certain specific types or groups of entities within our sample, thereby potentially leading to skewed findings and conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#sampling-in-practice-exploring-our-consumer-base-annual-income",
    "title": "Statistics Foundations: Populations and samples",
    "section": "Sampling in practice: Exploring our consumer base annual income",
    "text": "Sampling in practice: Exploring our consumer base annual income\nTo illustrate these concepts, let’s delve into an example using a dataset from Kaggle. This dataset comprises data about 2,000 supermarket customers, encompassing a range of characteristics such as age, annual income, and education level. For the sake of this demonstration, let’s envision that this dataset encapsulates information about every single one of our customers, effectively serving as a representation of our customer population. Now, let’s suppose our objective is to gain a more profound insight into the annual income distribution among our customers.\n\nAnnual income distribution for our population\nHence, we can promptly delve into the examination of the data distribution, which we can observe through a histogram, depicted in Figure 1. This analysis unveils that the distribution of customer incomes exhibits an approximate normality, featuring an average annual income of $120,950 with a moderate degree of variability (Standard Deviation = $38,110). This implies that a substantial proportion of customers have incomes that closely align with the mean value, while fewer customers fall within the income extremes. Additionally, it is worth noting a slight rightward skew in the plot, indicating a minority of individuals with substantially higher incomes compared to the majority.\n\n\n\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\n\n\n\n\n\n\nLog-normal distribution\n\n\n\n\n\nIn the earlier paragraph, we highlighted that the “distribution of customer incomes exhibits an approximate normality.” To be more precise, the distribution we are discussing is formally identified as the log-normal distribution. Although it visually resembles a normal distribution, featuring a slightly bell-shaped curve, the log-normal distribution distinguishes itself by having a lower bound at zero and a positively skewed nature, leading to a more elongated right tail. The name “log-normal” stems from the phenomenon that transforming the variable into its logarithm results in a normal distribution. Figure 2 visually presents the original distribution of the customer’s annual income on the left and, on the right, showcases the result of applying a logarithmic transformation to the variable. This illustration vividly demonstrates how the logarithm transformation effectively transforms the distribution into a normal distribution.\n\n\n\n\n\n\n\n\nFigure 2: Original (left) and logarithmically transformed (right) customer annual income distributions\n\n\n\n\n\n\n\n\n\n\n“Collecting” a small customer sample and exploring their annual income\nHaving briefly explored the distribution of our customers’ annual income population, we must acknowledge the practical challenge of obtaining data from the entire population. Therefore, we opt to acquire a representative sample, a feasible alternative. In this scenario, we decided to select an easily obtainable sample of 20 customers, each having an equal probability of being included in the sample. Following the acquisition of this sample, we analyze their annual income distribution, which can be observed in Figure 3, revealing several noteworthy disparities.\nFirstly, we observe that certain income brackets, present in the population data, remain absent in our sample. Additionally, we notice a higher proportion of high-income customers in comparison to the population. These disparities culminate in a higher average customer annual income (Mean = $137,320) and increased variability (Standard Deviation = $47,770) within the sample. Consequently, if we were to draw inferences based on this data, our conclusions would suggest that individuals in the sample exhibit a higher average income and greater income variability compared to the population, thus drawing into error.\n\n\n\n\n\n\n\n\nFigure 3: Customer annual income distribution for our sample with 20 observations (thousands, $)\n\n\n\n\n\nDespite these disparities, our sample offers us an initial glimpse into the broader income distribution of the entire population. While our sample may not perfectly mirror the population due to its size and inherent limitations, it serves as a foundational reference point for comprehending income patterns within the larger group. It grants us a preliminary understanding of the income ranges, tendencies, and variations we can anticipate when considering the overall population’s income distribution. However, it’s essential to acknowledge the presence of these disparities and recognize that they may impact the conclusions we can draw regarding the population.\n\n\nCapturing nuances better with increased sample size\nThese observed disparities are unsurprising, given the inherent limitations of capturing the intricacies of our data with a small sample of just 20 consumers. Consequently, we observe the absence of individuals in various income brackets and a skewed composition compared to our population (which, in this hypothetical case, we have knowledge of, but in practice, we might not).\nOne might naturally question whether increasing the sample size could enhance the richness of our sample and consequently enable us to better capture the nuances present in the population. This would ultimately result in a more faithful representation. To investigate this, we will generate samples of varying sizes, specifically seven additional samples consisting of 40, 80, 120, 150, 300, 500, and 1000 randomly selected consumers from our population. Figure 4 visually illustrates, through histograms, how the distribution changes for each of these sample sizes, including the initially created one with 20 consumers.\n\n\n\n\n\n\n\n\nFigure 4: Customer annual income distribution for varying sample sizes (thousands, $)\n\n\n\n\n\nIn this Figure, we can discern that larger sample sizes excel in capturing the subtleties inherent in the population’s data distribution. Notably, the distribution of bigger samples closely mirrors that of the population, with fewer missing income ranges and diminished disparities.\nWhen we work with larger samples, we effectively broaden our scope of observation, incorporating a more diverse range of data points. This expanded sample size minimizes the influence of random variation and offers a more robust representation of the population. In essence, larger samples provide a more comprehensive cross-section of the population, enhancing our ability to accurately capture the underlying patterns, variations, and nuances present in the data.\nNow, you might be wondering: what constitutes the ideal sample size? There is not a one-size-fits-all answer; instead, the sample size should strike a balance. It must be large enough to capture the nuances of the population while still being feasible to acquire within budget and time constraints. It is crucial to recognize that a sample will inherently exhibit differences from the population, a fundamental aspect of statistical analysis."
  },
  {
    "objectID": "posts/2021/statistics-foundations-population-and-sample/index.html#summary",
    "href": "posts/2021/statistics-foundations-population-and-sample/index.html#summary",
    "title": "Statistics Foundations: Populations and samples",
    "section": "Summary",
    "text": "Summary\n\nStatistics empowers us to glean valuable insights from data, enriching our comprehension of issues of interest.\nIdeally, a complete understanding of any issue necessitates data from every entity involved, referred to as the population.\nPractical constraints often require us to work with smaller, representative subsets, known as samples, where each entity in the population has an equal chance of inclusion.\nSamples, while essential for making inferences about the population, have inherent limitations due to their size, as they can’t fully capture the population’s intricacies, introducing errors into our inferences.\nLarger samples excel at capturing population nuances, offering a more faithful representation of its characteristics.\n\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gganimate)\nlibrary(scales)\n\n# Read customer data from CSV file and adjust income values\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income / 1000\n\n# Calculate the average and standard deviation of the income for the entire population\naverage_income_population &lt;- mean(customer_data$Income)\nstd_deviation_population &lt;- sd(customer_data$Income)\n\n# Create and display a histogram for the entire population's income distribution\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data$Income, na.rm = TRUE) * 0.95, y = 12, \n           label = paste0(\"Mean: \", round(average_income_population, 2), \"\\nSD: \", round(std_deviation_population, 2))) \n\n# Set a random seed for reproducibility\nset.seed(150)\n\n# Create a random sample of 20 observations from the population\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 20), ]\n\n# Calculate the average and standard deviation of the income for the sample\naverage_income_sample &lt;- mean(customer_data_sample$Income)\nstd_deviation_sample &lt;- sd(customer_data_sample$Income)\n\n# Create and display a histogram for the sample's income distribution\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  annotate(\"text\", x = max(customer_data_sample$Income, na.rm = TRUE) * 0.95, y = 13, \n           label = paste0(\"Mean: \", round(average_income_sample, 2), \"\\nSD: \", round(std_deviation_sample, 2))) \n\n\n\n\n\n## Create samples of different sizes and create animation with their distribution\n\n# Create an empty data frame to store the samples and add a column for observations\ncustomer_data_samples &lt;- data.frame()\ncustomer_data_samples$Observations &lt;- character(0)\n\n# Define the sample sizes\nsample_sizes &lt;- c(20, 40, 80, 120, 150, 300, 500, 1000)\n\n# Set a random seed for reproducibility\nset.seed(350)\n\n# Loop through each sample size\nfor (sample_size in sample_sizes) {\n  # Create a random sample of the specified size\n  customer_data_sample &lt;- customer_data[sample(nrow(customer_data), sample_size), ]\n  \n  # Create a label for the observations indicating the sample size\n  customer_data_sample$Observations &lt;- paste0(sample_size, \" Observations\")\n  \n  # Append the sample to the data frame\n  customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n}\n\n# Add a label for the population\ncustomer_data$Observations &lt;- \"Population\"\n\n# Append the population data to the data frame\ncustomer_data_samples &lt;- rbind(customer_data_samples, customer_data)\n\n# Convert the \"Observations\" column to a factor with custom labels\ncustomer_data_samples$Observations &lt;- factor(\n  customer_data_samples$Observations,\n  levels = c(\"20 Observations\", \"40 Observations\", \"80 Observations\", \"120 Observations\", \"150 Observations\", \"300 Observations\", \"500 Observations\", \"1000 Observations\", \"Population\")\n)\n\n# Loop through the levels of the \"Observations\" factor and update labels\nfor (i in seq_along(levels(customer_data_samples$Observations))) {\n  sample_type &lt;- levels(customer_data_samples$Observations)[i]\n  subset &lt;- customer_data_samples[customer_data_samples$Observations == sample_type, \"Income\"]\n  average_income &lt;- round(mean(subset), 2)\n  std_deviation_income &lt;- round(sd(subset), 2)\n  \n  # Update labels with mean and standard deviation information\n  if (i &lt;= 8) {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      levels(customer_data_samples$Observations)[i],\n      \"Sample\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  } else {\n    levels(customer_data_samples$Observations)[i] &lt;- paste(\n      \"Population\\n(M = \", average_income, \", SD = \", std_deviation_income, \")\"\n    )\n  }\n}\n\n#Create and animate the plot\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = percent_format()) +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 150, duration = 15)\nanim"
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html",
    "href": "posts/2021/statistics-foundations-sample-error/index.html",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#building-on-previous-insights-recap-from-our-previous-post",
    "title": "Statistics Foundations: Sampling error",
    "section": "",
    "text": "In our previous post on the Statistics Foundations series, we highlighted the potency of statistics as a valuable tool for further understanding issues of interest through data. To do so, we would ideally want to study the whole population, which represents the complete collection of entities affected by the issue under investigation. However, in most cases, obtaining data for every entity within this population is simply unfeasible due to constraints such as cost and logistics or even impossible. Consequently, we employ a strategy of working with samples—subsets of this population that are randomly selected in a manner ensuring that every entity has an equal probability of being selected.\nThese samples, though more manageable in size, serve as our window into the broader population, enabling us to draw conclusions and glean insights about the population—a process known as inference. However, it’s important to acknowledge that working with samples introduces a critical challenge: the inherent limitations stemming from their smaller size in comparison to the population. As a consequence, we inevitably encounter errors in our analyses.\nThe essence of these errors lies in the inability of small samples to capture all the intricate nuances present within the population. While we can gain valuable insights and broad trends from our samples, the finer details and subtle variations within the population often elude our grasp. Thus, we find ourselves contending with sampling errors that can influence the accuracy of our conclusions."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#examining-sampling-errors-in-data-summarization-the-case-of-the-mean",
    "title": "Statistics Foundations: Sampling error",
    "section": "Examining Sampling Errors in Data Summarization: The Case of the Mean",
    "text": "Examining Sampling Errors in Data Summarization: The Case of the Mean\nAt the core of statistical analysis lies the foundational task of data summarization—a process that condenses data into a concise and understandable format. This fundamental procedure yields a clear and easily comprehensible overview of the data, facilitating a straightforward grasp of its essential characteristics.\nAmong these essential characteristics, two prominently stand out: central tendency and variability. Central tendency relates to the value around which the different data points cluster around. Conversely, variability quantifies the extent to which data points deviate from this central point, providing insights into the dispersion or spread of data relative to its central location. For instance, the mean serves as an example of a central tendency measure, while the standard deviation exemplifies a variability measure.\nTo visually illustrate the impact of sampling error, we will once again utilize the Kaggle dataset used in our previous post. This dataset contains information on 2,000 supermarket customers, including their age, annual income, and education level. For the purposes of this analysis, we will assume that this dataset represents our entire customer population.\nLet’s envision a scenario: our objective is to rapidly glean insights into the annual income of our customers. One straightforward strategy to achieve this is by computing the mean income, which furnishes us with a succinct metric representing the central tendency around which the majority of our customers’ annual incomes gravitate. In this endeavor, we observe that the mean annual income stands at a noteworthy $120,950, serving as a prominent reference point around which the annual incomes of our customers tend to concentrate.\n\n\n\n\n\n\n\n\nFigure 1: Customer annual income distribution (thousands, $)\n\n\n\n\n\n\nSampling 40 customers and calculating their annual income mean\nIn this hypothetical case, we possess information about the sample. Consequently, we can obtain information about our population without any error by directly observing it. Therefore, now we know that our population has an average annual income of $120,950. However, in real-life scenarios, and as previously said, obtaining data for the whole population may be unfeasible or even not possible. For this reason, we will assume that we extract a random sample of 40 customers and compute the mean annual income from this sample.\n\n\n\n\n\n\n\n\nFigure 2: Customer annual income distribution for our sample with 40 observations (thousands, $)\n\n\n\n\n\nAs observed, in Figure 2, the mean value within this sample diverges from that of the broader population. Specifically, the mean for this sample stands at $137,320, contrasting with the population mean of $120,950. This difference amounts to $16,370, and it encapsulates what we commonly refer to as “error.” Notably, in this instance, we possess knowledge about the population, allowing us to discern this difference.\nFor this reason, the terminology we use to describe the metrics summarizing the data characteristics varies depending on whether they are computed within the population or a sample. In the former case, they are referred to as parameters, whereas in the latter, they are known as statistics. In statistical notation, parameters are typically denoted by Greek letters, such as \\(\\sigma\\) for the standard deviation or \\(\\mu\\) for the mean, while statistics are denoted by Latin letters, such as \\(m\\) for the mean and \\(s\\) for the standard deviation.\n\n\nRandomness and sampling: Extracting several means of 40 observations\nMoreover, it’s crucial to note that this sample was derived through a process of random selection. In other words, we randomly picked 40 customers from our population, ensuring that each customer had an equal likelihood of being included. This randomness implies that if we were to generate another sample of 40 customers, it would be improbable for this new sample to mirror the exact composition of the previous one or yield the same mean.\nFigure 3 illustrates the annual income distribution of various samples, each consisting of 40 customers randomly selected from our initial population (including the sample we previously examined). It becomes evident that the distribution undergoes fluctuations across these diverse samples. Consequently, this variability gives rise to a spectrum of computed means, ranging from as low as $106,600 to as high as $137,320.\n\n\n\n\n\n\n\n\nFigure 3: Customer annual income distribution for different samples of 40 customers (thousands, $)\n\n\n\n\n\n\n\nDigging deeper: Exploring mean customer annual income with 10,000 different 40 samples\nTo deepen our comprehension of the variance in computed means, we embark on a more extensive analysis by replicating the previous procedure but on a much larger scale: generating precisely 10,000 samples, each composed of 40 individuals. For every one of these 10,000 samples, we compute the mean annual income. The resulting distribution of these 10,000 means, each originating from distinct samples of 40 individuals randomly selected from our complete population, is visually represented in Figure 4 through a histogram.\n\n\n\n\n\n\n\n\nFigure 4: Average income distribution of 10,000 samples of 40 customers (thousands, $)\n\n\n\n\n\nFigure 4 reveals significant insights. Notably, there is a substantial variation in the computed average annual incomes across the various samples, spanning an extensive spectrum from $100,684 to $145,543. This disparity translates into an error range spanning from -$20,266 to $24,593 in contrast with the population’s mean.\nNonetheless, an intriguing revelation emerges from this analysis. Despite the marked variability in sample means, the overall average of these mean annual incomes, drawn from distinct samples, precisely mirrors the population average. This observation means that the average incomes for the different samples consistently cluster around this point.\nMoreover, it is worth noting that the distribution of annual income means extracted from these various samples adheres to a bell-shaped pattern, commonly known as a normal distribution. This pattern signifies that as we move farther away from the population average, the number of observations gradually diminishes.\nTaken together, this implies that, in most instances, the mean annual income estimated from our sample tends to be closer rather than farther away from the population’s mean annual income. Nevertheless, it’s important to acknowledge that there are still situations where significant deviations from the sample mean can occur. The key concern here lies in the fact that if we lacked information about the population and solely possessed a sample with an annual income mean of $145,543, we might mistakenly conclude that, on average, our customers are wealthier than they actually are.\n\n\nIncreasing sample size and analyzing mean annual income distribution of 10,000 samples\nAs previously mentioned, small samples encounter challenges in capturing the subtleties present within the population. Consequently, the larger the sample size, the more effectively we can apprehend these nuances. To illustrate this, we investigate how the variance of computed means changes when we collect samples of 150 customers, as opposed to the previous samples of 40 customers.\nOnce again, we generate 10,000 samples, each containing 150 customers, and calculate the mean annual income for each of these samples. Subsequently, we visualize the distribution of these mean annual incomes for these larger samples by creating a histogram.\nFigure 5 provides a visual comparison between the distributions of means computed using 10,000 samples, each with 40 observations, and another set of samples, each comprising 150 observations.\n\n\n\n\n\n\n\n\nFigure 5: Average income distribution of 10,000 samples of 40 customers vs 10,000 samples of 150 customers (thousands, $)\n\n\n\n\n\nFigure 5 provides insights akin to those observed with 40 observations: the distribution of means exhibits a bell-shaped pattern, with the average closely approximating the population mean. However, a significant distinction emerges: the range within which sample means deviate from the overall mean, equivalent to the population average, is notably narrower. In essence, the variability is considerably reduced, indicating that the margin for error when using samples of 150 observations is substantially smaller than that with samples of 40.\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\n\n\nIn the previous examples, we’ve observed that when we calculate means from various samples, these means tend to follow a particular pattern – a bell-shaped distribution known as a normal distribution. This is not just a coincidence; it’s a fundamental concept in statistics called the Central Limit Theorem.\nThe Central Limit Theorem tells us that, regardless of the original shape of the data distribution, when we repeatedly draw samples of sufficient size from that data and calculate their means, those sample means will follow a normal distribution. This is a powerful idea because it allows us to make certain assumptions and conduct statistical analyses even when we don’t know the shape of the population’s distribution.\n\n\n\nIn this scenario, the distribution of means spans from $109,374 to $132,064, resulting in an error range of -$11,576 to $11,114 relative to the population mean. This range is significantly tighter compared to the error range obtained from samples of 40, where the deviation ranged from -$20,266 to $24,593."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#measuring-the-sampling-error",
    "title": "Statistics Foundations: Sampling error",
    "section": "Measuring the sampling error",
    "text": "Measuring the sampling error\nThrough the repetitive extraction of samples of consistent size from a given population, as demonstrated in our previous examples (with both 40 and 150-sized samples), we gain valuable insights into the potential magnitude of errors associated with samples of a particular size.\nAs we’ve witnessed, the average of multiple means calculated from samples of identical size closely aligns with, or is essentially identical to, the true mean of the population. Consequently, the spread or dispersion of these computed means from this point provides a measure of the magnitude of the sampling error. Put simply, the variability in the mean derived from multiple samples of equal size offers a quantifiable measure of the magnitude of the sampling error for samples of that particular size. This measure is commonly known as the standard error of the mean (which we will abbreviate as SEM).\nMathematically, we can express this as the standard error of the mean being equal to the standard deviation of the means of the different samples. Specifically, we prefer using the standard deviation rather than the variance because the former has the same units as the mean, while the latter has squared units. For instance, in the case of annual income, the units for variance would be in dollars squared (\\(\\$^2\\)), while for the standard error, it’s just in dollars ($). In other words:\n\\(\\sqrt{Var(\\bar{X})} = SEM\\)\nThis expression can be translated into the following form:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\nHere, \\(σ\\) represents the population standard deviation and \\(n\\) is the sample size. Establishing an inverse relationship between the standard error and the sample size: as the sample size increases, the standard error decreases. This principle aligns with our intuitive understanding, as seen in the previous post for the statistics foundations series, and as visually depicted in Figure 5.\n\n\n\n\n\n\nStandard Error Derivation\n\n\n\n\n\nLet’s recall that the mean of any variable is equal to the sum of the values of each observation of that variable divided by the total number of observations (which equals our sample size): \\(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\). Therefore, we can rewrite the previous formula as follows:\n\\(\\sqrt{\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nWe also know that the variance of a random variable multiplied by a constant “a” is equal to the variance of that variable multiplied by the square of that constant, i.e., \\(Var(aR) = a^2Var(R)\\), where \\(a\\) is a constant, and \\(R\\) is a random variable. This means that:\n\\(\\sqrt{\\frac{1}{n^2}\\text{Var}\\left(\\sum_{i=1}^{n}X_i\\right)} = SEM\\)\nAdditionally, when dealing with a set of pairwise independent random variables (where the variability in one doesn’t depend on the others, as in our case), the variance of their sum is equal to the sum of their individual variances, i.e., \\(\\text{Var}[R_1 + R_2 + \\cdots + R_n]\\) \\(=\\) \\(\\text{Var}[R_1] + \\text{Var}[R_2] + \\cdots + \\text{Var}[R_n]\\), where \\(R_1, R_2,…, R_n\\) are pairwise independent random variables. This allows us to rewrite the formula for SEM as:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}Var(X_i)} = SEM\\)\nLet’s remember that our individual variables, denoted as \\(X_i\\), come from a population with variance equal to \\(\\sigma^2\\). So, our formula becomes:\n\\(\\sqrt{\\frac{1}{n^{2}}\\sum_{i = 1}^{n}\\sigma^2} = SEM\\)\nSince we’re summing up \\(n\\) identical values, we can simplify further:\n\\(\\sqrt{\\frac{1}{n^{2}}n\\sigma^2} = SEM\\)\nUltimately, this can be further simplified to:\n\\(\\frac{\\sigma}{\\sqrt{n}} = SEM\\)\n\n\n\nApplying these formulas, we can now calculate the Standard Error of the mean for sample sizes of 40 and 150. When we compute the standard deviation of the means obtained from multiple samples of 40 customers, we obtain a value of 5.86. Conversely, for samples of 150 customers, we obtained a value of 3.01, which is approximately two times smaller in terms of standard error.\nAlternatively, we can utilize the formula that links the standard error to the population’s standard deviation and the sample size, represented as \\(\\frac{\\sigma}{\\sqrt{n}}\\). Given our population’s standard deviation for annual income is 38.11, dividing this by \\(\\sqrt{40}\\) yields a value of 6.03 for a sample size of 40, while dividing it by \\(\\sqrt{150}\\) results in a value of 3.11 for a sample size of 150.\nHowever, it’s important to note that there are disparities between the results obtained from these two approaches. This is because the first formula relies on a finite number of samples (10,000 in this case), and to obtain equivalent values, the number of samples would need to tend towards infinity, meaning a significantly larger amount of samples.\n\nEstimating the standard error\nUp until now, we have seen that we can compute the standard error of the mean by taking multiple samples of the same size from the population, calculating their means, and extracting the variability of such means.\nYet, let’s face it—in the real world, resources are finite, and repeatedly plucking samples from the population to estimate the standard error can be an extravagant expenditure of these precious assets. Instead, it’s often a more judicious allocation of resources to channel our efforts into amassing a larger sample. As we’ve come to appreciate, a larger sample which better captures the nuances of the population, decreasing the sampling error.\nIn addition to the aforementioned method, we’ve also explored an alternative approach for calculating the standard error—one that bypasses the need to repeatedly extract multiple samples from the population. This alternative method involves dividing the population’s standard deviation by the square root of the sample size (\\(\\frac{\\sigma}{\\sqrt{n}}\\)). However, it’s essential to note that this formula hinges on having access to information about the entire population. This requirement underscores the very reason why we find ourselves seeking to compute the standard error in the first place—a challenge born out of the impracticality of obtaining data for the entire population, compelling us to work with samples and consequently introducing the sampling error.\nNonetheless, it’s crucial to remind ourselves that the core objective when working with a sample is to glean insights into the larger population and derive meaningful conclusions from it. Within this context, it’s reasonable to assume that the standard deviation we observe within our sample (\\(s\\)) can serve as a dependable proxy for the population’s standard deviation (\\(\\sigma\\)). This assumption empowers us to substitute the population standard deviation in the traditional formula (\\(\\frac{\\sigma}{\\sqrt{n}}\\)) with the sample standard deviation (\\(s\\)) derived from that very population:\n\\(SE \\approx \\frac{s}{\\sqrt{n}}\\)\nBy making this substitution, we arrive at an approximation for the standard error. It provides us with a measure of the potential error we may encounter when drawing conclusions from a sample, all without the need for complete information about the population."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#a-final-note",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#a-final-note",
    "title": "Statistics Foundations: Sampling error",
    "section": "A final note",
    "text": "A final note\nThroughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. Throughout this post, we’ve centered our attention on the standard error of the mean. Yet, it’s imperative to acknowledge that other statistics, such as variance and standard deviation, likewise harbor their own standard errors. When we compute these statistics from a sample, the values we obtain can deviate from those of the population, consequently introducing an element of error into our analyses.\nHowever, it’s crucial to acknowledge that the formulas we’ve previously derived for calculating the standard error aren’t universally applicable to all statistics. These formulas have been derived with the mean as their reference point. Nevertheless, it’s worth noting that the fundamental concept behind deriving formulas for computing the standard error for other statistics remains consistent: It is based on the variability of a specific statistic obtained from various samples of the same size."
  },
  {
    "objectID": "posts/2021/statistics-foundations-sample-error/index.html#summary",
    "href": "posts/2021/statistics-foundations-sample-error/index.html#summary",
    "title": "Statistics Foundations: Sampling error",
    "section": "Summary",
    "text": "Summary\n\nSampling is necessary because obtaining data for the entire population is often impractical.\nThe use of samples introduces the challenge of sampling errors.\nSampling errors arise because small samples cannot capture all the nuances present in the population.\nThis leads to variations in common measures like the mean between the sample and the population.\nTo illustrate this, we simulated the extraction of 10,000 samples of the same size from our exemplary population and observed that:\n\nThe mean from different samples exhibits variability.\nDespite this variability, the average of mean values from various samples tends to closely align with the population average.\nA normal distribution pattern is evident in the distribution of annual income means from different samples, indicating that samples with means close to the population mean are more likely.\nLarger sample sizes reduce the standard error and yield more accurate estimates, as they better capture the population’s intricacies.\n\nWhen extracting multiple samples of the same size from a population and calculating their means, the average of these sample means corresponds to the population mean. This allows us to quantify the degree of error associated with that sample size by measuring their variability, i.e., how much they deviate from the population mean.\nStandard error of the mean (SEM) is mathematically linked to the population’s standard deviation and sample size.\nIn practice, we estimate the SEM using our sample’s standard deviation.\nIncreasing the sample size results in a smaller standard error.\nStandard error serves as a valuable tool for quantifying the precision of sample-based estimates and is essential for robust statistical analysis.\n\n\n\nCode\n## Reading the \"population\" and visualizing it\nlibrary(gganimate)\nlibrary(tidyverse)\nlibrary(ggthemes)\n\n#Read customer data\ncustomer_data &lt;- read.csv(\"assets/customer.csv\")\ncustomer_data$Income &lt;- customer_data$Income/1000\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data$Income)\n\nggplot(customer_data, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.25, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n  \n## Sampling 40 customers and calculating their annual income mean + visualizing their distribution\n\nset.seed(150)\ncustomer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40), ]\n\n#Compute the average and standard deviation of the income\naverage_income &lt;- mean(customer_data_sample$Income)\n\nggplot(customer_data_sample, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count) * 100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.15, y = 13, label = paste0(\"Mean: \", round(average_income, 2))) \n   \n ## Extracting several 40 customers and calculating their annual income mean + visualizing their distribution\n \n customer_data_samples &lt;- customer_data_sample\ncustomer_data_samples$Average_Sample &lt;- \"Sample 1\"\n\nseeds &lt;- seq(300, 5500, length.out = 8)\n\ni &lt;- 2\nfor(seed in seeds) {\n    \n    set.seed(seed)\n    customer_data_sample &lt;- customer_data[sample(nrow(customer_data), 40),]\n    row.names(customer_data_sample) &lt;- NULL\n    average_income &lt;- round(mean(customer_data_sample$Income), 2)\n    customer_data_sample$Average_Sample &lt;- paste0(\"Sample \", i)\n    customer_data_samples &lt;- rbind(customer_data_samples, customer_data_sample)\n    i &lt;- i + 1\n}\n\n\ndata_income_average &lt;- customer_data_samples %&gt;%\n  summarise(meanIncome = mean(Income), .by = Average_Sample)\n\n\np &lt;- ggplot(customer_data_samples, aes(x = Income, fill = Average_Sample)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = meanIncome * 1.25, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Average_Sample, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 20)\nanim\n\n## Extracting 10,000 samples of 40 observations and calculate their mean + visualize the distribution of the means\nset.seed(3)\n# Create an empty numeric vector of length 10000 named 'sample40_means'\nsample40_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 40 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 40)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample40_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample40_means), 2)\nsample_means &lt;- data.frame(Income = sample40_means)\n\n\nggplot(sample_means, aes(x = Income)) +\n  geom_histogram(aes(y = after_stat(count / sum(count)*100)), fill = \"steelblue\") +\n  geom_vline(xintercept = average_income, color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  labs(\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  theme(plot.title = element_text(hjust = 0),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)) +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n   annotate(\"text\", x = average_income * 1.05, y = 9, label = paste0(\"Mean: \", round(average_income, 2))) \n\n## Extracting 10,000 samples of 150 observations and calculate their mean + visualize the distribution of the means\n\nset.seed(1500)\n# Create an empty numeric vector of length 10000 named 'sample_means'\nsample_means &lt;- numeric(length = 10000)\n\n# Generate a for loop iterating 10000 times\n# For each iteration, the variable 'i' takes the value of the current iteration\nfor (i in 1:10000) {\n  # Generate a sample of 150 observations from the Income variable\n  sample_population &lt;- sample(customer_data$Income, 150)\n  # Calculate the mean of the sample and store it in the 'i' position of the 'sample_means' vector\n  sample_means[i] &lt;- mean(sample_population)\n}\n\naverage_income &lt;- round(mean(sample_means), 2)\nsample_means_40 &lt;- data.frame(Income = sample40_means)\nsample_means_40$Observations &lt;- \"Samples of 40 Observations\"\nsample_means_150 &lt;- data.frame(Income = sample_means)\nsample_means_150$Observations &lt;- \"Samples of 150 Observations\"\nsample_means &lt;- rbind(sample_means_40, sample_means_150)\n\n\ndata_income_average &lt;- sample_means %&gt;%\n  summarise(meanIncome = mean(Income), .by = Observations)\n\n\np &lt;- ggplot(sample_means, aes(x = Income, fill = Observations)) +\n  geom_histogram(aes(y = after_stat(density*width))) +\n  geom_vline(\n    data = data_income_average, aes(xintercept = meanIncome),\n    color = \"#B44655\", linetype = \"dashed\", size = 0.75) +\n  geom_text(data = data_income_average, aes(x = average_income * 1.08, y = 0.23, label = paste0(\"Mean: \", round(meanIncome, 2)))) +\n  labs(\n    title = \"Annual Income Distribution\",\n    x = \"Annual Income (thousands, $)\",\n    y = \"Frequency\"\n  ) +\n  theme_fivethirtyeight() +\n  scale_fill_economist() +\n  scale_y_continuous(labels = scales::percent_format())  +\n  theme(plot.title = element_text(hjust = 0.95, size = 11),\n    axis.title.x = element_text(hjust = 0.5),\n    axis.title.y = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0), legend.position=\"none\") +\n  transition_states(Observations, transition_length = 1.25, state_length = 2) +\n  enter_fade() +\n  exit_fade() \n\np &lt;- p + labs(title = \"{closest_state}\")\n\n# Render the animation\nanim &lt;- animate(p, nframes = 180, duration = 10)\nanim"
  },
  {
    "objectID": "posts/2024/intro-to-flask/index.html",
    "href": "posts/2024/intro-to-flask/index.html",
    "title": "Data-driven web applications basics: Getting started with Flask",
    "section": "",
    "text": "In this series of posts, we will explore some of the fundamental concepts of building data-driven web applications. To do so, we will create a simple web application that lets users share their favorite websites and view the most popular ones based on user input.\nThis application will consist of only two pages: (1) a home page where users can register their favorite websites, and (2) a page of popular sites where users can see a table displaying the top favorite websites. The envisioned design for these pages is illustrated in Figure 1.\nTo build this web application, we will follow a structured approach, breaking down the process across four comprehensive posts:\nBy following this series, you will learn how to:\nEach part will provide detailed, step-by-step instructions, making it easy to follow along and build a simple, but functional and efficient web application."
  },
  {
    "objectID": "posts/2024/intro-to-flask/index.html#sec-building-the-home-page-with-flask",
    "href": "posts/2024/intro-to-flask/index.html#sec-building-the-home-page-with-flask",
    "title": "Data-driven web applications basics: Getting started with Flask",
    "section": "Building the Home page with Flask",
    "text": "Building the Home page with Flask\nIn this first post, we will lay the foundation for our web application by starting the user interface. Here, we will focus solely on developing the Home page. Users will be able to input and submit their favorite websites through this page. Our backend will handle these submissions and provide a confirmation response to the user. Figure 2 provides a visual representation of the Home page design, showcasing the intended user experience when submitting a website—confirming successful registration of the website inputted by the user.\n\n\n\n\n\n\nFigure 2: Building the Home page: Favorite website registration and confirmation flow\n\n\n\nTo achieve this, we will utilize Flask—a micro-framework for web development in Python. Flask provides a versatile and lightweight way to build web applications, making it easy to get started with web development projects.\nFlask will handle the backend behavior of our application, including routing, handling requests, and managing the server-side logic. For the frontend, we will use HTML and CSS to create and style the visual components of our application.\nAt this stage, we will not permanently save the website data inputted by the user; the backend will only acknowledge receipt of the submissions. In the subsequent post, we will expand our application to include the “Popular Websites” page and begin implementing data persistence.\nNow that we’ve precisely defined the behavior of our home page, it’s time to get our hands dirty and start coding.\n\nLoading initial libraries and initializing a Flask instance\nTo begin building our web application, we first import the essential components from the flask library. These include the Flask object, which is the core of our web app, the requests module for handling HTTP requests, and the render_template function, which allows us to render HTML files (or Flask templates).\n\nfrom flask import Flask, request, render_template\n\nAfter importing the requirements, we will create an instance of the Flask application:\n\napp = Flask(__name__)\n\nNow, let’s break down what’s happening here:\n\nFlask: This object from the Flask framework helps us create an instance of a Flask web application.\n__name__: A special variable in Python representing the name of the current module. It’s passed as an argument to locate resources like HTML templates and static files.\n\nTherefore, what we are doing is creating an instance of the Flask application, passing __name__ as an argument so that Flask can locate necessary resources, such as HTML templates and static files (CSS, JavaScript, images). Finally, we assign this instance to an object called app, which we will later use to define the behavior of our web application.\n\n\nCreating the Home page\nNow that we have initialized our application, let’s turn our attention to the front-end, i.e., the part users will see and interact with.\nWe want a simple page with a text field where users can enter a URL and a button to register it as shown in Figure 3.\n\n\n\n\n\n\nFigure 3: Envisioned design for the Home page\n\n\n\nTo begin creating this webpage, we will first establish its structure using HTML.\n\nHTML for the Home page\nAs seen in Figure 3, our website has two key elements: (1) a title and (2) a user input form. The form allows users to enter a website URL via a text input field and submit it using a submit button.\nTo set the page title, we will use a top-level heading element, i.e., &lt;h1&gt;, containing the text “What is your favorite website?”. Therefore, the HTML code for the title will be as follows: &lt;h1&gt;What is your favorite website?&lt;/h1&gt;\nMoving on to the form section, we’ll create a form with the &lt;form&gt; element setting its method attribute to POST, i.e., &lt;form method=\"post\"&gt;. This ensures that the data submitted through the form is included within the body of the HTTP request, as opposed to being appended to the URL (like with the GET method). The form itself will consist of two key elements:\n\nText input field for entering a URL: To create a text input field for entering a URL, we’ll use an &lt;input&gt; element. This element lets users input information into a form. Inputs can have different types, like checkboxes or text fields. Since we want a text input for URLs, we’ll specify the type attribute as \"text\". We’ll also add a placeholder using the placeholder attribute with the text \"Enter URL\" to give users a hint about what to input. The placeholder disappears once users start typing.\nTo make sure users fill out this field before submitting the form, we’ll mark it as required using the required attribute. Additionally, we’ll assign a class to this input field called \"form-text-input\". This allows us to style this element and any others with the same class. Lastly, we’ll give the input field a name attribute, set to 'urlInput', so we can easily identify the text entered and retrieve it from the backend after form submission.\nTherefore, we would have the following HTML code for the text input:\n&lt;input type=\"text\" placeholder=\"Enter URL\" class=\"form-text-input\" name=\"urlInput\" required&gt;\nButton to submit the entered URL: Finally, to complete the form, we’ll add a submit button with the text “Register URL”. When users click this button, the form will be submitted, and the URL they entered will be registered. We’ll achieve this using a &lt;button&gt; element. Like input elements, buttons can come in different types, such as reset buttons, which reset the form-data to its initial values, or submit buttons, which submit form-data. Since we require a submit button, we’ll specify the type attribute as \"submit\". Furthermore, for consistent styling among related elements, we’ll assign to this element a class named \"submit-btn\".\nThus, the HTML code for the submit button would be:\n&lt;button type=\"submit\" class=\"submit-btn\"&gt;Register URL&lt;/button&gt;\n\nTo enhance the structure of our page, we will also incorporate some additional elements like &lt;div&gt;. Using &lt;div&gt; elements helps organize and group content, making it easier to style and manage different sections of the webpage. Here’s the HTML code reflecting these additions:\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        &lt;div class=\"content\"&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;What is your favorite website?&lt;/h1&gt;\n                &lt;form method=\"post\"&gt;\n                    &lt;input type=\"text\" placeholder=\"Enter URL\" class=\"form-text-input\" name=\"urlInput\" required&gt;\n                    &lt;br&gt;\n                    &lt;button type=\"submit\" class=\"submit-btn\"&gt;Register URL&lt;/button&gt;\n                &lt;/form&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\nNext, let’s save this HTML code into a file named index.html. Afterward, you can simply open it using a web browser to view the rendered HTML, seeing what is shown in Figure 4.\n\n\n\n\n\n\nFigure 4: Rendered HTML code\n\n\n\nAs we can see, the current appearance of our interface leaves much to be desired. It’s clear that our interface needs a makeover. But don’t worry! We’ve strategically assigned classes to elements within our HTML markup. This serves as the foundation for applying CSS styles, which will significantly enhance the visual appeal and user experience of our application. So, let’s start creating some CSS to improve the style of our application!\n\n\nCSS for the Home page\nTo achieve the theme we envisioned for the homepage design, as described in Figure 3, we will utilize CSS. This theme predominantly employs gray for most elements and accentuates prominent features with green.\nFirst, we will set the body background color to gray. This choice provides a neutral and clean canvas that is easy on the eyes and allows the more prominent elements to stand out. The green color will be applied specifically to the submit button, making it a focal point due to its contrasting hue. This not only enhances the button’s visibility but also emphasizes its importance and functionality within the form.\nTo enhance the minimalist aesthetic and ensure the form attracts the user’s attention, we will center the form on the screen. Centering the form not only creates a balanced and harmonious layout but also directs the user’s focus to this essential part of the page.\nAdditionally, we will add other tweaks such as margins and padding to make the layout more appealing and visually pleasing. Below you can see the complete CSS code:\nbody {\n    font-family: Arial, sans-serif; /* Sets the text font for the entire document body */\n    margin: 0; /* Removes the default margin */\n    padding: 0; /* Removes the default padding */\n    height: 100vh; /* Sets the body height to 100% of the viewport */\n}\n\n.content {\n    display: flex; /* Uses the flexible box model for content */\n    justify-content: center; /* Horizontally centers the content */\n    align-items: center; /* Vertically centers the content */\n    height: 100%; /* Sets the content height to 100% of the parent element */\n    background-color: #f0f0f0; /* Sets the background color of the content */\n    flex-direction: column; /* Arranges child elements in columns */\n}\n\n.form-text-input {\n    width: 300px; /* Sets the width of the URL input field */\n    padding: 10px; /* Adds padding around the input field */\n    font-size: 16px; /* Sets the font size of the input field */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the input field */\n    margin-bottom: 20px; /* Adds bottom margin to separate the input field from the button */\n}\n\n.submit-btn {\n    background-color: #4CAF50; /* Sets the background color of the submit button */\n    color: white; /* Sets the text color of the submit button */\n    padding: 10px 20px; /* Adds padding around the button text */\n    font-size: 16px; /* Sets the font size of the button text */\n    border: none; /* Removes the border from the submit button */\n    border-radius: 5px; /* Applies rounded corners to the submit button */\n    cursor: pointer; /* Changes the cursor to a pointer when hovering over the button */\n    transition: background-color 0.3s; /* Adds a smooth transition to the button's background color */\n}\n\n.submit-btn:hover {\n    background-color: #3e8e41; /* Changes the button's background color when hovering over it */\n}\n\n.container {\n    text-align: center; /* Centers the text inside the container */\n    margin-bottom: 20px; /* Adds bottom margin to separate the form from the result */\n}\nAs you can see, we have predominantly used class selectors to apply specific styles to various elements. For instance, the url-input class styles the URL input field, while the submit-btn class styles the form’s submit button. This approach allows for consistent styling across different pages, as these classes can be reused for similar elements, ensuring a uniform look and feel throughout the application.\nNow we can link this CSS file to our HTML file by adding the following code right after the opening &lt;html&gt; tag:\n&lt;head&gt;\n&lt;link rel=\"stylesheet\" href=\"home.css\"&gt;\n&lt;/head&gt;\nWith this, our initial page no longer looks like in Figure 4, but as shown in Figure 5.\n\n\n\n\n\n\nFigure 5: Rendered HTML code with added CSS styling\n\n\n\nGreat, our application is starting to look nice!\n\n\n\nStructuring our directory\nWhen setting up a Flask application, it’s advisable to organize your files in a structured manner. Typically, you’ll start with a main folder that encompasses all the files related to your application. Within this main folder, you’ll find the Python file containing the Flask code. By convention, this file is often named app.py. Alongside app.py, you’ll also have two additional folders: templates and static.\nThe templates folder is where you’ll store your HTML files. These files contain the structure and layout of your web pages. Meanwhile, the static folder is reserved for files like CSS, JavaScript, images, and other resources that remain static and are not altered by your application’s code.\nTherefore, this would be the file structure for our application:\n- my_flask_app/            (Main folder of the application)\n    |\n    |- app.py              (Main file for the Flask application)\n    |\n    |- templates/          (Folder to store HTML templates)\n    |    |\n    |    |- index.html     (HTML template for the main page)\n    |\n    |- static/             (Folder to store static files such as CSS, JavaScript, images, etc.)\n         |\n         |- styles         (Folder to store style sheets, such as CSS files)\n            |\n            |- home.css    (CSS file to style the main page)\nNote: Following the modification of the file structure, it’s important to update the link to the CSS file in our HTML file.\nAfter having set up the HTML file and structured our Flask application, the next step is to integrate the HTML file with our backend. This means that our Python application will utilize the HTML file as the main page.\nTo accomplish this, we’ll create a function that automatically executes when a user accesses the root path of the application, denoted as “APPLICATION URL/”. We achieve this by utilizing the route decorator with the argument “/” and defining a function to be executed. In this scenario, our function will render the HTML file, index.html, using the render_template function.\nAdditionally, we’ll utilize the methods parameter within the route decorator to enable the home function to accommodate both GET and POST requests. GET requests are typically utilized to fetch data from the server, such as when a user initially accesses a webpage. Conversely, POST requests are commonly employed when users submit data to the server, as is the case when they provide their favorite website URL in our case.\nThus, the combined process can be translated into code as follows:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    return render_template('index.html')\n\nNote: We define the name of the function as home, but we could choose any other name.\n\n\nAdding functionalities\nRight now, our application doesn’t do much, it merely renders the HTML file we’ve created. However, it lacks any interactive functionality. If we add text to the input field and click the “Register URL” button, nothing will occur. What we need is for our backend—our Python code—to capture the URL entered in the text field upon button click.\nWhen a button within a form is clicked, it initiates a “request,” specifically in our case a POST request, sending the form’s values to our backend. Utilizing the previously imported request object, we can access these values and manipulate them within our backend.\nThe request object provides various attributes, including form, which facilitates the extraction of parameters submitted by a form. By using this attribute jointly with the get method, we can extract the desired data from the request. In this instance, we aim to retrieve the content entered by the user into the text field identified by the name urlInput.\nTherefore, we could write the following code to extract the value sent from that field and store it in an a variable called url:\n\nurl = request.form.get('urlInput')\n\n\nUsing templates\nKeep in mind that up to the point we’ve utilized the render_template function to display a static HTML file, specifically for rendering our index.html page. However, this function is named render_template rather than render_html for a reason.\nTemplates in Flask serve as dynamic canvases, enabling the incorporation of placeholders that the application dynamically populates. These placeholders, known as template variables, are encapsulated within double opening and closing brackets, like so: { template_variable }. This lets us include variables right into the HTML, and our backend can then change them on the fly.\nAdditionally, in Flask templates, we can utilize control structures to manage the flow of content. These include conditionals like if-else statements, for-loops, as well as advanced features such as macros and blocks. These control structures are enclosed within {% %} brackets, denoted as {% control_structure %}, and are concluded with {% end_control_structure %}. For example, these control structures allow us to show or hide different parts of the template based on certain conditions.\n\n\n\n\n\n\nJinja 2\n\n\n\nThe template engine used by Flask is Jinja2. You can find more information about template creation on its website: https://jinja.palletsprojects.com/\n\n\nOne of the most basic applications of templates is to dynamically manage the locations of static files, such as CSS files. Hardcoding these paths can create issues if the file locations change over time. Instead, it’s preferable to use Flask’s URL generation function, url_for. This function automatically generates URLs for specified routes or resources, which is particularly useful when project URLs undergo changes during deployment.\nFor example, during development, URLs may follow a certain structure. However, upon deployment, it might be necessary to prefix all URLs with a specific identifier. With Flask’s url_for function, this transition is handled without manual intervention. The function automatically adjusts the generated URLs to include the designated prefix without the need for manual code alterations.\nConsequently, we can enhance our HTML file to incorporate the link to the CSS file using url_for:\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/home.css')}}\"&gt;\nHowever, templates offer more than just dynamically setting the CSS path; they enable us to add engaging features. For example, we can leverage templates to define the behavior of the confirmation message indicating that the entered URL has been successfully registered.\nTo implement this, let’s first update our HTML file by introducing a new &lt;div&gt; element to contain the confirmation message. This message should only appear if the user has entered a URL, meaning the url variable holds a value. If the variable exists, we’ll display its value. Here’s how we can accomplish this:\n&lt;div id=\"confirmationContainer\"&gt;\n  {% if url %}\n    &lt;p&gt;You have registered the following URL: {{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n  {% endif %}\n&lt;/div&gt;\nIn this code snippet, we’re utilizing an if control structure to conditionally display content based on whether the url variable holds a value. The content we want to display is enclosed within {% if url %} and {% endif %} tags. Inside the {% if url %} block, we include a message confirming the URL registration. To dynamically insert the value of the url variable into the message, we use double curly braces { url } within a paragraph (&lt;p&gt;) element.\n\nPassing variables to templates\nTo make sure our template can access the url value, we need to transfer it from the backend to the frontend. In other words, we must make this variable available within the template. We can accomplish this by adjusting our app.py file, specifically by modifying the render_template function.\nThe render_template function facilitates the passing of variables to the template we are rendering. To pass the url value to the index.html page and make it accessible with the same name (url), we employ the following code: render_template('index.html', url=url). This ensures that the url variable is transmitted to the index.html page, where it can be utilized under the same name.\nHowever, we only intend to transfer the url value to the frontend if the user submits a URL via a POST request. In such instances, we render our HTML file while transmitting the provided URL value to it. Conversely, if the user does not submit a URL, we render the HTML file without transmitting any variables to it. To implement this logic, we need to adjust the home function in app.py, as we show below:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        return render_template('index.html', url=url)  # Pass the value of 'urlInput' to the template\n    else:\n        return render_template('index.html')\n\n\n\n\n\nReviewing what we have\nSo, now we have three fairly complete files. Let’s directly list the code we have so far.\n\nBackend\n\nFlask application: app.py\n\nfrom flask import Flask, request, render_template\n\napp = Flask(__name__)\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        return render_template('index.html', url=url)  # Pass the value of 'urlInput' to the template\n    else:\n        return render_template('index.html')\n\n\n\n\nFront-end\n\nHTML file: index.html\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/home.css')}}\"&gt;\n&lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"content\"&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;What is your favorite website?&lt;/h1&gt;\n                &lt;form method=\"post\"&gt;  &lt;!-- Specify method=\"post\" for form submission --&gt;\n                    &lt;input type=\"text\" name=\"urlInput\" placeholder=\"Enter URL\" class=\"form-text-input\" required&gt;\n                    &lt;br&gt;\n                    &lt;button type=\"submit\" class=\"submit-btn\"&gt;Register URL&lt;/button&gt;\n                &lt;/form&gt;\n                &lt;div id=\"confirmationContainer\"&gt;\n                  {% if url %}\n                  &lt;p&gt;You have registered the following URL: {{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n                  {% endif %}\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\nCSS file: home.css\nbody {\n    font-family: Arial, sans-serif; /* Sets the text font for the entire document body */\n    margin: 0; /* Removes the default margin */\n    padding: 0; /* Removes the default padding */\n    height: 100vh; /* Sets the body height to 100% of the viewport */\n}\n\n.content {\n    display: flex; /* Uses the flexible box model for content */\n    justify-content: center; /* Horizontally centers the content */\n    align-items: center; /* Vertically centers the content */\n    height: 100%; /* Sets the content height to 100% of the viewport */\n    background-color: #f0f0f0; /* Sets the background color of the content */\n    flex-direction: column; /* Arranges child elements in columns */\n}\n\n.form-text-input {\n    width: 300px; /* Sets the width of the URL input field */\n    padding: 10px; /* Adds padding around the input field */\n    font-size: 16px; /* Sets the font size of the input field */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the input field */\n    margin-bottom: 20px; /* Adds bottom margin to separate the input field from the button */\n}\n\n.submit-btn {\n    background-color: #4CAF50; /* Sets the background color of the submit button */\n    color: white; /* Sets the text color of the submit button */\n    padding: 10px 20px; /* Adds padding around the button text */\n    font-size: 16px; /* Sets the font size of the button text */\n    border: none; /* Removes the border from the submit button */\n    border-radius: 5px; /* Applies rounded corners to the submit button */\n    cursor: pointer; /* Changes the cursor to a pointer when hovering over the button */\n    transition: background-color 0.3s; /* Adds a smooth transition to the button's background color */\n}\n\n.submit-btn:hover {\n    background-color: #3e8e41; /* Changes the button's background color when hovering over it */\n}\n\n.container {\n    text-align: center; /* Centers the text inside the container */\n    margin-bottom: 20px; /* Adds bottom margin to separate the form from the result */\n}\n\n\n\n\nRunning our application\nWith the initial parts of the code in place, we’re ready to run our application. To do so, we can open a terminal, navigate to the folder where our app.py file is located, and then execute the command flask run. After running this command, we will see an output with a local IP address (by default, 127.0.0.1:5000), as you can see in the Figure 6. This is the IP address where our application is running.\n\n\n\n\n\n\nFigure 6: Executing the flask run command\n\n\n\nNow we can navigate to this address in our web browser to access our application. Once there, we can input a URL (for instance: www.google.com) into the text field, click the submit button, and we’ll then observe a confirmation message generated by the previously established template, as you can see in Figure 7.\n\n\n\n\n\n\nFigure 7: Registering www.google.com in our application\n\n\n\nThis message confirms that our action of registering a URL has been successful. Essentially, it signifies that the user’s POST request has been correctly captured by the backend, transmitted to the front-end, and then integrated into the template.\n\n\nA first issue: When refreshing the page, our request is sent again\nIf we refresh the page, for instance, by pressing F5, we’ll notice that the request we initially entered, such as www.google.com, is automatically resent, as you can see in Figure 8.\n\n\n\n\n\n\nFigure 8: The request is automatically resent upon page refresh\n\n\n\nThis behavior is typical when a page has been loaded through a POST request, often encountered when working with forms.\nTo tackle this issue, there are various strategies we could employ. One of the most prevalent is the Post/Redirect/Get (PRG) strategy (you can find more information here). Essentially, after making a POST request, we redirect to a new page, which then retrieves (GETs) the confirmation of our current state.\nImplementing this strategy in our application is relatively straightforward. We need to make two key adjustments:\n\nInstead of rendering our template with the URL submitted by the user upon detecting a POST request, we redirect to a new page.\nTo take this redirection into effect, we incorporate additional code using the @app.route decorator, specifying the behavior of the route to which we redirect.\n\nTo do this, we need to import two additional functions from the Flask library: redirect and url_for. What these functions do is quite obvious from their names. The redirect function redirects us to a URL we specify, while the url_for function allows us to dynamically create URLs within our application. If you recall, we previously utilized the url_for function to dynamically link the index.html file to our stylesheet.\nBy using these functions, we can adjust our code so that when we get a POST request from a user registering a URL, instead of simply refreshing our template, we redirect them to a new URL, like app_url/display_url/url, where url refers to the URL provided by the user. This redirection is made possible with the line of code: redirect(url_for('display_url', url=url)). Here, url_for generates a URL pointing to the display_url view within the Flask application and passes the value of the url variable as a route parameter, yielding the desired URL. Thus, the Python code for our application appears as follows:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        return redirect(url_for('display_url', url=url))\n    else:\n        return render_template('index.html')\n\n\nDynamic URL handling with Flask converters\nThe code we’ve just implemented redirects users when they input a URL to app_url/display_url/url, where url refers to the URL inputted by the user. However, this specific URL hasn’t been defined within our application yet. We need to clarify what actions should occur when this URL is accessed. However, looking at the structure of the URL, we can see it has two main parts: one that remains constant (app_url/url) and another that varies (url) based on user input. Does this mean we need to define a route for every possible URL a user might input? Thankfully, no. We don’t need to define a route for every potential URL variation. Instead, we can use Flask converters to define a dynamic route that captures any URL input by the user. This allows us to handle any URL entered by the user without needing to predefine each one individually.\nConverters are defined using angle brackets &lt; &gt;, which serve as placeholders for dynamic content. We can define a dynamic route for this page using converters, such as /display_url/&lt;url&gt;, where url is the name of our converter that takes whatever value is placed in its position. Converters also allow us to capture these variable parts of the URL into Python variables. For example, url in the route /display_url/&lt;url&gt; captures whatever value is provided in place of &lt;url&gt; and allows us to access its value within Python by referring to the url variable.\nMoreover, we can specify the type we want to allow for these converter values by specifying the type followed by a colon (:) and then the name we want to assign to this converter. By default, they are set to strings (without slashes). However, since we want to accommodate URLs, which may contain slashes, we should specify the type as path, like &lt;path:url&gt;.\nTherefore, we can now proceed to define the route in our Flask application to which users will be redirected after inputting a URL, i.e., /display_url/&lt;path:url&gt;:\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET'])\n\nNow we need to specify the behavior for this route. In other words, we need to write the function that will execute each time the user is redirected to this URL. Specifically, we want to render our home page, index.html, showing a confirmation message that the inputted URL has been registered correctly.\nAs previously mentioned, converters such as &lt;path:url&gt; allow us to capture dynamic parts of the URL and use them as Python variables. This means we can access the value of the URL parameter directly within the view function defined by the @app.route decorator.\nTherefore, in this page we can do the following. First, we can check if the url variable contains a value. If url is not None (meaning a URL segment has been provided), we proceed to render the index.html template by using the render_template function and passing the url variable to the template. In this way, the confirmation message will be displayed to the user.\nHowever, if the url variable is None, indicating that no URL segment was provided, we redirect the user to the home page of the application. This is done using the redirect function in combination with url_for, generating a redirect response to the URL defined by url_for('home'). This redirection is specified to handle cases where the user manually sets the URL to app_url/display_url/.\nTaking all this into account, the code for this route would look like the following:\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET'])\ndef display_url(url=None):\n    if url:\n        return render_template('index.html', url=url)\n    else:  \n        return redirect(url_for('home'))\n\nNotice that in this case, we are solely transmitting information. However, envision a scenario where the user might continue registering new URLs, indicating a flow where we receive input from the user. This aspect is not presently accounted for in our code.\nTo address this gap in functionality, we need to expand our code to capture the URLs that users intend to register when they visit a page with the route /display_url/&lt;path:url&gt;. We can accomplish this by adapting the code we’ve already used for the main route, /. Specifically, we’ll detect if the user is submitting a POST request. If so, we’ll retrieve the submitted URL and redirect to the confirmation page. Below, you’ll find the expanded code for the route /display_url/&lt;path:url&gt;:\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url=None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            return redirect(url_for('display_url', url=url2))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\nTherefore, our updated Python code will look as follows:\n\nfrom flask import Flask, request, render_template, redirect, url_for\n\napp = Flask(__name__)\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        return redirect(url_for('display_url', url=url))\n    else:\n        return render_template('index.html')\n\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url=None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            return redirect(url_for('display_url', url=url2))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\nGreat! now if we register a URL and refresh the page, the POST request won’t be made again.\n\n\n\nAdding style to our confirmation container\nFinally, let’s enhance the visual presentation of the confirmation message. Currently, it’s displayed as plain text directly below the “Register URL” without any clear distinction, as depicted in Figure 7. If you recall from section Section 1.4.1, we encapsulated the confirmation message within a &lt;div&gt; element that we identified with the id confirmationContainer. Hence, we can utilize this unique identifier (id) to delineate a distinctive style for this element within our CSS. For instance, we could envelop the confirmation message within a rounded-corner box (specified by border-radius, for example, 5px) embellished with shadowed borders (defined by box-shadow, such as 0 2px 4px rgba(0, 0, 0, 0.1)), featuring a slightly variant shade of gray compared to other application components (e.g., setting the background-color to #f0eaea), and adding some top margin to visually separate it from the form. Below, you can find the style defined for this ID:\n#confirmationContainer {\n    padding: 20px; /* Adds padding around the content of the result container */\n    background-color: #f0eaea; /* Sets the background color of the result container */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the result container */\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Adds a shadow effect to the result container */\n    margin-top: 20px; /* Adds top margin to visually separate the confirmation container from the form */\n\n}\nWith this addition to the CSS file, our confirmation text will now be displayed within a clearly defined container. For instance, upon registering www.google.com, the confirmation message verifying the successful registration of this URL will be neatly enclosed in a well-defined box, as depicted in Figure 9.\n\n\n\n\n\n\nFigure 9: Confirmation message container after updating CSS file\n\n\n\nHowever, implementing this change will introduce a minor issue. Upon returning to the main page—by removing the /display_url/www.google.com portion from the URL and pressing ENTER—you’ll notice that the confirmation container still remains despite no confirmation being displayed, as illustrated in Figure 10.\n\n\n\n\n\n\nFigure 10: Unintended confirmation container visibility\n\n\n\nThis issue can be addressed simply by adjusting our HTML template. Specifically the part of the template which shows the confirmation message. Currently, that part appears as follows:\n&lt;div id=\"confirmationContainer\"&gt;\n  {% if url %}\n      &lt;p&gt;You have registered the following URL: {{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n  {% endif %}\n&lt;/div&gt;\nIn our current template, the text “You have registered the following URL: [entered URL]” is displayed only when a user submits a URL. However, the div element with the id confirmationContainer enclosing this text is shown in all cases, regardless of whether a URL is provided or not.\nWhat we aim for is to display both the text and its enclosing box only when a URL is entered. If there’s no URL, neither the text nor the box should appear. Nevertheless, right now, the confirmation box is always displayed as depicted in Figure 10.\nTo rectify this issue, we only need to relocate the control structure {% if url %} {% endif %} to not just wrap around the paragraph containing the text “You have registered the following URL: [entered URL]”, but also around the enclosing div. Hence, the part of template responsible for displaying the confirmation message would now appear as follows:\n{% if url %}\n  &lt;div id=\"confirmationContainer\"&gt;\n        &lt;p&gt;You have registered the following URL: {{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n  &lt;/div&gt;\n{% endif %}\nBy implementing this adjustment, if we navigate to the main page without registering any URL, the container won’t be visible."
  },
  {
    "objectID": "posts/2024/intro-to-flask/index.html#final-code",
    "href": "posts/2024/intro-to-flask/index.html#final-code",
    "title": "Data-driven web applications basics: Getting started with Flask",
    "section": "Final code",
    "text": "Final code\nYou can unfold the section below to see the code implementation incorporating all the necessary adjustments.\n\n\nFile structure and files content\n\n\nFinal file structure\n- my_flask_app/            (Main folder of the application)\n    |\n    |- app.py              (Main file for the Flask application)\n    |\n    |- templates/          (Folder to store HTML templates)\n    |    |\n    |    |- index.html     (HTML template for the main page)\n    |\n    |- static/             (Folder to store static files such as CSS, JavaScript, images, etc.)\n         |\n         |- styles         (Folder to store style sheets, such as CSS files)\n            |\n            |- home.css    (CSS file to style the main page)\n\n\nPython - app.py\n\nfrom flask import Flask, request, render_template, redirect, url_for\n\napp = Flask(__name__)\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        return redirect(url_for('display_url', url=url))\n    else:\n        return render_template('index.html')\n\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url=None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            return redirect(url_for('display_url', url=url2))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\n\n\n\n\n\nA small change\n\n\n\nNotice that in this block of code, we have added two additional lines that we hadn’t mentioned before:\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\nThis modification indicates that when the Python script is executed directly, for instance using the command python app.py, the application will initiate in debugging mode. Importantly, these lines will remain inactive when the application is run through other methods, such as flask run, ensuring they are executed only under specific conditions.\n\n\n\n\nHTML - index.html\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/home.css')}}\"&gt;\n&lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"content\"&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;h1&gt;What is your favorite website?&lt;/h1&gt;\n                &lt;form method=\"post\"&gt;  &lt;!-- Specify method=\"post\" for form submission --&gt;\n                    &lt;input type=\"text\" name=\"urlInput\" placeholder=\"Enter URL\" class=\"form-text-input\" required&gt;\n                    &lt;br&gt;\n                    &lt;button type=\"submit\" class=\"submit-btn\"&gt;Register URL&lt;/button&gt;\n                &lt;/form&gt;\n                \n                {% if url %}\n                  &lt;div id=\"confirmationContainer\"&gt;\n                      &lt;p&gt;You have registered the following URL: {{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n                  &lt;/div&gt;\n                {% endif %}\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\nCSS - home.css\nbody {\n    font-family: Arial, sans-serif; /* Sets the text font for the entire document body */\n    margin: 0; /* Removes the default margin */\n    padding: 0; /* Removes the default padding */\n    height: 100vh; /* Sets the body height to 100% of the viewport */\n}\n\n.content {\n    display: flex; /* Uses the flexible box model for content */\n    justify-content: center; /* Horizontally centers the content */\n    align-items: center; /* Vertically centers the content */\n    height: 100%; /* Sets the content height to 100% of the viewport */\n    background-color: #f0f0f0; /* Sets the background color of the content */\n    flex-direction: column; /* Arranges child elements in columns */\n}\n\n.form-text-input {\n    width: 300px; /* Sets the width of the URL input field */\n    padding: 10px; /* Adds padding around the input field */\n    font-size: 16px; /* Sets the font size of the input field */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the input field */\n    margin-bottom: 20px; /* Adds bottom margin to separate the input field from the button */\n}\n\n.submit-btn {\n    background-color: #4CAF50; /* Sets the background color of the submit button */\n    color: white; /* Sets the text color of the submit button */\n    padding: 10px 20px; /* Adds padding around the button text */\n    font-size: 16px; /* Sets the font size of the button text */\n    border: none; /* Removes the border from the submit button */\n    border-radius: 5px; /* Applies rounded corners to the submit button */\n    cursor: pointer; /* Changes the cursor to a pointer when hovering over the button */\n    transition: background-color 0.3s; /* Adds a smooth transition to the button's background color */\n}\n\n.submit-btn:hover {\n    background-color: #3e8e41; /* Changes the button's background color when hovering over it */\n}\n\n.container {\n    text-align: center; /* Centers the text inside the container */\n    margin-bottom: 20px; /* Adds bottom margin to separate the form from the result */\n}\n\n#confirmationContainer {\n    padding: 20px; /* Adds padding around the content of the result container */\n    background-color: #f0eaea; /* Sets the background color of the result container */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the result container */\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Adds a shadow effect to the result container */\n    margin-top: 20px; /* Adds top margin to visually separate the confirmation container from the form */\n\n}"
  },
  {
    "objectID": "posts/2024/intro-to-flask/index.html#summary",
    "href": "posts/2024/intro-to-flask/index.html#summary",
    "title": "Data-driven web applications basics: Getting started with Flask",
    "section": "Summary",
    "text": "Summary\nIn the first post of the data-driven web applications basics, we built the foundation of our web application, by starting its Home page using Flask. We implemented a simple form allowing users to submit their favorite websites, providing them with confirmation upon successful submission.\nTo achieve this functionality, we leveraged Flask’s routing capabilities to define how the application responds to user interactions. We captured the submitted URL and used Flask’s templating system to display a confirmation message. Additionally, we implemented the Post/Redirect/Get (PRG) pattern to prevent accidental resubmissions when users refresh the page. This involved using Flask’s redirect and url_for functions, along with Flask converters.\nWhile currently, submitted URLs are not stored, the next post will delve into connecting our application to a database. This will enable us to save user-submitted URLs, allowing us to create a new page that dynamically displays the top favorite websites among users."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html",
    "href": "posts/2023/intro-to-voice-analytics/index.html",
    "title": "Introductory voice analytics with R",
    "section": "",
    "text": "Interpersonal communication transcends mere words, incorporating nuanced nonverbal signals where the voice plays a pivotal role. We dynamically adjust our voice to convey emotions, such as happiness or sadness, and intentions, including subtle nuances like sarcasm. We even form impressions from the way someone speaks. Therefore, analyzing not just the content but also the delivery—the voice—is essential for a more comprehensive understanding of communication."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#voice-analytics-decoding-the-vocal-spectrum",
    "href": "posts/2023/intro-to-voice-analytics/index.html#voice-analytics-decoding-the-vocal-spectrum",
    "title": "Introductory voice analytics with R",
    "section": "Voice analytics: Decoding the vocal spectrum",
    "text": "Voice analytics: Decoding the vocal spectrum\nVoice analytics precisely aims to achieve this by examining the voice beyond its linguistic content. Various methods exist for conducting voice analytics, with one of the most common involving the extraction of different characteristics from the voice, known as vocal features. Two key vocal features are amplitude and fundamental frequency. Amplitude reflects the loudness or intensity of a sound, offering insights into the volume and energy of speech. Fundamental frequency, on the other hand, is associated with pitch, determining how high or low a voice sounds to the listener."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#the-analytical-process-from-acquisition-to-statistical-analysis",
    "href": "posts/2023/intro-to-voice-analytics/index.html#the-analytical-process-from-acquisition-to-statistical-analysis",
    "title": "Introductory voice analytics with R",
    "section": "The analytical process: From acquisition to statistical analysis",
    "text": "The analytical process: From acquisition to statistical analysis\nHowever, before delving into the extraction of vocal features, a series of pivotal steps forms an integral part of the analytical process. The initial phase entails the acquisition of voice recordings, achievable through direct recording or retrieval from publicly accessible sources. Once the files are obtained, meticulous processing becomes indispensable, involving the arrangement of metadata and validation of collected files to ensure precision and organizational coherence.\nFollowing this preparatory phase, the subsequent step involves reading and preprocessing the voice files. This encompasses data preprocessing and transformation by primarily eliminating extraneous elements, such as irrelevant utterances and background noise. These preprocessing steps are crucial for ensuring the quality of the data.\nAfter preprocessing the audio files, we can extract the vocal features of interest, such as amplitude and fundamental frequency. These features can subsequently be explored through visualization and the computation of summary statistics to gain a deeper understanding. This exploration may reveal further anomalies, or matters requiring additional processing may be detected. Consequently, we may proceed to further preprocess the audio files. Once the data attains sufficient quality, the process culminates in statistical analysis. In this phase, the extracted vocal features may be compared through statistical tests or used to train prediction models. This whole process is depicted in Figure 1.\nIt is imperative to recognize that the process just described here is a simplified abstraction of the voice analytics process. Practical voice analytics is characterized by flexibility and adaptability rather than a rigidly linear progression. This iterative nature accommodates refinements and adjustments, ultimately enhancing the robustness and accuracy of the outcomes.\n\n\n\n\n\n\nFigure 1: Streamlined view of the voice analytics pipeline\n\n\n\nTo illustrate how voice analytics operates in a real-world context, we offer a simple, practical tutorial using R. This tutorial walks you through the fundamental steps of the voice analytics pipeline, from reading audio files to extracting vocal features and drawing basic inferences."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#understanding-user-frustration",
    "href": "posts/2023/intro-to-voice-analytics/index.html#understanding-user-frustration",
    "title": "Introductory voice analytics with R",
    "section": "Understanding user frustration",
    "text": "Understanding user frustration\nIn this tutorial, we will analyze the audio from the video provided below. The video shows a female Scottish user attempting to get Alexa to play the song “Something’s Cooking in the Kitchen” by Dana on Spotify. Despite her efforts, Alexa consistently misunderstands the command. As a result, the user becomes visibly frustrated, adjusting her tone and enunciation while repeatedly rephrasing the command in an increasingly exasperated manner.\nVideo"
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#data-acquisition-and-processing",
    "href": "posts/2023/intro-to-voice-analytics/index.html#data-acquisition-and-processing",
    "title": "Introductory voice analytics with R",
    "section": "Data acquisition and processing",
    "text": "Data acquisition and processing\n\nData acquisition\nFor our comprehensive analysis, we begin by extracting the audio from the previous video and converting it into the Waveform audio file format (WAV). In this scenario, we are interested in two pivotal aspects of this interaction:\n\nSpeech Formation of the wake word “Alexa”\nVocal Changes During the Issuance of a Command (“Alexa, play something is cooking in my kitchen on Spotify by Dana”)\n\nTo facilitate our analysis, we cropped the voice recordings, retaining only the segments containing the two initial commands, including the wake word “Alexa”. In the first command, the speaker calmly requests Alexa to play a song. However, it becomes apparent that Alexa doesn’t comprehend the given command. Consequently, the speaker repeats the same command with a noticeable tone of frustration.\nOur following sections will delve into a detailed examination of this particular case, untangling the distinctions between these two commands that lead us to perceive frustration from the user’s perspective. You can download the files for this example by clicking the following button.\n\n\n\nDownload\n\n\n\nOur analytical approach primarily leverages the seewave package, which has emerged as the gold standard in R-sound analysis. This versatile package encompasses an impressive array of 130 functions designed for the analysis, manipulation, representation, editing, and synthesis of time-based audio waveforms. While seewave serves as our cornerstone, we also make reference to other valuable packages, such as tuneR, soundgen, and phonTools, for their specialized functionalities as needed.\n\n\nReading sound files\nAs previously mentioned, the primary focus of this tutorial centers around the utilization of the seewave package. While it is important to note that seewave lacks native capabilities for sound file reading, we adeptly overcome this limitation by harnessing functions from complementary packages. It is important to emphasize that some packages may use distinct classes for sound objects. Consequently, when choosing an alternative package to load sound data, it becomes paramount to consider this inherent class compatibility.\nIn the context of seewave, its core functionalities are tailored to work with sound objects of the Wave class. These Wave class sound objects are conventionally created using the tuneR package. Hence, when working with seewave, it is strongly recommended to employ tuneR for sound data loading.\nTo load the two user commands including the wake word from the interaction with Amazon Alexa, we use the readWave function from the tuneR package. This function loads or reads a sound file from a specified location, which we need to pass as its main argument. Additionally, we assign the resulting outputs from reading the two commands to two objects called cmd1 and cmd2, as shown below:\n\nlibrary(tuneR)\ncmd1 &lt;- readWave(\"alexa_cmd1.wav\")\ncmd2 &lt;- readWave(\"alexa_cmd2.wav\")\n\nAfter loading these two recordings into R, we can call them to obtain an informative output showing several basic characteristics of these recordings. These characteristics encompass:\n\nNumber of Samples: This indicates the total count of discrete data points in the audio waveform.\nDuration (in seconds): The elapsed time in seconds, capturing the length of the audio.\nSampling Rate (in Hertz): Denoting the rate at which individual samples are taken per second.\nNumber of Channels: It signifies whether the audio is mono (single channel) or stereo (two channels).\nBit Rate: Representing the number of bits processed per unit of time.\n\nBelow we can see the output for the cmd1 and cmd2 objects:\n\ncmd1\n\n\nWave Object\n    Number of Samples:      335789\n    Duration (seconds):     7.61\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\ncmd2\n\n\nWave Object\n    Number of Samples:      368128\n    Duration (seconds):     8.35\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nUpon inspecting this information, it becomes evident that both recordings share identical sampling rates, channel numbers, and bit rates. However, the second recording is 0.74 seconds longer than the first.\nMoreover, the readWave function provides additional optional arguments to enhance control over file reading. Notably, the from and to arguments enable users to selectively read specific segments of the audio file. By default, these arguments operate in sample units, defining the segment based on sample counts. However, the readWave function introduces the units argument, allowing users to customize the units of the from and to arguments to seconds, minutes, or hours.\nTo illustrate, suppose we aim to extract two segments from the first command, denoted as cmd1.s1 and cmd1.s2. The first segment covers the initial 0.5 seconds of the recording, while the second spans from that point to 2 seconds. This can be accomplished by directly using the readWave function and specifying the from, to, and units arguments, as shown below:\n\n(cmd1.s1 &lt;- readWave(\"alexa_cmd1.wav\",from=0,to=0.5,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      22050\n    Duration (seconds):     0.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(cmd1.s2 &lt;- readWave(\"alexa_cmd1.wav\",from=0.5,to=2,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      66150\n    Duration (seconds):     1.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\n\n\n\nNote\n\n\n\nWrapping the code in parentheses triggers automatic printing.\n\n\n\n\nPlaying a sound file\nSomething that we may need at several points of the voice analytics pipeline is to play the recordings/processed recordings, as an additional way to inspect it. Although, R itself cannot play sound files the seewave’s listen function allows us to call the default audio player of the user’s operating system from R to play the selected audio.\nTo do so, we first load the seewave package:\n\nlibrary(seewave)\n\nNow, you can employ the listen function to play audio, for instance, to play the sound recorded in cmd1:\n\nlisten(cmd1)\n\n\n\n\nWe could do the same for the second command:\n\nlisten(cmd2)\n\n\n\n\nBoth commands convey identical content but with a slight variation in order. In the first command, the speaker instructs: “Alexa, play ‘Something Is Cooking in My Kitchen’ on Spotify by Dana”. In contrast, the second command the speaker says: “Alexa, play ‘Something Is Cooking in My Kitchen’ by Dana on my Spotify”.\nSimilar to the readWave function, listen supports the from and to arguments, enabling precise selection of sections for auditory playback. Additionally, it allows us to manipulate the sampling frequency rate through the f argument, altering the speaking rate. You can run the following code to hear the first command (cmd1) with a sampling rate 10% higher and with a sampling rate 10% lower, respectively:\n\nlisten(cmd1, f=cmd1@samp.rate*1.1)\n\n\n\n\n\nlisten(cmd1, f=cmd1@samp.rate/1.1)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor convenience, subsequent sections directly include sound players after each processed or newly generated audio without explicitly calling the listen function. Nevertheless, it is crucial to remember that for playing Wave objects through R, the listen function must be utilized.\n\n\n\n\nPreprocessing sound files\nIn many instances, effective preprocessing of diverse voice files is crucial to optimize their overall quality. This preprocessing involves a variety of tasks, such as (1) extracting specific segments of interest from a sound wave, (2) removing selected utterances from a soundwave, (3) trimming periods of silence at the beginning or end of a sound file, (4) filtering out all unvoiced frames from a sound file, and (5) eliminating background noise.\nThe tuneR and seewave packages provide a comprehensive set of functions designed to address these diverse preprocessing procedures:\n\nextractWave: This function facilitates the extraction of desired segments from a soundwave. Users can specify the segments using the from and to arguments, as discussed earlier. The default units for the extractWave function are samples, but users can adjust this using the xunit argument. Specifically, setting ‘xunit’ to “time” enables the extraction of segments in seconds.\ndeletew: This function removes specific portions from a soundwave. As in the case of extractWave, users can specify segments using the from and to arguments. Notably, for this function, these values are directly specified in seconds. By default, this function returns a matrix, but we can change the output type to a Wave object by specifying the output argument to \"Wave\".\nnoSilence: Particularly useful for removing periods of silence from the beginning and/or end of a sound file. By default, it removes silence periods from both the beginning and end. However, users can modify this behavior using the where argument, specifying \"start\" to remove only the beginning silent frames or \"end\" to remove only the end silent frames.\nzapsilw: This function eliminates all unvoiced frames from a sound file. Users can customize this operation by setting the ‘threshold’ argument, which measures the amplitude threshold (in percent) distinguishing silence from signal. The zapsilw function also, by default, plots oscillograms for both the original sound file and the modified version (after removing the silent voice frames), providing visual insight into the process. Automatic plotting of oscillograms can be deactivated by setting the plot argument to FALSE. Like other functions within the seewave package, this function returns a matrix by default. However, the output type can be changed to a Wave object specifying the output argument as \"Wave\".\nrmnoise: The rmnoise function effectively eliminates background noise from a sound file through smoothing. Like other functions within the seewave package, this function returns a matrix by default. However, the output type can be changed to a Wave object specifying the output argument as \"Wave\".\n\nThese functions allow us to easily manipulate sound files, ensuring they are tailored to meet the specific requirements of the analyses. To illustrate their practical utility, let’s delve into some illustrative examples.\n\nUsing the extractWave function\nFor example, let’s employ the extractWave function to isolate a specific segment from the first command which we assigned to the cmd1 object. Suppose our goal is to extract the initial 0.8 seconds of that file. To achieve this, we must set four arguments. Initially, the primary argument should be the object of the Wave class, representing the sound file from which we intend to extract a segment. Next, we need to specify the from and to arguments, indicating 0 and 0.8, respectively—indicating the segment we wish to extract spans from 0 to 0.8 seconds. It’s essential to note that, by default, these arguments are not expressed in seconds. Consequently, we need to explicitly set the xunit argument to \"time\" to ensure the units are interpreted as seconds. Otherwise, they would be interpreted as sample units. Therefore, we can extract the first 0.8 seconds from the first command stored in cmd1, which corresponds to the wake word “Alexa”, storing the resulting isolated segment in an object called cmd1.xtr, as demonstrated below:\n\n#Extract first 700ms\ncmd1.xtr &lt;- extractWave(cmd1, from = 0, to = 0.8, xunit = \"time\") \n\n\n\n\n\n\nUsing the deletew function\nAlternatively, rather than extracting this segment, we can adopt the opposite strategy: removing this segment. This task is easily accomplished using the deletew function. The arguments required for this operation are quite similar to the previous ones, with the distinction that there’s no need to specify an xunit argument, as the units are already in seconds (and cannot be changed). However, it is essential to specify an output argument to obtain an output of the Wave class. Consequently, we can create a new Wave object that excludes the first 0.8 seconds from the initial command, i.e., excluding the wake word “Alexa”, storing the output into cmd1.rem in the following manner:\n\n#Delete first 800ms\ncmd1.rem &lt;- deletew(cmd1, from=0, to=0.8, output=\"Wave\") \n\n\n\n\n\n\nUsing the noSilence function\nThus far, we have delved into the extraction and deletion of specific audio segments defined by a time frame. However, there are scenarios where our interest lies in removing segments that meet specific conditions, such as unvoiced segments at the outset and conclusion of an audio file. This practice is frequently employed to standardize audio files, as variations in the length of unvoiced frames at the start and end may not necessarily be linked to speaker pauses but could be influenced by other factors. For instance, this variability could be attributed to the individual recording, taking additional time to instruct the speaker to commence or conclude their speech, or to manage the recording process after the speaker has concluded.\nTo accomplish this, we can use the noSilence function. Therefore, if we wish to eliminate the initial and end unvoiced frames of the initial command, stored in cmd1, and store the output in a new object called cmd1.cut, we can achieve this with the following code:\n\n#Remove only unvoiced start and ending\ncmd1.cut &lt;- noSilence(cmd1, level = 350)\n\nIt is important to highlight that we define a argument called level with a value of 350. This argument determines the amplitude level below which samples are considered unvoiced and subsequently removed. BBy default, this value is initialized to zero, which proves overly restrictive in our context. This default setting would result in the detection of no unvoiced areas, given the presence of background noise at the end of the audio, despite these areas being unvoiced.\nTo address this limitation, we choose a much higher value, specifically 350. This value is carefully selected to be sufficiently elevated to avoid removing voiced areas while effectively identifying and removing unvoiced segments. After running the previous code, we proceed to compare the original and processed commands:\n\ncmd1\n\n\nWave Object\n    Number of Samples:      335789\n    Duration (seconds):     7.61\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\n\ncmd1.cut\n\n\nWave Object\n    Number of Samples:      305734\n    Duration (seconds):     6.93\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nUpon comparing both audio files, we can observe that the processed version is slightly shorter, specifically by 0.68 seconds, compared to the original command. Additionally, when listening to both audios, we can discern that the content of the audio has been effectively preserved in the processed version.\n\n\nUsing the zapsilw function\nAlternatively, we could eliminate all the unvoiced frames—not only those at the start and end but across all segments—of the first command (cmd1) using the zapsilw function, as shown below:\n\n#Remove all unvoiced frames of a soundwave\ncmd1.nosil &lt;- zapsilw(cmd1, output=\"Wave\")\n\n\n\n\n\n\n\nFigure 2: Oscillogram comparison: original (top) vs. processed first command recording with unvoiced frames removed using default zapsilw parameters (bottom)\n\n\n\n\n\nBy default, the zapsilw function generates an oscillogram that compares the original sound recording with its processed counterpart, exemplified in Figure 2. By comparing both oscillograms we can see how unvoiced frames have been removed, characterized by minimal or absent amplitude. Furthermore, for a comprehensive analysis, the characteristics of both audio files can be compared by calling cmd1 and cmd1.nosil:\n\ncmd1\n\n\nWave Object\n    Number of Samples:      335789\n    Duration (seconds):     7.61\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\n\ncmd1.nosil\n\n\nWave Object\n    Number of Samples:      101647\n    Duration (seconds):     2.3\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nWhen we look at the two files and compare their features, we can see that removing the unvoiced frames (cmd1.nosil) results in a reduction in both the number of samples and the recording duration. The original first command lasts for 7.61 seconds, whereas the processed command, with unvoiced frames removed, is much shorter, specifically having a duration of 2.30 seconds. In simpler terms, this means that 5.31 seconds, which corresponds to the unvoiced frames, have been effectively removed during the process. However, upon listening to cmd1.nosil, it becomes evident that the audio is now nearly incomprehensible. This is because the zapsilw function removed frames that weren’t strictly unvoiced but had a significantly lower amplitude compared to the majority of voiced frames. The zapsilw function includes an additional argument, the threshold, aimed at determining what qualifies as an unvoiced frame. The default threshold is 5% (5). Since the current threshold value resulted in the removal of an excessive number of frames for our specific case, let’s investigate the effects of adopting a much lower value, such as 0.3% (0.3):\n\ncmd1.nosil2 &lt;- zapsilw(cmd1, threshold=0.3, output=\"Wave\")\n\n\n\n\n\n\n\nFigure 3: Oscillogram comparison: original (top) vs. processed first command recording with unvoiced frames removed using threshold parameter equal to 0.3 (bottom)\n\n\n\n\n\n\ncmd1.nosil2\n\n\nWave Object\n    Number of Samples:      257922\n    Duration (seconds):     5.85\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nAs observed, the processed audio file is now longer than in the previous (5.85 vs 2.30 seconds), reflecting a decrease in the number of frames identified as unvoiced. Additionally, upon listening, the processed audio exhibits a more natural sound compared to the previous version. It’s worth noting that, while it may not replicate the exact naturalness of the original, this deviation is common. Humans naturally introduce pauses and breaks in speech, and the removal of unvoiced frames can contribute to this altered perception.\n\n\n\n\n\n\nDefining function arguments\n\n\n\nThe choice of preprocessing function arguments, which influence the behavior of the function, is a deliberate and thoughtful process. When establishing these parameters, we typically engage in an iterative approach. This involves listening to the resulting audio and visualizing it to ensure that the outcome aligns with our desired specifications. Although we refrain from explicitly illustrating this iterative process to maintain the clarity of this post, it’s essential to acknowledge its presence.\nIt’s important to recognize that determining the appropriate values for these arguments during file preprocessing is not a straightforward task. Instead, it demands careful consideration and may involve multiple iterations to arrive at the optimal values. This underscores the significance of a meticulous and thoughtful approach when fine-tuning these parameters to achieve the desired results.\n\n\nIt’s essential to consider that the decision to remove all unvoiced breaks should align with our analytical goals. If our aim is to analyze or extract information from the breaks and their duration, it might be preferable to solely eliminate unvoiced frames from the beginning and end of the recording using the noSilence function instead of the zapsilw function.\n\n\nUsing the rmnoise function\nTo improve audio quality, we take additional steps beyond eliminating unvoiced frames. We enhance the quality further by utilizing the rmnoise function to reduce background noise, as in this audio clip, you can clearly hear a disturbance that sounds like a metal object, most likely a teaspoon, hitting a glass or a cup.\nRecognizing that breaks in audio can offer insights into user frustration, we focus on using the processed version of the command—cmd1.cut—where we have only removed unvoiced frames from the start and the end, instead of using the version in which we removed all the unvoiced frames.\nIt’s important to highlight that in this specific scenario, we must explicitly set the output argument as \"Wave\" when using the rmnoise function to obtain a Wave object. Otherwise, the output would be in the matrix format. Additionally, we fine-tuned the noise reduction process by adjusting the spar argument. This parameter essentially governs the extent of noise reduction—higher values lead to less audible background noise. However, it’s essential to be cautious since increasing the spar value not only diminishes background noise but also introduces a trade-off, potentially altering other parts of the audio. The spar argument typically takes values between 0 and 1, but it can also take other values. In this case, since the background noise is quite prominent, we proceed to set a spar value equal to 1.15:\n\n# Remove noise\ncmd1.nonoise &lt;- rmnoise(cmd1.cut, output = \"Wave\", spar = 1.15)\n\n\n\n\nUpon listening, it’s evident that the rmnoise function effectively reduced the volume of the background noise— the metallic sound resembling an object striking a glass or cup—although traces of that sound persist. However, the heightened value of the spar argument in the rmnoise function slightly impacted the overall audio quality. To address this, we employ additional functions to further minimize the noise and enhance audio quality:\n\nInitially, we apply the afilter function, designed to eliminate signals with amplitudes below a specified threshold. The objective is to target the background noise, which now has a significantly lower amplitude compared to the rest of the audio. We control the threshold using the threshold argument, setting it to a low value, specifically 0.075.\nSubsequently, having applied the afilter function, we revisit the rmnoise function, this time with a reduced spar value of 0.75. With this step, we ensure thorough noise removal.\nFinally, we use the preemphasis function, which amplifies high-frequency content in the sample. Given that we have either completely or nearly eliminated the background noise, we emphasize the high-frequency content that may have downplayed by earlier functions. This strategic emphasis aims to enhance the quality of the remaining sound.\n\nWe store the resulting processed audio in a new object called cmd1.filtered. The code for all the mentioned steps is provided below:\n\ncmd1.filtered &lt;- afilter(cmd1.nonoise, output = \"Wave\", threshold = 0.075, plot = FALSE)\n\ncmd1.filtered &lt;- rmnoise(cmd1.filtered, output = \"Wave\", spar = 0.75)\n\ncmd1.filtered &lt;- preemphasis(cmd1.filtered, alpha = 0.975, output = \"Wave\")\n\n\n\n\nAs we can tell, the voice quality has significantly improved now. While there’s still a bit of background noise, it’s notably reduced compared to the original audio.\n\n\nPreprocessing the second command\nAfter completing the preprocessing for the initial command, we proceed to apply the same preprocessing steps for the second command in this way to have fair comparisons of audio files (as with the preprocessing we manipulate some of the features of the audio file).\n\n#Remove only unvoiced start and ending. In this case, we use a parameter level equal to 800, as there's some background noise at the start and at the end. In this way, we can cut these areas.\ncmd2.cut &lt;- noSilence(cmd2, level = 800)\n#Remove noise\ncmd2.nonoise &lt;- rmnoise(cmd2.cut, output = \"Wave\", spar = 1.15)\n\ncmd2.filtered &lt;- afilter(cmd2.nonoise, output = \"Wave\", threshold = 0.075, plot = FALSE)\n\ncmd2.filtered &lt;- rmnoise(cmd2.filtered, output = \"Wave\", spar = 0.75)\n\ncmd2.filtered &lt;- preemphasis(cmd2.filtered, alpha = 0.975, output = \"Wave\")\n\n\n\n\n\n\n\nWriting sound files\nOnce you’ve made adjustments or enhancements to a sound file, preserving the edited version for future use is essential. The seewave package makes this process easy through savewav function, specifically designed for saving R sound objects as .wav files. To utilize this function effectively, you’ll need to specify three crucial arguments:\n\nR Sound Object (wave): R object you want to save as a .wav file.\nSampling Frequency (f): Sampling frequency for the saved .wav file. If the R object you want to save is of the Wave type, there’s no need to specify such argument.\nFilename (filename): Name under which the edited sound object will be saved.\n\nAs a practical example, let’s save the background noise-free version of cmd1 and cmd2 (cmd1.filtered, and cmd2.filtered) as .wav files named cmd1_filtered.wav and cmd2_filtered.wav, respectively, within our system.\n\nsavewav(cmd1.filtered, filename = \"cmd1_filtered.wav\")\nsavewav(cmd2.filtered, filename = \"cmd2_filtered.wav\")"
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#visualizing-sound",
    "href": "posts/2023/intro-to-voice-analytics/index.html#visualizing-sound",
    "title": "Introductory voice analytics with R",
    "section": "Visualizing sound",
    "text": "Visualizing sound\nAfter addressing the stages of reading, editing, and saving sound objects, our next step involves visualizing the characteristics of a sound wave. Visualization serves as the process of translating a sound wave into a graphical representation. The key aspects typically depicted in a sound wave visualization are its (1) amplitude, (2) frequency, and (3) a combination of the previous two. All of them are usually illustrated against time.\nThe primary visualizations employed for this purpose are:\n\nOscillograms: These representations focus on capturing amplitude variations, providing a visual insight into the intensity or strength of the sound wave at different points in time.\nSpectrograms: This type of visualization offers a comprehensive view of both frequency and the dynamic relationship between frequency and amplitude over time.\n\nIt is important to highlight that, in this context, our progression directly shifts to visualization without the intermediary step of feature extraction, as depicted in Figure 1. This decision is motivated, in part, by the inherent capability of the visualization functions to directly extract the pertinent features prior to their visualization.\n\nVisualizing amplitude\n\n\n\n\n\n\nAmplitude\n\n\n\nAmplitude quantifies the extent of a wave’s displacement from its average value. In the context of sound, it specifically denotes the degree to which air particles deviate from their equilibrium position. Amplitude serves as a key factor in determining the strength or loudness of a sound and is expressed in decibels (dB).\n\n\nOscillograms offer a visual representation of the amplitude of a soundwave plotted against time. They are often referred to as waveforms, as they graphically depict the variations within the sound wave itself. Oscillograms serve as valuable tools for discerning potential changes in loudness over time within a soundwave. In R, you can create oscillograms using the oscillo function from the seewave package. This function requires just one argument, the sound object. Moreover, oscillo provides the flexibility to customize various visual aspects, such as the title (using the title argument), label color (via the collab argument), and wave color (by setting the colwave argument). Additionally, you can specify the from and to arguments, similar to what we did during data processing, to generate an oscillogram for a specific time interval in seconds.\nTo gain insights from the oscillograms of the two Alexa commands, we aim to first visualize the entire soundwave including the wake word (cmd1 and cmd2) and then zoom in to focus solely on the articulation of the wake word.\nTo consolidate all four graphs within a unified plotting area, we employ the standard par and mfrow arguments in R, partitioning the plot into four distinct sections. For an exclusive display of the wake word, the from and to arguments within the oscillo function are utilized. Additionally, we utilize the colwave argument to distinguish the entire command plots in black and the isolated wake words in blue. We set automatic titles for the oscillograms by enabling the title argument to be TRUE for the complete commands, providing information on total time and sampling rate. For the wake words, we actively set a custom title by specifying the desired text in this argument.\nTo enhance differentiation between whole command and isolated wake word plots, we go a step further and adjust their axis label colors to red, setting the collab argument to red. The complete code is provided below:\n\nGeneral viewDetailed view of commands 1 and 2\n\n\n\npar(mfrow=c(2,2))\n\noscillo(cmd1.filtered, colwave=\"black\", collab=\"black\", title = TRUE)\n\noscillo(cmd2.filtered, colwave=\"black\", collab=\"black\", title = TRUE)\n\noscillo(cmd1.filtered, from=0, to=.7, colwave=\"blue\", collab=\"red\", \n             title = \"First     Command - wake word Only\")\n\noscillo(cmd2.filtered, from=0, to=.7, colwave=\"blue\", collab=\"red\", \n             title = \"Second Command - wake word Only\") \n\n\n\n\n\n\n\nFigure 4: Oscillograms for the first and second command, including the wake word (upper panel, colored in black) and isolated wake words (lower panel, colored in blue)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Detailed view of the oscillogram for the first command, including the wake word\n\n\n\n\n\n\n\n\n\nFigure 6: Detailed view of the oscillogram for the second command, including the wake word\n\n\n\n\n\n\nIn addition to the oscillograms we just created, which are presented in Figure 4. We have included two additional and separate oscillograms, presenting a detailed view for the initial and second commands, including the wake word. Notably, we adjusted the oscillogram of the initial command to align with the temporal segment of the second command. This adjustment facilitates a direct comparison between the two. Moreover, we have accompanied these two additional oscillograns by textual transcriptions for each utterance. The visual representations of these detailed oscillograms are illustrated in Figure 5 and Figure 6, respectively, and can be viewed by clicking the “Detailed view of commands 1 and 2” tab.\nUpon closer examination of these oscillograms, a few notable observations come to light at first glance:\n\nDifference in voice breaks: The first command exhibits a broader voice break between the wake word “Alexa” and the subsequent portion compared to the second command. Following this, the majority of voice breaks are relatively brief. However, in the second command, there is another extended voice break between “by Dana” and “on my Spotify”.\nEmphasis on Individual Words: The second command exhibits a somewhat clearer distinction between utterances in comparison to the first command. Moreover, discernible variations in the articulation of specific words are observable, as indicated by the distinct shapes of the utterances. For instance, in the first command, the term “on” is accentuated with a higher amplitude, signifying a louder pronunciation in contrast to the second command. Moreover, the wake word “Alexa” reveals disparities between the two commands, with more pronounced “irregularities” in the second command. These irregularities entail fluctuations in amplitude, particularly noticeable when enunciating the initial part of the word.\n\n\n\nVisualizing fundamental frequency\n\n\n\n\n\n\nFundamental frequency\n\n\n\nThe fundamental frequency (F0) is the lowest frequency present in a waveform, and it determines the perceived pitch of the voice, influencing how sounds are interpreted as high or low. At higher F0 values, the associated sounds are perceived as higher in pitch.\nTherefore, it plays a critical role in conveying the tonal and rhythmic properties of speech, being instrumental in transmitting linguistic objectives in speech communication. Additionally, it is intimately tied to gender perception—adult men generally exhibit F0 values ranging from 80 to 175Hz. While adult women typically fall between 160 and 270Hz.\n\n\nIn addition to examining amplitude, a crucial aspect involves visualizing the fundamental frequency. This is often represented as a plot of fundamental frequency against time, referred to as an “F0 contour” or “pitch track.”\nThis visualization yields valuable insights into the linguistic aspect of tone, contributing supplementary information to enrich our comprehension of message delivery. For the sake of simplicity, we will narrow our focus to the wake word for this visualization and subsequent visualizations. This decision is based on our previous observations in the oscillograms, where we discerned subtle variations in the shapes of the wake word “Alexa” between the two commands. Consequently, our objective is to delve deeper into understanding the delivery of this specific word, particularly in terms of its tone.\nTo proceed with this exploration, we employ the extractWave function to extract the wake words from the first and second commands. Specifically, we focus on the initial 0.7 seconds of both commands, the duration during which (approximately) the wake word is situated. In addition, we use the noSilence function, to make sure that we did not extract some unvoiced section at the start or at the end. The extracted wake word from the first command is saved as an object named w1, while the wake word from the second command is stored as w2:\n\nw1 &lt;- extractWave(cmd1.filtered, from = 0, to = .7, xunit = \"time\")\nw1 &lt;- noSilence(w1, level = 5)\n\n\n\n\n\nw2 &lt;- extractWave(cmd2.filtered, from = 0, to = .7, xunit = \"time\")\nw2 &lt;- noSilence(w2, level = 5)\n\n\n\n\nHaving isolated the two wake words, the next step involves concatenating them into a single audio file, with the second wake word playing immediately after the first one. This concatenation is achieved using the bind function from the tuneR package, which seamlessly combines the provided Wave objects:\n\n(wake_all &lt;- bind(w1,w2))\n\n\nWave Object\n    Number of Samples:      55317\n    Duration (seconds):     1.25\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nAfter combining the two wake words, we utilize the autoc function of the seewave package on the concatenated sound object, wake_all. This function produces a plot that visualizes the fundamental frequency of the audio object across different time points. Additionally, we specify the ylim parameter to limit the y-axis between 0 and 600 Hz (0.6 KHz). Finally, we add a vertical line using the abline function, separating the wake words from the first and second commands. The resulting plot from this process is depicted in Figure 7.\n\n#Note: ylim units need to be in KHz\nF0 &lt;- autoc(wake_all, ylim = c(0, 0.6)) \n#We add a separation line, between the wake word 1 and wake word 2\nabline(v = 0.635,col=\"red\",lwd=2,lty=2)\n\n\n\n\n\n\n\nFigure 7: Pitch track for the two wake words\n\n\n\n\n\nThis plot reveals distinct pitch contours for both wake words. In the wake word of the first command, the fundamental frequency remains relatively constant, hovering around 200 Hz (0.2 KHz) for the initial 0.2 seconds. Subsequently, there is some variability for a few seconds before a noticeable increase during the last 0.15 seconds, reaching approximately 300 Hz. Conversely, in the wake word of the second command, the fundamental frequency ascends from 200 Hz (0.2 KHz) to around 300 Hz in the first 0.2 seconds, followed by a subsequent decrease.\n\nVisualizing formants\nTo enrich our analysis, we proceed to visualize key formants, which are the frequencies that resonate most prominently in human speech. The first three formants, denoted as F1, F2, and F3, are particularly informative. Formants play a pivotal role in discerning distinct vowels, contributing to the nuances of speech sounds.\nFor formant visualization, we employ the formanttrack function from the phonTools package. It’s essential to note that the phonTools package exclusively supports mono audio. In our case, since we are dealing with stereo files, we specify only one channel. The channel of a wave object can be accessed using the @ operator along with the desired channel name (left or right). Additionally, since phonTools is not specifically tailored for Wave objects, the sampling frequency of the sound file must be manually set using the fs argument. Thus, we proceed to load the phonTools package to plot the first three formants and, then, use the formanttrack function to the two wake words:\n\nlibrary(phonTools)\npar(mfrow=c(1,2))\nformanttrack(w1@left, fs=w1@samp.rate, formants=3, periodicity=.5)\nformanttrack(w2@left, fs=w2@samp.rate, formants=3, periodicity=.5)\n\n\n\n\n\n\n\nFigure 8: First, second ,and third formant tracks for the two wake words associated with the first (left panel) and second (right panel) commands\n\n\n\n\n\nIn Figure 8, we can see the first three formants for each wake word represented by different colors. By scrutinizing the formant tracks of the wake words in both commands, discernible distinctions emerge, offering insights into the speaker’s tone and emotional disposition.\nIn dissecting the first command’s wake word, we observe a more tightly spaced and evenly distributed set of formant frequencies compared to the second command’s wake word. This discrepancy implies a composed and relaxed speech pattern for the first command’s wake word than for the second, which exhibits greater variability, hinting at heightened tension and force in the speaker’s voice—reflecting the speaker frustration.\nDigging deeper into the analysis, the higher first formant frequency (F1) in the second command’s wake word suggests a wider mouth opening or an elevated tongue position, contributing to a more resonant and forceful vocal delivery. Similarly, the elevated second formant frequency (F2) in the second command’s formant track points to lip rounding or vocal tract narrowing, characteristics associated with increased vocal strength.\nOf particular note is the less pronounced third formant frequency (F3) in the second command’s wake word, indicating a degree of vocal tract constriction. While this intensifies the voice, it may also impart a muffled or harsh quality.\nIn summary, the formant tracks strongly imply that the speaker imparts greater force and tension in the second command, highlighting the undercurrent of frustration. Recognizing these discernible patterns in vocal intensity and tension enriches our understanding of the speaker’s emotional state and demeanor.\n\n\n\nSpectrograms\nSpectrograms offer a detailed, multidimensional representation of a soundwave, representing time along the x-axis, frequency along the y-axis, and amplitude levels (loudness) through varying color codes. They are specially useful for detecting audio problems by sight.\nThe seewave package provides the spectro function, allowing us to easily create spectrograms. When using this function, you only need a Wave object as input. However, there are optional parameters that allow you to customize the appearance of the spectrogram. For instance, the flim argument allows the specification of minimum and maximum frequencies displayed, andthe osc parameter introduces an oscillogram at the bottom of the spectrogram plot.\nThe default color scheme in the spectro function is relative, utilizing cyanred for regions with the highest amplitude in comparison to the entire audio representation, with all other colors relative to that maximum value. As a result, for an accurate comparison between the two wake words, rather than creating separate spectrograms for each, we will generate a unified spectrogram for the binned wake words (wake_all). Furthermore, we add a vertical line separating both wake words by using the abline function:\n\nspectro(wake_all, osc=TRUE, flim=c(0,1.5))\nabline(v = 0.62,col=\"red\",lwd=2,lty=2)\n\n\n\n\n\n\n\nFigure 9: Spectrogram for the wake words associated with the first and second commands\n\n\n\n\n\nBy checking Figure 9, we can observe how the second command’s wake word, shows a wider range of frequencies, especially at the beginning of the word “Alexa”. Moreover, the lower frequencies in this wake word are louder than any other part of both wake words. Importantly, the second wake word also seems to have higher variability in terms of intensity."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#acoustic-feature-extraction",
    "href": "posts/2023/intro-to-voice-analytics/index.html#acoustic-feature-extraction",
    "title": "Introductory voice analytics with R",
    "section": "Acoustic feature extraction",
    "text": "Acoustic feature extraction\nAfter visually inspecting the audio, we move on to acoustic feature extraction. Visual examination helps us understand vocal features like amplitude and fundamental frequency, but it’s more of a qualitative overview. This overview can guide us in preprocessing or identifying specific areas that need attention. Getting a greater understanding of the audio data requires extracting numerical information that we can obtain through acoustic feature extraction. This process converts auditory signals into measurable characteristics, allowing for a more detailed analysis. Hence, we proceed to extract some key vocal features across the time, amplitude, frequency, and spectral domains.\n\nTime associated characteristics\nThe primary measure in the time domain is duration, usually expressed in seconds or milliseconds. It indicates the temporal length of a soundwave. The duration function in the seewave package facilitates the direct extraction of a sound object’s duration in seconds. By applying this function to two commands, we observe that the first command has a shorter duration (6.93 seconds), compared to the second command (7.68 seconds).\n\nduration(cmd1.filtered)\n\n[1] 6.932744\n\nduration(cmd2.filtered)\n\n[1] 7.677347\n\n\nThe soundgen package is a handy package for feature extraction, and more specifically its analyze function. This function enables the extraction of various features spanning time, amplitude, frequency, and spectral domains. Examples include the fundamental frequency, percentage of voiced frames, amplitude, Harmonics-to-Noise ratio, and more.\nWhen you use the analyze function, it generates two data.frames. The first, a detailed data.frame ($detailed), breaks down each frame of the analyzed audio, with each column representing a vocal feature. The second, a summarized data.frame ($summary), condenses information to one row per file, summarizing vocal features with statistics like mean and standard deviation.\nWe can proceed to employ the analyze function to extract multiple vocal features from both the first and second commands, storing its outcome to feat_cmd1 and feat_cmd2:\n\nlibrary(soundgen)\nfeat_cmd1 &lt;- analyze(cmd1.filtered, plot = F)\nfeat_cmd2 &lt;- analyze(cmd2.filtered, plot = F)\n\nNow that we possess two objects containing information on distinct vocal characteristics from the two commands, our next step is to complement and quantify some of the key insights derived from our previous visualizations. In Figure 4, we observed differences in voice breaks between both commands. Specifically, the first command exhibited a longer voice break between the wake word and the rest of the command, whereas the second command had an extended voice break between “by Dana” and “on my Spotify”.\nDespite these observations, visually determining which command has a lower proportion of voice breaks was challenging. To address this, we can observe the percentage of voiced frames extracted by the analyze function. This can be achieved by extracting the voiced column from the summary data.frame for both feat_cmd1 and feat_cmd2:\n\n#Returns the proportion of voiced samples\nfeat_cmd1$summary$voiced\n\n[1] 0.5217391\n\nfeat_cmd2$summary$voiced\n\n[1] 0.5588235\n\n\nRevealing a greater percentage of voiced frames in the second (55.88%) compared to the first command (52.17%). In other words, the second command has a lower percentage of unvoiced frames (voice breaks) than the first command. This information can be easily translated into total seconds of voice breaks by subtracting the voiced value from 1 and multiplying the result by the duration of each command:\n\n(1 - feat_cmd1$summary$voiced) * duration(cmd1.filtered)\n\n[1] 3.31566\n\n(1 - feat_cmd2$summary$voiced) * duration(cmd2.filtered)\n\n[1] 3.387065\n\n\nWhen examining the duration of voice breaks in seconds, we can still see that the first command has longer duration voice breaks. However, this difference is more pronounced in relative terms, considering that the length of the first command is shorter than that of the second.\nIn addition, as we previously saw in Figure 4 is that the first command displays a lengthier voice break between the wake word and the subsequent part of the command compared to the second command. Thus, we may think that excluding that part, the second command has a greater amount of voice breaks, i.e. that the duration of the voice breaks in the rest of the command is lengthier.\nTherefore, it’s interesting to examine the percentage of voice breaks solely within the rest of the command, excluding the wake word and the break between it and the remainder of the command. To achieve this, we use the deletew function to remove the portion of each command containing the wake word. Subsequently, we ensure the elimination of all unvoiced frames preceding the remaining command section using the noSilence function:\n\ncmd1.without.wakeword &lt;- deletew(cmd1.filtered, from = 0, to = 1, output = \"Wave\")\ncmd1.without.wakeword &lt;- noSilence(cmd1.without.wakeword, level = 25)\nfeat_cmd1_no_wakeword &lt;- analyze(cmd1.without.wakeword, plot = F)\n\ncmd2.without.wakeword &lt;- deletew(cmd2.filtered, from = 0, to = 1, output = \"Wave\")\ncmd2.without.wakeword &lt;- noSilence(cmd2.without.wakeword, level = 25)\nfeat_cmd2_no_wakeword &lt;- analyze(cmd2.without.wakeword, plot = F)\n\nAfter doing that, we can now extract the proportion of voiced frames by referencing the voiced column in the summary data.frame of the generated output:\n\nfeat_cmd1_no_wakeword$summary$voiced\n\n[1] 0.7043011\n\nfeat_cmd2_no_wakeword$summary$voiced\n\n[1] 0.6300813\n\n\nThese values reveal that after removing the wake word and the voice break between it and the subsequent part of the command, the first command exhibits a higher proportion of voiced frames (70.43%) compared to the second command (63.01%). Consequently, when excluding the wake word, the voice breaks in the second command are longer than those in the first command.\nTherefore, the first command has an overall greater proportion of voice breaks than the second command. However, this increased proportion of voice breaks is largely attributed to the extended break following the wake word. Once we eliminate the wake word and the subsequent break from both commands, it becomes evident that the second command actually has a higher proportion of voice breaks.\n\n\nIntensity associated characteristics\nPreviously, we delved into this domain by inspecting the oscillograms for the two commands. In doing so, we discerned subtle distinctions in the shapes of the individual words within each command. Expanding on this observation, our subsequent action entails quantifying the central point around which the amplitude tends to fluctuate within each command. This quantification is achieved by calculating the mean of the amplitude across each command. Additionally, we aim to measure the extent of deviation from this central point, which we will quantify using the standard deviation of the amplitude.\nAs previously said, the amplitude of a soundwave indicates its power or loudness, where smaller amplitudes represent softer sounds, and larger amplitudes denote louder ones. It essentially measures how far air particles deviate from their usual position. It’s important to note that these deviations can be both positive and negative. To tackle this, a common method for measuring amplitude is using the root mean square.\nThe analyze function extracts the root mean square amplitude (ampl), which calculates the root mean square of the amplitude, excluding unvoiced frames. However, when summarizing this value over a time range, it might be lower than its actual value because unvoiced segments are considered. To address this, the analyze function also extracts the root mean square amplitude for only the voiced areas (ampl_noSilence). Therefore, we proceed to compare the average root mean square amplitude, excluding unvoiced areas, between the first and the second command:\n\nfeat_cmd1$summary$ampl_noSilence_mean\n\n[1] 0.00150876\n\nfeat_cmd2$summary$ampl_noSilence_mean\n\n[1] 0.001395994\n\n\nExamining these values, we notice that the first command, on average, has a slightly higher amplitude than the second command. Moving forward, our next step involves determining the extent to which the amplitude deviates from this average within each command. This variability is quantified through the calculation of the standard deviation, which can also be obtained from the summary data.frame generated by the analyze function:\n\nfeat_cmd1$summary$ampl_noSilence_sd\n\n[1] 0.0009200839\n\nfeat_cmd2$summary$ampl_noSilence_sd\n\n[1] 0.0009812488\n\n\nWe can observe that both commands have nearly identical standard deviations, indicating similar variations in amplitude for both commands.\nSimilarly, we could obtain information regarding the subjective loudness through the loudness column extracted through the analyze function, providing a more comprehensive measure:\n\nfeat_cmd1$summary$loudness_mean\n\n[1] 0.2950901\n\nfeat_cmd2$summary$loudness_mean\n\n[1] 0.3132622\n\nfeat_cmd1$summary$loudness_sd\n\n[1] 0.2664184\n\nfeat_cmd2$summary$loudness_sd\n\n[1] 0.2642991\n\n\nSimilar to the analysis of amplitude, we observe a close resemblance in both the average and standard deviation of loudness between the two commands.\nTo get into a higher level of granularity, we could also extract such information for the wake words associated with each command. To do so, first, we need to use the analyze function, which we can do in the following way:\n\nfeat_w1 &lt;- analyze(w1, plot = F) \nfeat_w2 &lt;- analyze(w2, plot = F) \n\nNow, we can move forward to compute the average value and standard deviation of the loudness for the wake words associated with each command by accessing to the columns loudness_mean and loudness_sd:\n\nfeat_w1$summary$loudness_mean\n\n[1] 0.2046364\n\nfeat_w2$summary$loudness_mean\n\n[1] 0.2783571\n\nfeat_w1$summary$loudness_sd\n\n[1] 0.1235279\n\nfeat_w2$summary$loudness_sd\n\n[1] 0.175175\n\n\nAs we can observe, there are differences in loudness between the two wake words. The wake word in the second command exhibits both a higher average loudness (0.28 sone) and a greater standard deviation in loudness (0.18 sone) compared to the first (M = 0.28 sone, SD = 0.18 sone).\n\n\nFrequency associated characteristics\nThe fundamental frequency (F0) is a key vocal feature in the frequency domain. Similar to the approach taken with amplitude, we will now extract the average and standard deviation of the fundamental frequency using the features previously obtained through the analyze function:\n\nfeat_cmd1$summary$pitch_mean\n\n[1] 229.3688\n\nfeat_cmd2$summary$pitch_mean\n\n[1] 234.4607\n\nfeat_cmd1$summary$pitch_sd\n\n[1] 40.54858\n\nfeat_cmd2$summary$pitch_sd\n\n[1] 28.99324\n\n\nObserving how the second command has a slightly higher average fundamental frequency (M = 234.46 Hz) than the first command (M = 229.37 Hz), while also having a higher standard deviation of the fundamental frequency (SD command 1 = 40.55 Hz; SD command 2 = 28.99 Hz).\nTo get into a higher level of granularity, we can apply the same analysis only to the wake word “Alexa”, by extracting the average and standard deviation of the fundamental frequency for the two wake words by accessing the previously created feat_w1 and feat_w2 objects:\n\nfeat_w1$summary$pitch_mean\n\n[1] 171.4803\n\nfeat_w2$summary$pitch_mean\n\n[1] 238.2434\n\nfeat_w1$summary$pitch_sd\n\n[1] 13.13345\n\nfeat_w2$summary$pitch_sd\n\n[1] 53.01586\n\n\nIn this case, the previously observed differences become even more pronounced—the second wake word has a much higher average fundamental frequency (M = 238.24 Hz) and standard deviation (SD = 53.02 Hz) than the first (M = 171.48 Hz; SD = 13.13 Hz).\nIn addition to extracting details about the fundamental frequency, we can delve into information about the formants. In Figure 8, we already saw that the track of the formants presented slight differences between the two wakewords. Consequently, our next step involves further characterising these differences, by extracting both the average and standard deviation of the frequency for the first formant (F1) across various commands and wake words. This information is accessible through the f1_freq_mean and f1_freq_sd columns in the detailed data.frame:\n\nfeat_cmd1$summary$f1_freq_mean\n\n[1] 346.1914\n\nfeat_cmd2$summary$f1_freq_mean\n\n[1] 392.5255\n\nfeat_cmd1$summary$f1_freq_sd\n\n[1] 117.9901\n\nfeat_cmd2$summary$f1_freq_sd\n\n[1] 242.5611\n\n\n\nfeat_w1$summary$f1_freq_mean\n\n[1] 377.39\n\nfeat_w2$summary$f1_freq_mean\n\n[1] 427.1445\n\nfeat_w1$summary$f1_freq_sd\n\n[1] 65.50526\n\nfeat_w2$summary$f1_freq_sd\n\n[1] 92.08512\n\n\nAs we can observe, the second command exhibits a slightly higher average frequency for the first formant (M = 392.53Hz) compared to the first command (M = 346.19 Hz). However, it’s noteworthy that the standard deviation of the first formant is greater for the second command (SD = 242.56 Hz) than for the first command (SD = 117.99 Hz).\nUpon closer examination of the wake words, this relationship undergoes partial modification. The wake word associated with the second command displays both a higher average and standard deviation for the frequency of the first formant (M = 427.14 Hz; SD = 92.09 Hz) in comparison to the wake word linked to the first command (M = 377.39 Hz; SD = 92.09 Hz).\nSimilarly, we can extract that information for the second formant (F2), by accessing the f2_freq_mean and f2_freq_sd columns in the detailed data data.frame:\n\nfeat_cmd1$summary$f2_freq_mean \n\n[1] 785.8252\n\nfeat_cmd2$summary$f2_freq_mean \n\n[1] 879.1609\n\nfeat_cmd1$summary$f2_freq_sd \n\n[1] 433.0889\n\nfeat_cmd2$summary$f2_freq_sd\n\n[1] 492.0018\n\n\n\nfeat_w1$summary$f2_freq_mean \n\n[1] 728.2679\n\nfeat_w2$summary$f2_freq_mean \n\n[1] 849.607\n\nfeat_w1$summary$f2_freq_sd \n\n[1] 253.4525\n\nfeat_w2$summary$f2_freq_sd\n\n[1] 130.4056\n\n\nExamining these values, we observe that the first command has a higher average and standard deviation for the frequency of the second formant (M = 785.83 Hz; SD = 433.09 Hz) compared to the second command (M = 879.16 Hz, SD = 492 Hz).\nZooming in on the wake words, we note that the average frequency for the wake word associated with the second command (M = 849.61 Hz) is higher than that for the wake word associated with the first command (M = 728.27 Hz). However, it’s interesting to point out that the wake word linked to the first command exhibits a higher standard deviation of the frequency of the second formant (SD = 130.41 Hz) compared to the wake word associated with the second command (SD = 130.41 Hz).\n\n\nSpectral associated characteristics\nSpectral features of a soundwave capture disturbances within the sound. Features assessing the spectral qualities of a soundwave typically gauge the level of disturbance or periodicity in the sound. Two such features of disturbances are the Harmonics-to-Noise Ratio (HNR) and the Wiener entropy (entropy), both directly extractable through the analyze function.\n\n\n\n\n\n\nHarmonics-to-Noise Ratio and Weiner Entropy\n\n\n\nThe Harmonics-to-Noise Ratio (HNR) measures the additive noise level in a voice signal. Lower HNR values signify a higher proportion of noise in comparison to the harmonic components, often associated with breathy or hoarse sounds. Consequently, the higher the HNR, the clearer the voice sounds.\nThe Wiener entropy, commonly referred to as spectral flatness, serves to measure the degree to which a sound exhibits characteristics of a pure tone rather than resembling noise. This quantification is achieved by analyzing the shape of the spectrum, which represents the distribution of energy across different frequencies in the signal.\nWhen the spectrum is flat, indicating a balanced energy distribution, the Wiener entropy value approaches 1.0, signifying white noise. Conversely, if the spectrum is spiky, with energy concentrated at specific frequencies, the Wiener entropy value tends towards 0, indicating a pure tone.\n\n\nTherefore, we proceed to extract the average entropy and HNR for the two commands:\n\nfeat_cmd1$summary$entropy_mean\n\n[1] 0.03265698\n\nfeat_cmd2$summary$entropy_mean\n\n[1] 0.03072856\n\n\n\nfeat_cmd1$summary$HNR_mean\n\n[1] 15.92719\n\nfeat_cmd2$summary$HNR_mean\n\n[1] 16.74486\n\n\nUpon examining the average entropy for the two commands, it becomes evident that their values are quite comparable. It’s noteworthy, however, that these values are relatively close to 0, indicating a resemblance more akin to a pure tone than to white noise.\nIn terms of the average HNR, both commands also possess similar values, having the second command with a slightly higher value (M = 16.74 dB) than the first (M = 15.93 dB). The high HNR value for both commands indicates a substantial dominance of harmonics to noise within the audio signals.\nIn addition to comparing the means of the entropy and HNR, we will also observe their standard deviation as in the previous cases:\n\nfeat_cmd1$summary$entropy_sd\n\n[1] 0.03373136\n\nfeat_cmd2$summary$entropy_sd\n\n[1] 0.02609803\n\n\n\nfeat_cmd1$summary$HNR_sd\n\n[1] 6.61309\n\nfeat_cmd2$summary$HNR_sd\n\n[1] 6.178095\n\n\nSeeing how for both commands, the standard deviation of the entropy and the HNR is fairly similar between the two commands.\nAs in the previous cases, we could also zoom in on the isolated wake word for each command and extract these statistics regarding the HNR and the entropy:\n\nfeat_w1$summary$entropy_mean\n\n[1] 0.02015783\n\nfeat_w1$summary$entropy_sd\n\n[1] 0.01381312\n\nfeat_w1$summary$HNR_mean\n\n[1] 13.37802\n\nfeat_w1$summary$HNR_sd\n\n[1] 4.217952\n\nfeat_w2$summary$entropy_mean\n\n[1] 0.0267721\n\nfeat_w2$summary$entropy_sd\n\n[1] 0.02322313\n\nfeat_w2$summary$HNR_mean\n\n[1] 11.21771\n\nfeat_w2$summary$HNR_sd\n\n[1] 3.512729\n\n\nAs in the case of the commands, we can see how the different values between the two wake words are quite similar. However, the values for the HNR are lower than for the command, implying that there’s a lower proportion of harmonics in relation to noise. Thus, the speaker when saying the command has slightly hoarser voice compared to the rest of the command. This hoarseness could be linked to a direct increase in vocal intensity when initiating speech.\n\n\nVocal features summary\nAfter extracting the various vocal features from the two commands, we present a well-organized table, Table 1, which encapsulates these characteristics for each command. Furthermore, we also include the vocal characteristics for the isolated wake words associated with both commands.\n\n\n\n\nTable 1: Vocal features for the two commands and isolated wake words\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand 1\nCommand 2\nWake word 1\nWake word 2\n\n\n\n\nTime domain\n\n\n\n\n\n\nDuration\n6.93s\n7.68 s\n0.63 s\n0.63 s\n\n\nPercentage of voiced frames\n52.17 %\n55.88 %\n58.33 %\n62.5 %\n\n\nPercentage of voiced frames, excluding wake word and subsequent break\n70.43 %\n63.01 %\n—\n—\n\n\nAmplitude domain\n\n\n\n\n\n\nAverage root mean square of the amplitude\n0.0015\n0.0014\n7^{-4}\n7^{-4}\n\n\nStandard deviation of the root mean square of the amplitude\n9^{-4}\n0.001\n3^{-4}\n5^{-4}\n\n\nAverage loudness\n0.3 sone\n0.31 sone\n0.2 sone\n0.28 sone\n\n\nStandard deviation of the loudness\n0.27 sone\n0.26 sone\n0.12 sone\n0.18 sone\n\n\nFrequency domain\n\n\n\n\n\n\nAverage fundamental frequency\n229.37 Hz\n234.46 Hz\n171.48 Hz\n238.24 Hz\n\n\nStandard deviation of the fundamental frequency\n40.55 Hz\n28.99 Hz\n13.13 Hz\n53.02 Hz\n\n\nAverage first formant (F1) frequency\n346.19 Hz\n392.53 Hz\n377.39 Hz\n427.14 Hz\n\n\nStandard deviation of the first formant (F1) frequency\n117.99 Hz\n242.56 Hz\n65.51 Hz\n92.09 Hz\n\n\nAverage second formant (F2) frequency\n785.83 Hz\n879.16 Hz\n728.27 Hz\n849.61 Hz\n\n\nStandard deviation of the second formant (F2) frequency\n433.09 Hz\n492 Hz\n253.45 Hz\n130.41 Hz\n\n\nSpectral domain\n\n\n\n\n\n\nAverage Wiener entropy\n0.03\n0.03\n0.02\n0.03\n\n\nStandard deviation of the Wiener entropy\n0.03\n0.03\n0.01\n0.02\n\n\nAverage Harmonics-to-Noise Ratio\n15.93 dB\n16.74 dB\n13.38 dB\n11.22 dB\n\n\nStandard deviation of the Harmonics-to-Noise Ratio\n6.61 dB\n6.18 dB\n4.22 dB\n3.51 dB\n\n\n\n\n\n\n\n\nWakeword\nLooking at Table 1 we can easily notice that the second command’s wake word is spoken more loudly, especially in the initial part of the word, as we previously observed in Figure 9. This illustrates how, in the second command’s wake word, the speaker raises their voice to say “Ale.” Additionally, the second command has a higher percentage of voiced frames, mainly because the speaker prolongs the letter “e”, as seen in Figure 9. Furthermore, we observe that the second command’s wake word has a higher average and standard deviation of the fundamental frequency. Specifically, this frequency rises during the first part of the command, corresponding to “Ale”, and then decreases again, as shown in Figure 7. This indicates that in the initial part of the word “Alexa”, the voice is higher, suggesting the speaker is raising their voice. Moreover, in the second command’s wake word, the average frequencies for the first two formants are also higher, indicating a more forceful delivery.\n\n\nFull command\nThe second command is lengthier than the first, this is primarily because when excluding the wake word and the voice break between the wake word and the rest of the command, there’s a higher percentage of unvoiced frames, i.e. lengthier voice breaks. By listening to the audio and also looking at Figure 4, we can observe how the speaker is emphasizing the individual words, hoping that in this way the device will understand the command, unlike in the first command. As, in the case of the wake words, we can also observe an average higher fundamental frequency for the second command’s wake word, indicating a higher voice. In addition, the frequencies for the first and second formant are also higher, indicating a more forceful delivery.\nThus, we could hear how the speaker was expressing higher frustration on the second command and by decomposing the command into several audio characteristics that we have visualized and quantified, we have gained a deeper understanding of what’s driving our perception of the user’s frustration while interacting with Alexa."
  },
  {
    "objectID": "posts/2023/intro-to-voice-analytics/index.html#summary",
    "href": "posts/2023/intro-to-voice-analytics/index.html#summary",
    "title": "Introductory voice analytics with R",
    "section": "Summary",
    "text": "Summary\nIn this analysis, we delved into a user’s interaction with Alexa. The user initially instructed Alexa to play “Something’s Cooking in My Kitchen” by Dana. Unfortunately, Alexa struggled to comprehend the command due to the user’s Scottish accent. Frustrated by this, the user repeated the command in a tone reflecting her annoyance.\nTo understand the impact of these different deliveries on the commands, we compared the two instances. The first command was delivered in a natural and composed manner, while the second was tinged with frustration. Our approach involved going through the whole voice analytics pipeline breaking down the speech into various characteristics and scrutinizing these aspects for disparities between the commands.\nBy dissecting the speech and quantifying different characteristics, we were able to gain a nuanced understanding of what changed between the two commands. This not only enhanced our perception of user frustration but also provided valuable insights into the nuanced elements contributing to the interaction.\nIt’s important to note that this analysis serves as a basic tutorial. We directly compared two commands from the same user with almost identical content (except for the addition of the word “my” on the second command). In more advanced voice analytics, we would analyze more than just two audios from a single speaker, employing more sophisticated statistical methods or even training machine learning models to predict outcomes of interest. Hence, this post serves as an introduction, offering fundamental concepts of voice analytics from a practical perspective."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html",
    "href": "posts/2023/voiceR-R-package/index.html",
    "title": "My first R package: voiceR",
    "section": "",
    "text": "The subtleties of our speech often unveil more about us than the mere words we utter. These subtleties can be quantified through a constellation of distinct vocal features. Together, these features offer a glimpse into an individual’s speech pattern, revealing a trove of information. Within each individual’s unique vocal features lies valuable insights into their personal traits, such as age and gender, as well as their current emotional state. Furthermore, they have been linked to broader evaluative outcomes, including perceptions of physical attractiveness and strength. In medical contexts, these features have proven diagnostic, aiding in studying speech pathologies like vocal loading and enabling the detection of conditions such as Parkinson’s disease. Additionally, they have been instrumental in predicting and monitoring the treatment of clinical depression.\nWhile technically oriented fields like computer science, with their cadre of adept researchers, swiftly embraced and expanded the realm of voice analytics—utilizing deep learning models to dynamically recognize discrete human emotions—less technically inclined disciplines recognized the potential of voice analytics but fell short in harnessing its vast capabilities. These disciplines acknowledged but did not fully exploit the power of voice analytics to describe, comprehend, and predict affective and cognitive aspects of human expression.\nTo bridge this gap and offer a practical interface for voice analytics, we have developed an R package aiming at making voice analytics more accessible: the voiceR package, which today has been published to CRAN."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#what-is-voicer",
    "href": "posts/2023/voiceR-R-package/index.html#what-is-voicer",
    "title": "My first R package: voiceR",
    "section": "What is voiceR?",
    "text": "What is voiceR?\nvoiceR is an R package specifically designed to streamline and automate voice analytics for social science research. This package simplifies the entire process, from data processing and extraction to analysis and reporting of voice recording data in the behavioral and social sciences. It provides an intuitive and user-friendly interface, including an interactive Shiny app, making it accessible for researchers. One of its key features is batch processing, enabling the simultaneous reading and analysis of multiple voice files. Moreover, voiceR automates the extraction of crucial vocal features, facilitating further in-depth analysis. Notably, it goes a step further by automatically generating APA-formatted reports tailored for typical between-group comparisons in experimental social science research. Figure 1 offers a video summary of voiceR’s key features.\n\n\n\nVideo\n\n\nFigure 1: Overview of the main voiceR features"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#voicer-prerequisites",
    "href": "posts/2023/voiceR-R-package/index.html#voicer-prerequisites",
    "title": "My first R package: voiceR",
    "section": "voiceR prerequisites",
    "text": "voiceR prerequisites\n\nInstalling voiceR\nGetting started with voiceR is a straightforward process. Begin by installing the package from CRAN using the following command:\n\ninstall.packages(\"voiceR\")\n\nOnce you’ve successfully installed the package, you’re ready to embark on your voice analytics journey using voiceR.\n\n\nRequired file name structure\nvoiceR relies on a specific file naming convention to ensure seamless processing of audio files. This convention is comprised of up to three components of metadata about the file, two of which are optional:\n\nID: A unique identifier for the speaker or recording.\nCondition (optional): The experimental condition or another grouping variable.\nDimension (optional): Additional survey or experiment information, such as additional conditions.\n\nThe different file name components should be separated by a non-alphanumeric character, such as an underscore (_).\nvoiceR extracts these components to provide additional information about the audios and enable comparisons between groups.\nOrder of the components is not important, as long as you identify the correct file name pattern structure. For example, the following file names are all valid:\n\n12345_happy_male.wav (ID_Condition_Dimension)\n123bcf.wav (ID)\nCovidPositive_Patient1.wav (Condition_ID)\n\n\nUsing the Null placeholder\nIf there are parts of the file name that are not any of the required components, you can use the Null placeholder to avoid them. For example, if you have additional information in the file name that does not belong to any of the categories that voiceR processes, you can use the Null placeholder to ignore that information.\nFor example, imagine you have a file named Audio_Participant345_Happy.wav. The Audio component of the file name is not required, so you could define the following pattern: Null_ID_Condition. This file name would still be valid, and voiceR would ignore the first component given that we used the Null placeholder.\nFigure 2 showcases how the voiceR package uses the name pattern structure to identify the different components.\n\n\n\n\n\n\nFigure 2: Examples of File Name Patterns"
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#comprehensive-functionality",
    "href": "posts/2023/voiceR-R-package/index.html#comprehensive-functionality",
    "title": "My first R package: voiceR",
    "section": "Comprehensive functionality",
    "text": "Comprehensive functionality\nvoiceR offers a suite of functions designed to simplify the voice analytics process. These functions cover reading and preprocessing audio files, automatic feature extraction, visualization of results, and even automatic report generation.\nFigure 3 offers a simple overview of the key voiceR functions, highlighting their scope within the voice analytics pipeline.\n\n\n\n\n\n\nFigure 3: Overview of the main voiceR functions\n\n\n\n\nReading multiple audio files\nThe initial step in the audio analytics process involves reading designated audio files. The readAudio function in the voiceR package achieves this seamlessly, systematically processing all audio files in a specified directory and its subdirectories if specified. Users can customize this function by providing a file path (path) and an optional character vector to filter for specific patterns (filter). Upon execution, this function efficiently imports audio files into R, generating a comprehensive list of Wave objects, each representing an imported audio file.\n\n\nPreprocessing multiple audio files\nFollowing successful import, preprocessing becomes imperative. The preprocess function in voiceR automates this process by normalizing amplitude and eliminating background noise from a list of Wave objects (audioList). Two optional logical parameters, normalizeAmplitude and removeNoise, allow users to tailor the preprocessing scope. Default settings include both amplitude normalization and noise removal. While suitable for most scenarios, advanced users can integrate functions from other packages, such as tuneR’s extractWave, for more intricate preprocessing. The output is a preprocessed list of Wave objects, which can be stored locally using the saveAudio function.\n\n\nAutomatic feature extraction for multiple audio files\nThe pivotal autoExtract function facilitates the extraction of vocal features from raw or preprocessed audio files. Operating in two modes, it can either automatically read and analyze audio files based on a specified path and optional patterns or analyze a pre-existing list of audio files in the R environment. The function produces a table containing key audio features for each analyzed file, such as duration, voice breaks percentage, amplitude envelope root mean square, average loudness, average pitch, pitch standard deviation, average entropy, and average Harmonics to Noise Ratio.\n\n\nVisualizing results\nGiven the wealth of information produced by autoExtract, effective visualization is paramount. The voiceR package offers two specialized functions for this purpose:\n\nnormalityPlots: Generates density plots for each audio feature, facilitating normality assessment through the Shapiro-Wilk test.\ncomparisonPlots: Produces box plots, aiding in the comparison of audio features across different conditions or dimensions. These plots include relevant statistical tests based on data normality.\n\nThese visualization functions, seamlessly integrated into the voiceR package, enhance the interpretability of audio data, enriching the depth and breadth of analysis.\n\n\nAutomatic report generation\nDespite the automation provided by voiceR functions, thorough documentation of primary findings remains essential. The autoReport function addresses this need by utilizing autoExtract output to generate an HTML report. This report encapsulates key vocal features of the analyzed audio files, including density plots, box plots, and automatically generated APA-formatted text and tables, highlighting differences between conditions or dimensions."
  },
  {
    "objectID": "posts/2023/voiceR-R-package/index.html#the-no-code-approach-the-voicer-shiny-app",
    "href": "posts/2023/voiceR-R-package/index.html#the-no-code-approach-the-voicer-shiny-app",
    "title": "My first R package: voiceR",
    "section": "The no code approach: the voiceR shiny app",
    "text": "The no code approach: the voiceR shiny app\nFor users who prefer not to code, voiceR offers the voiceRApp function. By invoking this function, you can launch the voiceR Shiny app, simplifying the selection and subsequent analysis of multiple audio files. It provides a dynamic view of results and the ability to download a comprehensive report summarizing the key findings. Figure 4 provides a snapshot of the voiceR app’s initial screen.\n\n\n\n\n\n\nFigure 4: voiceR shiny app"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nFeb 23, 2025\n\n\nData-driven web applications basics: Improving the architecture\n\n\n\n\nAug 25, 2024\n\n\nData-driven web applications basics: Containerizing our application\n\n\n\n\nJun 4, 2024\n\n\nData-driven web applications basics: Connecting to a database\n\n\n\n\nFeb 8, 2024\n\n\nData-driven web applications basics: Getting started with Flask\n\n\n\n\nOct 25, 2023\n\n\nMoney and the ascent of Bitcoin\n\n\n\n\nSep 12, 2023\n\n\nMy first R package: voiceR\n\n\n\n\nJan 7, 2023\n\n\nIntroductory voice analytics with R\n\n\n\n\nApr 7, 2022\n\n\nStatistics Foundations: Confidence intervals\n\n\n\n\nDec 28, 2021\n\n\nStatistics Foundations: Sampling error\n\n\n\n\nSep 25, 2021\n\n\nStatistics Foundations: Populations and samples\n\n\n\n\nMay 19, 2021\n\n\nWhy potential inflation could lead to a financial crisis?\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html",
    "title": "Money and the ascent of Bitcoin",
    "section": "",
    "text": "In recent years, Bitcoin has captured the attention of nearly everyone. Its popularity has surged, driven by both positive and negative events, to the extent that it’s now a rarity to encounter someone unfamiliar with the concept of Bitcoin. However, there remains a somewhat elusive question that many find challenging to address: Can Bitcoin truly be categorized as money? In this blog post, our primary objective is to offer a comprehensive response to this question.\nTo accomplish this, we embark on a journey to establish a foundational understanding of money: its intrinsic nature, significance, the evolutionary path a commodity must traverse to attain the status of money, the essential attributes it must possess, and an exploration of the various forms of money that have prevailed throughout history.\nSubsequently, we delve into a succinct portrayal of Bitcoin, culminating in a decisive exploration of whether it can legitimately lay claim to the title of “money.”"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#barter",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#barter",
    "title": "Money and the ascent of Bitcoin",
    "section": "Barter",
    "text": "Barter\nBarter is the simplest form of exchange; it refers to the transfer of a good or service for another good or service. For this reason, it is typically considered direct exchange since no third object partakes of the transaction.\nBarter requires cooperation between individuals and double coincidence of wants, i.e., that both parties have and are willing to exchange the good or service that the other party desires for the good or service that the other party possesses. Therefore, this form of exchange involves high transaction costs due to the opportunity cost incurred in finding an individual with whom to make the barter."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#indirect-exchange-and-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#indirect-exchange-and-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Indirect exchange and money",
    "text": "Indirect exchange and money\nThese high transaction costs involved in the bartering process led to the emergence and prevalence of indirect exchange, i.e., a type of exchange in which a good or service is exchanged for a more widely acceptable item, which can be subsequently used to exchange for the goods or services desired. Therefore, for indirect exchange to occur, acquired goods must be more marketable than those surrendered. As the greater the marketability of a good, the more it will facilitate the final objective: the acquisition of the desired good or service.\nIn this way, in indirect exchange systems, the most marketable goods became a media of exchange, i.e., widely accepted. At the same time, as these goods became more widely accepted they further increased their marketability, bolstering their position as a medium of exchange. And, in turn, displaced those goods with lower marketability as means of exchange. Thus, leading to an inevitable scenario in which only a single good was universally employed as a medium of exchange: money."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#functions-of-money-and-their-development",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#functions-of-money-and-their-development",
    "title": "Money and the ascent of Bitcoin",
    "section": "Functions of money and their development",
    "text": "Functions of money and their development\nTherefore, we can define money as a generally accepted medium of exchange. Nonetheless, in several definitions of money, two secondary functions are attributed to it:\n\nStore of value: It allows to transmit value through time and space.\nUnit of account: It permits the valuation of goods and services.\n\nNotwithstanding, for a good to become money, it is not necessary that it initially fulfills all the above functions. Indeed, goods are converted into money through a process by which they usually acquire some of these functions first, and then others are subsequently developed. In addition, the acquisition of new functions establishes synergies with the previous ones, reinforcing and consolidating their position.\nFor example, as the practice of using a good as a medium of exchange becomes widespread, people begin to hold it in preference to others, thus developing its function as a store of value and reinforcing its function as a medium of exchange. As a result, acceptability becomes more widespread leading economic agents to set prices using this good as a reference, thereby becoming a unit of account.\nOn the other hand, for a good whose value is relatively stable, there will be economic agents interested in buying it not to satisfy their most direct needs, but to maintain their future purchasing power. In this way, it will be accepted by a growing number of agents and, therefore, become a medium of exchange. And, thus, economic agents begin to treat it as a unit of account."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#properties-of-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#properties-of-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Properties of money",
    "text": "Properties of money\nNevertheless, for money to fulfill the above functions, it must meet various characteristic requirements:\n\nPortability: It must be possible to transport or accumulate a large amount of value in a small amount of space, thereby facilitating transferability and hoarding.\nDivisibility: Money should be divisible into different units to enable precise pricing and facilitate transactions.\nUniformity: It must be easy to identify units of money having the same value, enabling the counterparty receiving the money to promptly discern its value. Thus, facilitating its transferability.\nDurability: It must remain intact over time without physically degrading or disappearing, therefore favoring its hoarding."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#types-of-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#types-of-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Types of money",
    "text": "Types of money\n\n\n\n\n\n\nNote\n\n\n\nThis subsection is merely for informational purposes and is not relevant for the understanding of subsequent sections of the post. Readers who wish to omit it, may do so by clicking here.\n\n\nThroughout history, money has taken many forms. Although today fiat money is the norm, commodity money characterized much of earlier history.\n\nCommodity money\nCommodity money refers to real units of a specific commodity universally accepted as a counterpart for goods and services. Accordingly, commodity money has intrinsic value. Historically, a myriad of commodities has served at one time or another as a medium of exchange: animal skins, salt, barley, tea, gold, silver, tobacco, etc.\nAs economies became more complex, increasing the number of payments, commodity money became cumbersome. The quality of the metals was continually tested to ensure that they had not been tampered with or that they were not of a lower grade than assumed. On the other hand, agricultural products were relatively difficult to transport compared to metals because of their lower unit value. For this reason, two alternatives emerged that sought to solve these problems: coinage and representative money.\nCoinage was a revolutionary invention that changed people’s way of thought. Coinage seems to have first occurred in the Kingdom of Lydia around 600 BC when the first electrum coins were minted, a natural alloy of gold and silver. (recent findings suggest that coinage may have originated in China a few years earlier, near Guanzhuang in Henan province). Consequently, metallic coins are a type of commodity money, which is highly transportable and divisible. Moreover, minted coins contained a mark that guaranteed their weight and purity, i.e., their value, thus solving the uniformity problem that untreated metals faced.\n\n\nRepresentative money\nRepresentative money is money whose value does not derive from the value of the material it is made of, but from what it represents, since each monetary unit is supposed to represent a fixed quantity of something that has real value.\nSome scholars have suggested that this form of money pre-dates coinage. In the ancient empires of Babylon, Egypt, China, and India temples, and palaces were considered inviolable, the former due to religious reasons and the latter due to the heavy protection they possessed. Therefore, they became safe places to store precious goods. Depositors received a certificate attesting deposits, which was a claim to the deposited goods. These certificates have been associated with multiple objects which were used in international trade, such as glazed scarabs in Egypt and cylindrical seals in Babylon and India. For this reason, these certificates are believed to have been used as a means of payment. Furthermore, due to the implementation of the gold standard, representative money occupied a central role during the 20th century.\n\n\nFiat money\nFiat money refers to money that has no intrinsic value and does not represent anything of intrinsic value. Public trust in both the issuer and the money itself is what drives its value. Such trust can be attributed, in most cases, to the confidence in the future stability of money’s purchasing power.\nSome authors have defined state-issued fiat money more critically as credit reimbursable for the payment of future tax obligations. And, therefore, associating fiat money as a way of using a government’s liabilities as a store of value.\nIn 1971, following the end of the Bretton Woods agreement, we find the emergence of modern fiat money. Nevertheless, in the fifth century B.C in Carthage, we already find one of the earliest known forms of widespread use of fiat money. This money was a small piece of leather sealed by the state, which enveloped a mysterious substance that nobody knew its composition except the maker. Only by breaking the seal, its composition could be known. However, in the presence of this event, this money was considered worthless.\nRecent studies have speculated that the mysterious substance was, in fact, tin or a compound of copper and tin and that the wrapping of this compound was not leather, but parchment."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#a-concise-overview-of-how-bitcoin-works",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#a-concise-overview-of-how-bitcoin-works",
    "title": "Money and the ascent of Bitcoin",
    "section": "A concise overview of how Bitcoin works",
    "text": "A concise overview of how Bitcoin works\nEach time a transaction occurs, the network records the Bitcoin address of the receiver and sender together with the amount transferred. This information is entered into the end of a ledger, called the blockchain. The blockchain is updated about every 10 minutes, and it is sent to every full node (computers connected to the Bitcoin network that verify all of the rules of Bitcoin).\nEvery transaction is encrypted with public-key cryptography and is verified by miners, computers connected to the Bitcoin network that secure the blockchain. The main objective of the miners is to fix the transaction history and prevent transaction fraud. This is done by solving a computer-intensive process by which individuals involved are rewarded with newly minted Bitcoins.\nMoreover, rewards given to miners are not always the same, yet they decline geometrically, with a 50% reduction every 210,000 blocks. This pattern was established because it approximates the rate at which gold is extracted."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Is Bitcoin money?",
    "text": "Is Bitcoin money?\nBitcoin meets all the necessary characteristics required to fulfill the functions that we previously stated that money must accomplish. As a digital asset, it is extensively portable, being its transferability and accumulation easy. In addition, it is deeply divisible: one Bitcoin can be divided into 100 million units, commonly known as satoshis. Likewise, the digital nature of Bitcoins makes them uniform and durable.\nHowever, the fact that it meets the necessary characteristics to fulfill the functions of money does not imply that it fulfills them. Consequently, before we can say whether Bitcoin is money or not, we must first analyze whether it fulfills these functions: (1) generally accepted medium of exchange, (2) store of value, and (3) unit of account.\n\nGenerally accepted medium of exchange: As of today, Bitcoin is not a generalized medium of exchange. We cannot go to the bakery next to our house and buy bread with it, nor can we go to a car dealership and buy a car with it.\nStore of value: Bitcoin has historically had severe price volatility, which is not favoring its function as a store of value.\nUnit of account: The limited adoption of Bitcoin as a means of payment and its price volatility do not foster its use as a unit of account.\n\nThus, we can say that Bitcoin currently cannot be considered money. Notwithstanding this, given the attractive properties of Bitcoin, we might ask ourselves a slightly more complex question: is Bitcoin in the process of becoming money?"
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-in-the-process-of-becoming-money",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#is-bitcoin-in-the-process-of-becoming-money",
    "title": "Money and the ascent of Bitcoin",
    "section": "Is Bitcoin in the process of becoming money?",
    "text": "Is Bitcoin in the process of becoming money?\nIn the beginning, Bitcoin had a highly volatile price, as it was a new, virtually unknown asset that very few people owned. Nevertheless, Bitcoin was an asset with quite appealing monetary properties, coupled with a decentralized scheme and a finite money supply.\nThese properties led more and more economic agents to believe that Bitcoin could become a future store of value and, thus, decided to acquire and hold Bitcoin. Likewise, the growing demand for Bitcoin led to an increase in its popularity, which drove more economic agents to reach this reasoning. This escalating demand not only bolstered Bitcoin’s popularity but also created a self-reinforcing cycle of adoption, occasionally disrupted by external factors.\nThis progression led to a notable reduction in Bitcoin’s downside volatility, as demonstrated in Figure 1, making it increasingly attractive as a potential store of value. Nevertheless, this trajectory was not a continuous one. Instead, it was interrupted at different points by several external shocks. For instance, on February 8, 2021, coinciding with a low downside volatility period, Tesla announced a $1.5 billion Bitcoin. This event elevated Bitcoin’s appeal, indicating to investors that it could be a lucrative investment, thereby driving up its demand and price.\nHowever, in 2022, a series of external negative shocks, such as the TerraUSD stablecoin crash and the FTX collapse, shook investor confidence in the crypto market. This resulted in a decline in the prices of numerous cryptocurrencies, including Bitcoin, increasing its downside variability. However, after this significant price drop, Bitcoin’s value stabilized, suggesting to investors that the impact of these prior external factors might have dissipated. This stabilization restored their confidence, leading to gradual, albeit progressive, price increases and, consequently, one of the lowest levels of downside volatility for Bitcoin.\nThis newfound stability motivated investors, prompting numerous major companies like Ferrari to announce their willingness to accept Bitcoin as a payment method. Additionally and primarily, steps taken by Blackrock to launch a Bitcoin ETF further intensified demand, subsequently driving up its price.\n\n\n\n\n\n\n\n\nFigure 1: Bitcoin downside risk\n\n\n\n\n\nConsequently, the adoption of Bitcoin as a store of value is becoming more and more widespread. Once a store of value is well established enough, i.e., many agents understand that this asset is a good store of value, they can start to demand it against the sale of their goods.\nDespite this, not many companies do offer their goods or services in exchange for Bitcoin. However, if the popularity and the trend towards increased Bitcoin price stability are not affected mid-term, an increasing number of agents will accept Bitcoin as a means of payment.\nFinally, if Bitcoin’s function as a medium of exchange were to develop, it would increase its popularity and at the same time solidify its position as a store of value. Enabling future economic agents to start accounting with Bitcoin, i.e., opening the possibility of development to the function of unit of account.\nTherefore, we cannot say that Bitcoin is in the process of becoming money, but we can say that Bitcoin is currently in the process of becoming a store of value. That said, whether such a function is widely recognized depends on the maintenance of the trend in which it is now present: further decrease in its downward volatility without giving up its current popularity. Moreover, the development of other functions as a generalized medium of exchange and unit of account is still a long way off and is conditional on the soundness of the development of the store of value function. In addition, even if at some point the store of value function is fully developed, the development of other functions will still remain highly uncertain.\nFigure 2 summarizes the process by which Bitcoin could obtain the functions of money and thus become money. Take into account that this figure is an abstraction and does not consider various factors that could influence this process. This includes the potential impact of external shocks, exemplified by the events of 2022, as well as the variable durations of each transition phase. Moreover, it’s essential to acknowledge the non-linear nature of this progression, where steps forward can be accompanied by steps backward, adding complexity to the overall monetization process.\n\n\n\n\n\n\nFigure 2: Bitcoin monetization process\n\n\n\nRecently, Taleb has argued that Bitcoin can never be a store of value, since its fundamental value is 0. In the next subsection we address this criticism."
  },
  {
    "objectID": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#against-talebs-argument-of-bitcoins-impossibility-to-become-a-store-of-value",
    "href": "posts/2023/money-and-the-ascent-of-bitcoin/index.html#against-talebs-argument-of-bitcoins-impossibility-to-become-a-store-of-value",
    "title": "Money and the ascent of Bitcoin",
    "section": "Against Taleb’s argument of Bitcoin’s impossibility to become a store of value",
    "text": "Against Taleb’s argument of Bitcoin’s impossibility to become a store of value\nIn the summer of 2021, Nassim Taleb published a short article entitled Bitcoin, currencies, and fragility, in which one of his arguments is that the value of Bitcoin is exactly 0 and, therefore, Bitcoin cannot be a store of value.\nTo argue this, Taleb relies on the premise that the fundamental value of any asset is equal to the sum of the present value of its expected future cash flows together with the terminal value that the asset will have.\nTherefore, as Bitcoin does not generate cash flows, i.e., the mere fact of owning Bitcoin as such does not result in monetary payments, meaning that the value of Bitcoin only depends on its terminal value.\nAdditionally, according to Taleb, Bitcoin is a technology. Therefore, Bitcoin, like any other technology, will eventually be replaced by another. As a result, its terminal value will be 0. Consequently, Taleb argues that since its fundamental value is 0, Bitcoin will not become money.\nNevertheless, in this argument, Taleb avoids two important points: (1) humans are not completely rational, and (2) Bitcoin is in the process of becoming a store of value as we saw in the previous subsection. Taleb may be right, Bitcoin may not yet be a store of value as such. But, this does not imply that it cannot become one, as we have seen in the previous subsection.\nThe reason behind this is irrationality in the early stages of Bitcoin, at that time it could be valid to say that Bitcoin had a value of 0. Nevertheless, multiple economic agents were attracted by it, which, as we have seen in the previous section, led to the start of the development of Bitcoin’s store of value function. As a result, many economic agents already consider Bitcoin as a store of value, while others expect it to become one in the near future.\nSuch a fact is critical since assets that act as a store of value provide the holder with a service: the transfer of value in space and time. Consequently, as Bitcoin is in the process of developing its store-of-value function, this implies that the expected flows of Bitcoin are no longer zero, but the implicit value of this service. Therefore, Bitcoin’s fundamental value should be greater than 0.\nTherefore, in the case of Bitcoin, we face an instance in which a collective irrationality has endowed this asset with a value that a priori it should not have. Nevertheless, as part of this process, the store of value property has begun to develop, which justifies that this asset has value, and, at the same time, this value allows it to act as a store of value."
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html",
    "href": "posts/2024/connecting-db-app/index.html",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "",
    "text": "In the previous post of the data driven web applications basics, we built a basic Flask application that enabled users to register their favorite websites by inputting a URL and receive a confirmation that the URL had been successfully registered. While this was a useful starting point, the application had a significant limitation: it did not store the registered URLs. As a result, the inputted information was not preserved and could not be accessed or used later. Without a mechanism to save this data, the application lacked practical utility—after all, registering a URL is pointless if it can’t be retrieved and used in the future.\nWithout a system to store the registered URLs, we cannot, for instance, track which URLs users have submitted, determine which sites are most popular, or analyze user preferences. Therefore, to make our application more useful, we need a way to store information persistently. This requires a reliable storage solution that allows data to be saved, retrieved, and managed efficiently.\nInitially, using Excel might seem like a convenient solution—especially for small-scale applications or for users familiar with spreadsheet software. For example, when a user registers a URL, the application could write the URL to a new row in an Excel file. However, this method has significant limitations. Excel is not designed to handle large volumes of data or support simultaneous access and modifications by multiple users. In addition, as the number of records grows, performance can degrade significantly, and Excel files have size limitations that can lead to corruption and potential data loss.\nTo address these limitations, we turn to databases, which are purpose-built for efficient data management. Databases offer faster read and write operations, support concurrent access by multiple users, and ensure data integrity.\nAmong the various types of databases, relational databases are the most common due to their intuitive data model. They use a well-defined schema organized in a tabular format with rows and columns, simplifying data organization, querying, and management. Relational databases also support the creation of multiple tables and the establishment of relationships between them. This ability to link data across tables is why they are called “relational.”\nRelational databases are typically accessed and manipulated using SQL (Structured Query Language), a standardized language designed specifically for interacting with relational data. As a result, relational databases are often referred to as SQL databases.\nTo overcome the limitation of data persistence, we will enhance our Flask application by integrating a relational database to store the URLs submitted by users. This means that each time a user registers a URL, it will be saved in an SQL database, ensuring the data is preserved, as illustrated in Figure 1.\nIntegrating a relational database into our application will allow us to leverage the data submitted by users. With this addition, we will use the stored data to identify and showcase the top favorite websites, which we will also refer to as the most popular websites. To achieve this, we will add a new page to the application that retrieves and counts the different registered URLs, displaying the ones registered the most times. This process is schematically illustrated in Figure 2.\nMoreover, since our application will now feature multiple pages, and as you may have already spot in the previous Figure, we will add a navigation bar to facilitate easy navigation between pages.\nIn summary, building on the previous post, we will add three additional features to our web application:"
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#ensuring-data-validity-and-security",
    "href": "posts/2024/connecting-db-app/index.html#ensuring-data-validity-and-security",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Ensuring data validity and security",
    "text": "Ensuring data validity and security\nGiven that we will be storing user-generated data, it is crucial to ensure that the data is both valid and secure before it is saved. This means that users should not be able to save arbitrary or incorrect data in our database. Ensuring data validity involves checking that the data conforms to the desired format and requirements. For example, if users are supposed to enter a URL, we need to validate that the input is indeed in a valid URL format before accepting it.\nIn the previous example, we did not implement any format validation for user input. While our goal was to have users submit URLs, there was no enforcement to ensure that users actually entered a proper URL. As a result, users could input any value or format they wished. For instance, if a user typed “hello” and clicked the “Register URL” button, the application would erroneously confirm that the URL was registered, as illustrated in Figure 3.\n\n\n\n\n\n\nFigure 3: Registering a text entry instead of a URL\n\n\n\n\nCreating a function to validate URLs\nTo address this issue, we need to ensure that only valid URLs are registered. We can achieve this by creating a function that verifies whether the user input is a valid URL. This function will return True if the input is a valid URL and False otherwise. We could name this function is_valid_url, and its behavior can be summarized as follows:\n\ndef is_valid_url(url):\n  # Given a value for the parameter 'url'\n  # Check if it is a valid URL\n  # Return True if it is a valid URL\n  # Return False if it is not a valid URL\n\nThere are several ways to implement this function, including pattern matching or even pinging the website to verify its existence. For our purpose, we will use pattern matching with regular expressions to determine if the input is a valid URL.\n\nValidating the URL using regex\nTo check if the input value is a valid URL, we can use regular expressions (regex). Typically, URLs start with http:// or https://, followed by www.. However, it’s also common for URLs to omit the http:// or https://, beginning instead with just www., or even without the www. prefix. We can specify all these variations in regex through the following steps:\n\nDetecting the protocol (http or https): The initial part of a URL is often the protocol, which is either http or https. In regex, we can use https? to represent this. The s? means that the s is optional, so the URL may start with either http or https. Essentially, the question mark (?) signals that the character before it (in this case, s) may or may not appear in the URL.\nColon and Slashes: Following the protocol (http or https), a URL includes a colon and two slashes (://). In regex, the slash (/) is a special, so to match it literally, we need to escape it with a backslash (\\) . This results in :\\/\\/. Therefore, the regex expression to match the protocol and the following colon and slashes can be written in the following way: https?:\\/\\/.\nMaking the Protocol Optional: As mentioned, https:// or http:// are not mandatory; a URL could start directly with www.. Therefore, we enclose this part in parentheses and add a question mark (?) after it, making the entire segment optional. Our regex expression would now look in the following way: (https?:\\/\\/)?.\nDetecting the www. prefix: After the https:// or http:// part of a URL, we often encounter the www. prefix. In regex, the dot (.) is a special character that represents any character, so to match a literal dot, we need to escape it with a backslash (\\). Thus, www. is written as www\\. in regex. Adding this to our current expression, we get (https?:\\/\\/)?www\\..\nMaking the www. Prefix Optional: The www. part of a URL is not always present, so we need to make it optional. We achieve this by enclosing www\\. in parentheses and adding a question mark (?) after it. This indicates that the www. prefix may or may not appear in the URL. Therefore, our regex pattern becomes (https?:\\/\\/)?(www\\.)?.\n\nFollowing this, we expect the domain name, such as google, facebook, etc. We can specify this with the following steps:\n\nCharacter Set Specification: A domain name can contain lowercase letters (a-z), uppercase letters (A-Z), and even digits (0-9). In regex, we can specify which characters are allowed by placing them inside brackets ([]). Inside these brackets, you can either list specific characters or define ranges with a hyphen (-). For instance, to match any character that is a lowercase letter, an uppercase letter, or a digit, we can use the regex pattern [a-zA-Z0-9]. This pattern covers all characters from a to z, A to Z, and 0 to 9.\nRequiring Multiple Characters: While the expression [a-zA-Z0-9] matches only a single character, a URL often contains more than one such character. To handle this, we use a plus sign (+) after the character set in regex. The plus sign (+) indicates that one or more occurrences of the specified characters are allowed. Thus, [a-zA-Z0-9]+ matches sequences of one or more characters that are either lowercase letters, uppercase letters, or digits.\n\nAfter the domain name, there is always a dot followed by the domain extension, such as .com or .net.\n\nMatching the Domain Extension: To match domain extensions such as .com or .org, we need a regex pattern that identifies a literal dot followed by one or more alphabetic characters. This can be expressed in regex as \\.[a-zA-Z]+. Remember that, in regex, the dot is escaped with a backslash (\\) to ensure it is treated as a literal dot rather than a wildcard.\n\nPutting it all together, the regular expression we will use is ^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$.\nNotice that we’ve added two additional symbols: ^ at the beginning and $ at the end. These symbols ensure that the entire string must match the regular expression from start to finish. Specifically, ^ indicates the start of the string, and $ indicates the end.\n\n\n\n\n\n\nBeware: This regular expression isn’t perfect\n\n\n\nKeep in mind that the regular expression we have just written is not perfect. For example, some URLs may include hyphens in the domain name, subdomains, or other special characters that this basic pattern does not account for. Additionally, this regex does not ensure that the URL actually exists or is reachable; it only checks for a general structure. However, for the sake of this tutorial, this approach is sufficient and serves as a good starting point to highlight how data can be validated.\n\n\n\n\nIntegrating the regex pattern into a Python function\nNow that we have our regex pattern for detecting URLs, we can put it to use in Python to validate user input.\nTo do so, we will leverage the re library, which handles regular expression operations in Python. First, we’ll define our pattern and then use the fullmatch function from the re library to check if the user-provided URL matches the pattern exactly. Note that fullmatch doesn’t return a simple boolean value; instead, it provides a Match object if there’s a match, or None if there isn’t. To convert this result into a boolean, we can use the bool function—bool will return True for any value other than None, and False for None.\nUsing this approach, we can write our function to validate whether user input follows a correct URL format as follows:\n\nimport re  # We need to import the re library\n\ndef is_valid_url(url):\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))  # Checks if the URL matches the pattern\n\n\n\n\n\n\n\nBest practice: Adding type hints to strengthen your Python functions\n\n\n\nWhen creating functions, it’s advisable to specify the types of the arguments and the expected return type. For example, in this case, the parameter url should be a string (str), and the output of this function should be a boolean (bool). We can specify this as follows:\n\nimport re \n\ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\nBy adding type hints, we introduce an additional layer of verification, making our code more robust and specific.\n\n\n\n\n\nModifying the application’s behavior to validate and confirm URLs\nWe have now incorporated a function to check if a user’s input is a valid URL. With this in place, we can adjust the part of the code found in app.py responsible for rendering the page that displays the URL confirmation.\nTo recap, here is the code we built for the main route (/) in the previous post:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        return redirect(url_for('display_url', url=url))\n    else:\n        return render_template('index.html')\n\nThe current code captures the URL submitted by the user through the urlInput form element, which is intended for URL input. However, the code uses the submitted value regardless of its format. As a result, even when an invalid URL (such as “hello”) is entered, as demonstrated in Figure 3, the code still displays a confirmation message.\nFor this reason, we want to enhance this code by adding a validation step to ensure that the submitted URL is valid before doing anything with it. To accomplish this, we will use the is_valid_url() function that we have just created.\nIf is_valid_url returns True, it confirms that the submitted URL is valid, and we will then display a confirmation message to the user, indicating that the URL has been successfully registered. However, if is_valid_url returns False, an error message will be displayed to inform the user that the URL is invalid.\nTo implement this, we can modify our code by adding an if-else statement based on the result of is_valid_url(url). If the URL is valid, we will reuse the code we previously had, i.e., redirect the user to a confirmation page using redirect(url_for('display_url', url=url)). Conversely, if the URL is invalid, we will add a return statement with the following message: \"The URL you entered is not valid. Please check the format and try again.\" In Flask, returning a string generates a plain HTML page containing that message. Therefore, our code would now look as displayed below:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        if is_valid_url(url):\n            return redirect(url_for('display_url', url=url))\n        else:\n            return \"The URL you entered is not valid. Please check the format and try again.\"\n    else:\n        return render_template('index.html')\n\nIn addition, Figure 4 shows the behavior of our application after implementing these changes and entering both an invalid URL (hello) and a valid URL (www.google.com).\n\n\n\n\n\n\nFigure 4: Result of the URL validation submitted by the user\n\n\n\nThis method is quite basic, as it simply shows a plain page with the message “The URL you entered is not valid. Please check the format and try again.” A more effective approach would be to display this message within the confirmation container.\nIf the URL is valid, a confirmation box will appear with the message: “You have successfully registered the following URL: [url].” If the URL is invalid, the confirmation box will instead display: “The URL you entered is not valid. Please check the format and try again.”\nTo implement this update, we need to modify the HTML template (index.html), specifically the section related to the confirmation container, which is identified by the ID confirmationContainer.\nPreviously, we set up the confirmation container as follows:\n{% if url %}\n  &lt;div id=\"confirmationContainer\"&gt;\n        &lt;p&gt;You have registered the following URL: {{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n  &lt;/div&gt;\n{% endif %}\nIn this code, we included a predefined message: “You have registered the following URL:”.\nHowever, we can simplify this by removing the predefined message from the template and passing the entire confirmation text from the backend code, i.e., from our Python code. Therefore, the updated template would look like this:\n{% if url %}\n  &lt;div id=\"confirmationContainer\"&gt;\n    &lt;p&gt;{{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n  &lt;/div&gt;\n  &lt;/div&gt;\n{% endif %}\nNow, there is no predefined text inside the container; the full message is provided by the backend.\nIn addition, we need to update the backend code (app.py) so that it not only sends the URL but also includes a message based on the URL’s validity. If the URL is valid, it should prepend the text “Has entrado la URL:” to the URL. If the URL is invalid, it should display the text “Error URL incorrecta!”:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        if is_valid_url(url):\n            confirmation_message = \"You have successfully registered the following URL: \" + url\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n            return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\nNext, we need to make the same updates to the /display_url/&lt;path:url&gt; route. This route, as you might remember, is where users are redirected to see the confirmation container after registering a URL. Since users can also register new URLs from this route, it’s important to apply these updates here as well. Below is the revised code for this route:\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            if is_valid_url(url2):\n                confirmation_message = \"You have successfully registered the following URL: \" + url2\n                return redirect(url_for('display_url', url=confirmation_message))\n            else:\n                confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n                return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\nAfter implementing these changes, we can run our updated application and test it by entering an invalid URL, such as hello. When we do this, the confirmation container will display the message, “The URL you entered is not valid. Please check the format and try again.”\nAfter seeing this message, we can directly proceed to enter a valid URL, such as www.google.com. This time, the confirmation container will display the message, “You have successfully registered the following URL: www.google.com,” as shown in Figure 5.\n\n\n\n\n\n\nFigure 5: Updated URL validation behavior\n\n\n\n\n\n\n\n\n\nEncapsulating code\n\n\n\nAs our codebase grows, it becomes more complex and harder to read. This often leads to repetitive code in different parts of our application. Recognizing and addressing code repetition allows us to simplify our code by consolidating repetitive logic into functions. This approach, known as encapsulation, helps us avoid duplicating code and makes our application more maintainable. Instead of copying and pasting the same code repeatedly, we can create reusable functions.\nFor instance, the code we’ve added for both routes can be consolidated into a single function, which we might call process_url. This function will handle the URL processing and return the appropriate confirmation text.\nWe can define the process_url function as follows:\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\nBy using this function, we can simplify the code for the home and display_url routes as shown below:\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))"
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#setting-up-our-database",
    "href": "posts/2024/connecting-db-app/index.html#setting-up-our-database",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Setting Up Our Database",
    "text": "Setting Up Our Database\nWhen creating a database, you can choose to set it up on your own hardware locally or use a remote database hosted by a cloud service provider like Azure, AWS, or Google Cloud.\nSetting up a local database involves installing and managing it on your own computer, a dedicated server, or on-premises infrastructure. This method gives you full control over the environment, allowing you to customize settings, manage security, and optimize performance according to your needs.\nThe advantages of a local database include complete customization and no ongoing subscription fees. However, you are responsible for running costs such as electricity, hardware maintenance, and software updates. Additionally, scaling a local database can be costly and challenging, often requiring additional hardware. If your local server experiences downtime, your database’s availability may also be compromised unless you implement high-availability solutions, which can increase complexity and expenses.\nIn contrast, a cloud-based database is hosted on a remote server managed by a cloud service provider. Providers offer a range of options, including SQL and NoSQL databases, allowing you to select the best fit for your needs. Cloud databases simplify management as the provider handles infrastructure, maintenance, and upgrades, letting you focus on utilizing the database.\nCloud databases offer excellent scalability, enabling you to easily adjust resources such as storage and computing power as your needs evolve. They also ensure high availability with built-in redundancy and failover options, and provide global access, which is ideal for distributed teams or global applications. However, cloud databases can be more expensive over time, particularly as your usage grows. They also offer less control over the database environment since the provider manages the infrastructure. Data security in the cloud depends on the provider’s security measures and compliance with relevant regulations. Proper security practices are essential regardless of where the database is hosted.\n\nConfiguring Our Database with Supabase\nFor our purpose, we will use a cloud service, specifically Supabase, https://supabase.com/, which allows us to create dedicated PostgreSQL databases and supporting a free tier. PostgreSQL, also known as Postgres, is an open-source, cross-platform, and highly scalable relational database management system designed to offer advanced features and strong performance in production environments.\n\n\n\n\n\n\nNote\n\n\n\nIf you prefer and are already knowledgeable with other databases or cloud services, you can use alternatives for this tutorial. Whether it’s another managed PostgreSQL provider, a different relational database system, or even a local database setup.\n\n\nTo get started, you need to create a free account on Supabase. Once your account is set up and you’re logged in, you can initiate a new project.\nNavigate to the “Projects” section—this is usually the main screen you see after logging in—and click on “New Project,” as highlighted in red in the Figure 6. By following these steps, you will have access to a dedicated PostgreSQL database in the cloud, allowing you to efficiently manage and scale your database needs.\n\n\n\n\n\n\nFigure 6: “All Projects” page with “New project” button highlighted\n\n\n\nOnce you click this button, a menu will appear, as shown in Figure 7. In this menu, you will need to define several parameters. First, enter a unique name for your project. Next, create and enter a password for your database; make sure to remember or securely store this password, as you will need it later. Finally, select a region that is geographically close to you to ensure optimal performance and reduced latency.\n\n\n\n\n\n\nFigure 7: Project creation menu\n\n\n\nAfter creating a project, you’ll be redirected to the project view, as shown in Figure 8. You might notice a brief delay in being able to perform any actions; this is indicated by the status icon next to the project name, which will show that the project is being set up (highlighted in red in Figure 8). If you encounter any issues with redirection, you can alternatively access the project by going to the “Projects” section and clicking on the specific project.\n\n\n\n\n\n\nFigure 8: Project page after creation\n\n\n\n\n\nRetrieving the database connection information\nOnce the project is set up, you will notice that the project view content updates and a green status indicator appears at the top right corner of the page. This green status means your database is available. Next to this status indicator, you’ll find a “Connect” button, which is highlighted in red in Figure 9.\n\n\n\n\n\n\nFigure 9: Project page after setup is complete\n\n\n\nClicking this button will open a panel displaying the details required to connect to your database. Be sure to select the “Python” type, as highlighted in red in Figure 10, to get the specific connection values needed for Python.\n\n\n\n\n\n\nFigure 10: Connection panel showcasing python connection details\n\n\n\nAs shown in Figure 10, this panel provides a clear step-by-step guide for connecting to your database using Python. It includes the necessary connection parameters: user=[DB_USER], password=[DB_PASSWORD], host=[DB_HOST], port=[DB_PORT], and dbname=[DB_NAME]. The placeholders inside the square brackets ([]) represent where you should enter your specific account details. Be sure to replace these placeholders with your actual information. Remember, these values are sensitive and should be kept confidential to protect access to your database.\nPlease note that the password (DB_PASSWORD) should be the one you set when creating the current project.\n\n\nConnecting our application to our database\nNow that we have the required connection parameters to connect to our database, it’s a good practice to keep these configuration values separate from the main code file. Instead of hardcoding them, you should use a .env file.\nA .env file is a simple text file used to store environment variables and configuration settings outside of your main codebase. It helps keep sensitive information, like database credentials, secure and makes it easier to manage different settings for development, testing, and production environments.\nYou should create this .env file and place it in the root directory of your application. With this setup, the file structure for your application will look like this:\n- my_flask_app/            (Main folder of the application)\n    |\n    |- app.py              (Main file for the Flask application)\n    |\n    |- .env                (File to store environmental variables)\n    |\n    |- templates/          (Folder to store HTML templates)\n    |    |\n    |    |- index.html     (HTML template for the main page)\n    |\n    |- static/             (Folder to store static files such as CSS, JavaScript, images, etc.)\n         |\n         |- styles         (Folder to store style sheets, such as CSS files)\n            |\n            |- home.css    (CSS file to style the main page)\nThe content of this .env file will be as follows:\n\nDB_USER = 'your_username'\nDB_PASSWORD = 'your_password'\nDB_HOST = 'database_host'\nDB_PORT = 'port_number'\nDB_NAME = 'database_name'\n\nSince the database connection parameters are defined in a separate .env file, we need to import them into the backend of our application, specifically into app.py. To achieve this, we will use the os module, which provides functionality for interacting with the operating system. One useful feature of the os module is the os.getenv() method. This method retrieves the value of an environment variable if it exists; otherwise, it returns None or a default value if specified.\nMoreover, to ensure that these environment variables are properly loaded from the .env file, we will utilize the dotenv library and its load_dotenv function. The load_dotenv function reads the .env file and sets the environment variables in the current process.\nWithout load_dotenv, the Python code may not automatically recognize or load the .env file, which could lead to missing or incorrect environment variable values. By explicitly calling load_dotenv, we make sure that these variables are available in the environment before we attempt to retrieve them with os.getenv.\nTherefore, we will make three key updates to our app.py file: (1) we will import the os module and the load_dotenv function from the dotenv library, (2) we will call load_dotenv to ensure the .env file is loaded properly, and (3) we will use the os.getenv method to retrieve the database connection parameters. These additions will look like this in Python:\n\n# Import the os module\nimport os\n# Import load_dotenv from the dotenv library\nfrom dotenv import load_dotenv\n\n# Make clear that env variables are picked from .env file\nload_dotenv()\n\n# Retrieve database connection parameters\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\nNow that we have the necessary values to connect to our database available within our app.py file, we can proceed with establishing the connection. To accomplish this, we’ll use the psycopg library, which facilitates connecting to and managing Postgres databases through Python.\nFirst, we need to import the psycopg library. Our next step is to create a connection to the database using the connect function provided by psycopg. This function will use the connection parameters that we have loaded from the .env file. Here’s how you can set up the connection:\n\n# import psycopg library\nimport psycopg\n\n# Create a connection to our database and store it in a variable called conn\nconn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n\nTo make our code more modular and reusable, we can encapsulate this code within a function. Moreover, we can also add a try-catch statement to handle exceptions that might occur during the connection process and facilitating debugging. You can find the revised implementation below:\n\nimport psycopg\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(e)\n\n\n\nCreating our table\nHaving established the connection to the database, we can proceed to create the table we will be working with. This table only needs to be set up once during the initial setup.\nTo accomplish this, we will add an additional function to the app.py file, which will create the table only if it does not already exist.\nRemember, we are developing a basic application that allows users to register URLs, which helps us track the popularity of these URLs. To support this functionality, we will create a simple table with just two fields: an ID and the URL value.\nHere’s how it will work: Each time a user submits a URL, a new entry will be added to the table, consisting of a unique ID and the URL value. This design keeps the table immutable because each URL is recorded as a separate row without modifying existing entries.\nMaintaining an immutable table is crucial as it ensures the integrity of our data. By avoiding changes to existing records, we prevent data loss or corruption and ensure a reliable and accurate record of all submitted URLs.\n\n\n\n\n\n\nA small note\n\n\n\n\n\nIn this tutorial, we are simplifying the scenario to make it easier to understand compared to more complex real-world situations. This approach helps us focus on how different parts or components of a data-driven application work together in a straightforward way.\nFor example, in this tutorial, our table will only include two columns: an identifier (ID) and a URL value. In a real-world application, however, the table would likely include additional attributes, such as a user ID. This would require implementing a user identification system to ensure that each user can only register a URL once. We might also include other attributes, such as the date, to provide more detailed information.\n\n\n\nTo create a table in our database, we will follow these four essential steps:\n\nEstablish a Connection to the Database: Begin by using the connect_to_database() function to establish a connection to the database.\nCreate a Cursor: Once the connection is established, create a cursor to execute SQL commands. Cursors are temporary objects in SQL that allow us to interact with and manipulate data.\nExecute SQL Commands and Commit Changes: Use the cursor to execute the necessary SQL commands to create and modify the table. Once these commands have been executed, use the commit method to save the changes to the database permanently.\nClose the Database Connection: Finally, once all changes have been made and saved, close the connection to the database.\n\nWe can implement these steps in a function called create_table. The code for this function is provided below:\n\ndef create_table() -&gt; None:\n    try:\n        # Connect to the PostgreSQL database using the connect_to_database function\n        conn = connect_to_database()\n\n        # Create a cursor to execute SQL commands using the cursor method\n        cur = conn.cursor()\n\n        # Execute the command to create the table if it does not already exist\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        # Commit the changes and close the connection\n        conn.commit()\n        # Print statement confirming correct execution\n        print(\"create_table function executed correctly\")\n    except Exception as e:\n        print(e)\n    finally:\n        conn.close()\n\nAs you can see, in the code above, the first step is to establish a connection to the database using the connect_to_database function. We store this connection in a variable called conn, which has a method named cursor. This method allows us to create a cursor for the database, enabling us to execute SQL commands.\nUsing this method, we create a cursor and assign it to a variable named cur. With this cursor, we can execute SQL code using the execute method, which we use to execute the following SQL command:\nCREATE TABLE IF NOT EXISTS registered_urls (\n  id SERIAL PRIMARY KEY,\n  url TEXT\n)\nThis command creates a table in our database named registered_urls, but only if it does not already exist. It specifies that the table will have two columns: id, which will be the primary key and will have a serial value, meaning it will be numeric and automatically increment for each new row inserted; and url, which will be of type text and will store the registered URLs.\nAs previously mentioned, with this table design, we ensure that the values are immutable. This means that once rows are added to the database, they cannot be changed. If a user registers a URL that has already been registered by others, a new row will be created with the same URL but with a different id.\n\n\n\n\n\n\nFinally block\n\n\n\nIn the code above, we use a try-catch statement with a finally block. The try-catch statement helps manage situations where errors might arise while executing code, especially after opening a database connection.\nIf an error occurs within the try block, the code in the catch block will execute, but the connection might not be closed if the finally block is not used.\nTo prevent this, we include a finally block, which ensures that the code within it runs regardless of whether an error occurs. This guarantees that the database connection is always closed properly, even if an exception is thrown.\n\n\nAt this stage, it’s important to determine the most appropriate when and how to run our create_table function. This function is responsible for setting up a database table, but it’s not something that needs to be executed every time we run our Flask application. Running it repeatedly would be inefficient because the table only needs to be created once, or checked occasionally, not on every application start. To avoid unnecessary checks and improve the efficiency of our application, we can create a custom Flask command that allows us to run create_table only when we specifically want to.\nTo accomplish this, Flask provides a convenient way to define custom commands using the @app.cli.command decorator. By attaching this decorator to a function, we can register a new command that can be executed through the Flask command-line interface (CLI). This means that instead of embedding the table creation logic directly into our application’s startup process, we can isolate it in a command that can be manually triggered whenever needed.\nFor instance, we could define a command called init-db-table by using @app.cli.command(\"init-db-table\") to the create_table function as depicted below:\n\n@app.cli.command(\"init-db-table\")\ndef create_table() -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        print(\"create_table function executed correctly\")\n    except Exception as e:\n        print(e)\n    finally:\n        conn.close()\n\nYou can unfold the code below to see how app.py will look after these adjustments.\n\n\napp.py code after adjustments\nfrom flask import Flask, request, render_template, redirect, url_for\nimport os\nfrom dotenv import load_dotenv\nimport psycopg\nimport re \n\nload_dotenv()\n\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(e)\n\ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\napp = Flask(__name__)\n\n@app.cli.command(\"init-db-table\")\ndef create_table() -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        print(\"create_table function executed correctly\")\n    except Exception as e:\n        print(e)\n    finally:\n        conn.close()\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\nOnce we have updated our app.py file, we can move on to creating the registered_urls table. To accomplish this, we need to use the new CLI command: flask init-db-table.\nTo do this, we can open a terminal and navigate to the directory where the app.py file is located. Then, we can execute the command flask init-db-table, as shown below in Figure 11.\n\n\n\n\n\n\nFigure 11: Running the flask init-db-table CLI command\n\n\n\nGreat! The command seems to have executed successfully. Now, we need to ensure that the table has been created correctly. To do this, we will go back to the Supabase project dashboard, where we will select the project we are working on. Once in our project, we will click on the “Table Editor” tab, found in the right-hand panel, as highlighted in red Figure 12.\n\n\n\n\n\n\nFigure 12: Navigating to the table editor in a Supabase project\n\n\n\nAfter clicking on “Table Editor,” you’ll see all the tables created in your database. In this view, you should be able to find the table named registered_urls, which is highlighted in red in Figure 13.\n\n\n\n\n\n\nFigure 13: Verifying the table creation in Supabase\n\n\n\nAdditionally, we can click on the table registered_urls to see its contents. Doing so will reveal that the table has two empty columns: id and url. Each column is also labeled with its data type—int4 for id and text for url—as shown in Figure 14.\n\n\n\n\n\n\nFigure 14: Checking the registered_urls table\n\n\n\nGreat! We have successfully created the table and are now ready to start adding values to it!"
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#storing-submitted-urls-in-our-database",
    "href": "posts/2024/connecting-db-app/index.html#storing-submitted-urls-in-our-database",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Storing submitted URLs in our database",
    "text": "Storing submitted URLs in our database\nNow that we have established a connection with our database and created a table called registered_urls, we can store each URL that a user registers. To achieve this, we will create a new function that stores a URL in our database whenever a user registers one.\nWe can call this function insert_url_into_database(). This function will operate similarly to the previously created function connect_to_database. Specifically, it will:\n\nEstablish a connection to the database.\nCreate a cursor.\nUse the cursor to execute an SQL command to insert the URL into the registered_urls table. Here is how we can implement this using a simple SQL INSERT statement:\nINSERT INTO registered_urls (url) VALUES (%s)\", (url,)\nThis SQL query inserts a URL into the registered_urls table by specifying the url column and using a placeholder %s to safely insert the value provided by the url variable.\nNotice that we do not need to specify the value for the id column. This is because we defined the id column with the SERIAL property when we created the table, which automatically generates a unique identifier for each new row.\nFinally, we need to confirm the changes using the commit method and close the connection.\n\nTherefore, the implementation of the insert_url_into_database function in Python will look as follows:\n\ndef insert_url_into_database(url: str) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"INSERT INTO registered_urls (url) VALUES (%s)\", (url,))\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting URL into the database:\", e)\n    finally:\n        conn.close()\n\nThe next step is to execute this function every time a valid URL has been submitted. To achieve this, we can modify the process_url function, which, as you may recall, validates whether a URL is valid and generates the confirmation message to be displayed. Currently, this function looks like this:\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\nAll we need to do now is call our new insert_url_into_database function within the if statement. In other words, if the URL is valid, we will store it in our database. This will modify the function as follows:\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        insert_url_into_database(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\nTo view the updated app.py after these changes, unfold the code below:\n\n\napp.py code after adjustments\nfrom flask import Flask, request, render_template, redirect, url_for\nimport os\nfrom dotenv import load_dotenv\nimport psycopg\nimport re\n\nload_dotenv()\n\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(e)\n        \ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\ndef insert_url_into_database(url: str) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"INSERT INTO registered_urls (url) VALUES (%s)\", (url,))\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting URL into the database:\", e)\n    finally:\n        conn.close()\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        insert_url_into_database(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\n\napp = Flask(__name__)\n\n@app.cli.command(\"init-db-table\")\ndef create_table() -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        print(\"create_table function executed correctly\")\n    except Exception as e:\n        print(e)\n    finally:\n        conn.close()\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\nWith our code now updated, we can proceed to run our Flask application. At this point, we should verify that URLs registered through the application are correctly stored in the database.\nTo verify the successful addition of a URL to our dabatase, we can use the Table Editor view in Supabase. First, we should navigate to the Table Editor for our project and locate the registered_urls table, as shown in Figure 13.\nNext, open a separate tab with our running application and register a valid URL. Once you receive confirmation that the URL has been successfully registered, return to the Table Editor view to check that the new URL appears correctly in the registered_urls table, as shown in Figure 15. In this way, we can confirm that our application is correctly registering URLs into the registered_urls table.\n\n\n\n\n\n\nFigure 15: Verifying that the URLs registered in our application are correctly stored in our database.\n\n\n\nGreat! Our application is starting to become functional. Users can register URLs, and these URLs are stored in our database, which allows us to access and manage this information.\nNext, we can move forward by creating a new feature: a page that displays the top favorite websites to our users. This page will present a list of websites ranked by their registration count, showing which websites have been registered the most."
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#creating-a-new-page-top-favorite-websites",
    "href": "posts/2024/connecting-db-app/index.html#creating-a-new-page-top-favorite-websites",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Creating a new page: Top favorite websites",
    "text": "Creating a new page: Top favorite websites\nAfter updating our application to store submitted URLs and verifying their correct storage in our database, our goal is to leverage this data. We aim to identify the top favorite websites—those registered most frequently—and present them to our users on a dedicated page.\nTo achieve this, we will need to take two main steps:\n\nExtracting URL counts: We will count the frequency of each unique URL in our database, allowing us to identify the most frequently submitted URLs.\nCreating a page to display top favorite websites: We will then create a page that dynamically populates a table containing the most popular websites (URLs) and their counts.\n\n\nExtracting URL counts\nLet’s delve into the first step: determining the popularity of each URL by counting how often each URL appears in our database.\nTo achieve this, we will use an SQL query that groups identical URLs, counts their occurrences, and sorts the results in descending order of frequency. To focus on the most popular URLs, we will also limit the results to the top 10. Here’s the SQL query for this process:\nSELECT url, COUNT(*) as count \nFROM registered_urls \nGROUP BY url \nORDER BY count DESC \nLIMIT 10;\nHaving created this query, we want to execute it through our backend to enable our application to retrieve the relevant information from the database. To accomplish this, and in line with our previous database interactions, we will create a function, which we will call get_top_registered_urls(). This function will connect to the database, execute the previous SQL query, and return the results. The code for this function is shown below:\n\ndef get_top_registered_urls() -&gt; List[Tuple[str, int]] | None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"\"\"\n            SELECT url, COUNT(*) as count\n            FROM registered_urls\n            GROUP BY url\n            ORDER BY count DESC\n            LIMIT 10;\n        \"\"\")\n        top_registered_urls = cur.fetchall()\n        return top_registered_urls\n    except Exception as e:\n        print(f\"Error retrieving the URLs: {e}\")\n    finally:\n        if conn:\n            conn.close()\n\nIn this function, you will notice that, compared to the other functions we have created to interact with our database, we are not using the commit method. This is because we are not modifying any data in the database; we are merely retrieving it. The commit method is only necessary when making changes to the database, such as inserting or updating records.\nInstead, we use the fetchall method, which retrieves all rows resulting from the query. This method returns a list of tuples, with each tuple representing a row from the query result. Each tuple contains the URL and its count, showing how many times that URL appears in the database.\nFor example, the result from fetchall might look like this: [('www.facebook.com', 1), ('www.google.com', 1)]. This list provides each URL along with its count of occurrences. In this case, the output indicates that there are two URLs in the table—www.facebook.com and www.google.com—each with a registration count of 1.\nWe are now ready to create a new page in our application to present this data to users in an appealing and user-friendly format.\n\n\nCreating a page to display the top favorite websites\nTo display the information retrieved from the get_top_registered_urls function to users, we will need to set up a new page using an HTML template. This involves creating a new file named popular.html in the templates directory.\nWe will begin by copying the contents from index.html into this new file. However, unlike index.html, the popular.html template will not contain the form elements for users to submit URLs, such as the text field and button, nor will it have the confirmation container.\nThe primary focus of this new page will be to present a table of popular URLs along with their counts, without any additional functionalities. Therefore, the starting point for this file will look like this:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='styles/popular.css') }}\"&gt;&lt;!-- Replaced home.css file to popular.css --&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"content\"&gt;\n        &lt;div class=\"table-container\"&gt;&lt;!-- Replaced container class to table-container --&gt;\n            &lt;h1&gt;Top Favorite Websites&lt;/h1&gt; &lt;!-- Updated the title --&gt;\n            &lt;!-- Removed all sections related to the form and the confirmation container --&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNote that in the previous code, two additional changes have been made: the link to the CSS file has been updated to a new file specifically for this page, named popular.css, and the class name of the div containing the table has been changed to table-container.\nTo present the information retrieved by get_top_registered_urls on this page, we need to consider the best way to display it. This function returns a list of tuples, with each tuple containing a URL and its registration count. To clearly present this data, the best approach is to use a table format. In this table, each row will represent a different URL, with the URL displayed in the first column and its registration count in the second.\nSince this data is dynamically generated by our Python backend (app.py), we can use Flask’s templating system to render it. The frontend will receive a list of tuples from the backend, with each tuple containing information about a website. Each tuple can then be used to populate a row in an HTML table, using each entry to populate the columns.\nSpecifically, we can use a for loop in the template to iterate over the information retrieved by get_top_registered_urls, which we will call top_registered_urls. For each tuple in top_registered_urls, we extract its values to populate the table columns: the first value renders the URL in the URL column, and the second value renders the count in the tracking count column.\nHere is how we can implement this logic into our template:\n&lt;table&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;!-- Column headers --&gt;\n      &lt;th&gt;URL&lt;/th&gt; &lt;!-- Column for the URL --&gt;\n      &lt;th&gt;Tracking Count&lt;/th&gt; &lt;!-- Column for the tracking count --&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;!-- Loop through each item in the list --&gt;\n    {% for top_url in top_registered_urls %}\n      &lt;tr&gt;\n        &lt;!-- Data cell for the URL --&gt;\n        &lt;td&gt;{{ top_url[0] }}&lt;/td&gt; &lt;!-- Displays the URL of the current item in top_registered_urls, i.e., the first entry of the tuple --&gt;\n        &lt;!-- Data cell for the tracking count --&gt;\n        &lt;td&gt;{{ top_url[1] }}&lt;/td&gt; &lt;!-- Displays the tracking count of the current item in top_registered_urls, i.e., the second entry of the tuple --&gt;\n      &lt;/tr&gt;\n    {% endfor %}\n  &lt;/tbody&gt;\n&lt;/table&gt;\nWe can integrate this code into our popular.html file, which will now contain the following content:\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='styles/popular.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"content\"&gt;\n        &lt;div class=\"table-container\"&gt;\n            &lt;h1&gt;Top Favorite Websites&lt;/h1&gt;\n            &lt;table&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt;URL&lt;/th&gt;\n      &lt;th&gt;Tracking Count&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    {% for top_url in top_registered_urls %}\n      &lt;tr&gt;\n        &lt;td&gt;{{ top_url[0] }}&lt;/td&gt;\n        &lt;td&gt;{{ top_url[1] }}&lt;/td&gt; \n      &lt;/tr&gt;\n    {% endfor %}\n  &lt;/tbody&gt;\n&lt;/table&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nThe next step is to create the popular.css file. As with the HTML file for this page, we will start with the existing CSS from the main page, home.css, and make the necessary adjustments to reflect the updates in popular.html.\nSince the popular.html file no longer contains the container class, form elements, or the confirmation container, we need to remove all associated classes and IDs from popular.css. Instead, we will introduce new styles specifically for the elements within the table-container.\nOur goal is to make the table within the table-container both attractive and easy to read. To achieve this, we will center the table’s title to ensure the header information is well-organized. The table will be set to occupy 50% of the viewport width, which will provide a balanced and visually pleasing layout. We will also merge the table borders into a single continuous line and center the table horizontally for a cleaner look.\nTo enhance readability, we will add generous padding inside the table cells and use a subtle border to clearly define each cell. The text within the cells will be left-aligned for consistency, while header cells will feature a dark green background with white text to make them stand out.\nAdditionally, we will enhance user interaction by changing the background color of table rows to a slightly darker shade of gray when hovered over. This subtle color shift will provide a clear visual cue and make the table easier to navigate.\nBelow is the new popular.css, implementing all these changes:\nbody {\n    font-family: Arial, sans-serif;\n    margin: 0;\n    padding: 0;\n    height: 100vh; /* Ensure the body takes up the full height of the viewport */\n}\n\n.content {\n    display: flex;\n    align-items: center;\n    padding-top: 5vh;  /* Adds padding at the top to avoid overlap with the navbar we will create */\n    height: 90.5vh;   /* Sets the height of the container to 90.5% of the viewport height, i.e., area remaining after navbar and top padding */\n    background-color: #f0f0f0; /* Light gray background color */\n    flex-direction: column; /* Arrange children elements in a column */\n    justify-content: center; /* Horizontally centers the content */\n}\n\n/* CSS styling for the table format */\n.table-container h1 {\n    text-align: center; /* Center-aligns the table header */\n}\n\n.table-container table {\n    width: 50vw; /* Set table width to 50% of the viewport width */\n    border-collapse: collapse; /* Collapse borders for a cleaner look */\n    margin: auto; /* Center the table horizontally */\n}\n\n.table-container th,\n.table-container td {\n    padding: 12px; /* Add padding for better readability */\n    border: 1px solid #ddd; /* Light gray border for table cells */\n    text-align: left; /* Left-align text in table cells */\n}\n\n.table-container th {\n    background-color: #4CAF50; /* Dark green background for header cells */\n    color: white; /* White text color for header cells */\n}\n\n.table-container tr:nth-child(even) {\n    background-color: #f2f2f2; /* Light gray background for even rows */\n}\n\n.table-container tr:hover {\n    background-color: #ddd; /* Change table row background color on hover for better visibility */\n}\nWith all these changes, our file structure will now look like this:\n- my_flask_app/            (Main folder of the application)\n    |\n    |- app.py              (Main file for the Flask application)\n    |\n    |- .env                (File to store environmental variables)\n    |\n    |- templates/          (Folder to store HTML templates)\n    |    |\n    |    |- index.html     (HTML template for the main page)\n    |    |\n    |    |- popular.html   (HTML template for the top favorite websites page)\n    |\n    |- static/             (Folder to store static files such as CSS, JavaScript, images, etc.)\n         |\n         |- styles         (Folder to store style sheets, such as CSS files)\n            |\n            |- home.css    (CSS file to style the main page)\n            |\n            |- popular.css (CSS file to style the top favorite websites page)\nAlmost everything is set up. We are now retrieving the most frequently registered websites from the database—essentially, the most popular sites. We have created a new page to display these websites along with their counts. All that remains is to pass the retrieved data from the backend to the frontend so it can be rendered on this page.\n\n\nPopulating the top favorite websites table\nTo define the behavior for the popular page in our application, we need to start by specifying its route. For instance, we can designate the URL path /popular for this page. In our app.py file, we will then outline the necessary steps for handling requests to this route. Specifically,we need to implement the following steps:\n\nRetrieve the unique URLs and their counts by executing the get_top_registered_urls function.\nRender the popular.html page using the data obtained from get_top_registered_urls to populate the table.\n\nTo accomplish this, we will use similar approach to what we have employed for other routes in our application. In app.py, we will define a route handler function for the /popular route. This function will first gather the necessary data and then render the popular.html template, incorporating the retrieved data into the page. Consequently, the code for this route handler will be as follows:\n\n@app.route(\"/popular\", methods=['GET'])\ndef top_favorite_websites_page():\n    top_registered_urls = get_top_registered_urls()\n    return render_template('popular.html', top_registered_urls=top_registered_urls)\n\nYou can unfold the code below to view the updated app.py:\n\n\napp.py code after adjustments\nfrom flask import Flask, request, render_template, redirect, url_for\nimport os\nfrom dotenv import load_dotenv\nimport psycopg\nimport re \n\nload_dotenv()\n\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(e)\n\ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\ndef insert_url_into_database(url: str) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"INSERT INTO registered_urls (url) VALUES (%s)\", (url,))\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting URL into the database:\", e)\n    finally:\n        conn.close()\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        insert_url_into_database(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\ndef get_top_registered_urls() -&gt; List[Tuple[str, int]] | None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"\"\"\n            SELECT url, COUNT(*) as count\n            FROM registered_urls\n            GROUP BY url\n            ORDER BY count DESC\n            LIMIT 10;\n        \"\"\")\n        top_registered_urls = cur.fetchall()\n        return top_registered_urls\n    except Exception as e:\n        print(f\"Error retrieving the URLs: {e}\")\n    finally:\n        if conn:\n            conn.close()\n\napp = Flask(__name__)\n\n@app.cli.command(\"init-db-table\")\ndef create_table() -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        print(\"create_table function executed correctly\")\n    except Exception as e:\n        print(e)\n    finally:\n        conn.close()\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\n@app.route(\"/popular\", methods=['GET'])\ndef top_favorite_websites_page():\n    top_registered_urls = get_top_registered_urls()\n    return render_template('popular.html', top_registered_urls=top_registered_urls)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\nAfter implementing the changes to retrieve and display the top favorite websites in the newly created route, let’s check how these modifications appear in our application.\nTo do so, first restart the application. Open a terminal or command prompt, navigate to the directory containing our Flask application, and execute the command flask run. Alternatively, you can run the application in debug mode by executing the script directly with python app.py or python3 app.py, depending on your Python version.\nOnce the application is running, open a web browser and navigate to http://127.0.0.1:5000/. To view the new route, append /popular to the end of the URL, so it becomes http://127.0.0.1:5000/popular. If your application is hosted on a different IP address or port, use the address displayed in your terminal or command prompt, followed by /popular.\nOn this route, you will see the page we just created displaying a table with the most popular URLs, as shown in Figure 16.\n\n\n\n\n\n\nFigure 16: View of the top favorite websites page\n\n\n\nAmazing! We can see that the table of the top favorite websites is displayed correctly. Currently, we only see two websites, www.google.com and www.facebook.com, each with a registration count of 1. This is because these are the only websites we’ve added so far.\nIf you have registered additional URLs or logged these URLs more times, you would see different values reflecting those changes.\nHowever, you might have noticed a potential issue—accessing the popular URLs page is not very intuitive. We had to manually modify the URL by adding /popular to reach this page, which can make it difficult for users to find. Additionally, users might not be aware that this page exists, so they may never visit it.\nThe simplest solution to this problem is to create a navigation bar. By incorporating a navigation bar into our application, we can make it much easier for users to access various pages, including the popular URLs page."
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#creating-a-navigation-bar",
    "href": "posts/2024/connecting-db-app/index.html#creating-a-navigation-bar",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Creating a navigation bar",
    "text": "Creating a navigation bar\nAs we have just noticed, navigating through the pages of our application is not very straightforward. Whenever we develop applications, we must prioritize user experience (UX), ensuring that users find it comfortable and easy to interact with and use our application. In this case, to improve navigation between pages, what we need is a navigation bar.\nWe will design this navigation bar to blend seamlessly with our existing theme. Specifically, we will use the same green accent color that characterizes our design to make the navigation bar stand out against the gray background of the pages. The navigation bar will feature the different pages of our application—such as the home page and the top favorite websites—displayed on the right side using white text.\nTo enhance usability, the currently active tab will be highlighted with a slightly brighter shade of green. This will provide users with clear visual feedback about their location within the application. This approach not only aims to improve navigation but also ensures consistency with our overall design aesthetic. Figure 17 visually illustrates the envisioned design for the navigation bar.\n\n\n\n\n\n\nFigure 17: Schematic design for the navigation bar\n\n\n\nTo begin creating our navigation bar, we will start with HTML. Creating navigation bars with HTML is quite simple. We just need to use the &lt;nav&gt; tag to designate a section of our webpage specifically for navigation links. Within this &lt;nav&gt; section, we typically include an unordered list (&lt;ul&gt;) that contains list items (&lt;li&gt;) with hyperlinks (&lt;a&gt;) pointing to the various pages or sections of the site. For example, we could use the following HTML to create a navigation list containing the home and the top favorite websites (popular) pages:\n&lt;nav&gt;\n  &lt;!-- We create an unordered list: ul --&gt;\n  &lt;ul&gt;\n      &lt;!-- We add the routes/links as elements of this list --&gt;\n      &lt;li&gt;&lt;a href=\"/\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\"/popular\"&gt;Popular&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\nWe can save the HTML for this navbar in a file named navbar.html.\nNext, we will style the navbar we just created by adding a CSS file that will set its background to green and ensure it stretches across the top of the page. The items within the navbar will be aligned to the right and centered vertically. We’ll also add subtle shadowing to give the navbar a slightly lifted appearance. The links will be styled in white and will change color when hovered over to enhance user interaction.\n/* Styles for the navigation bar */\nnav {\n    background-color: #4CAF50; /* Light green background color */\n    color: #fff; /* White text color */\n    height: 4.5%; /* Height of the navigation bar */\n    display: flex; /* Use flexbox for layout */\n    justify-content: flex-end; /* Align items to the right edge */\n    align-items: center; /* Center items vertically */\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Add subtle shadow for a lifted effect */\n    padding-right: 2%; /* Add padding to the right side */\n}\n\nnav ul {\n    margin: 0; /* Remove default margin */\n    padding: 0; /* Remove default padding */\n    list-style-type: none; /* Remove default list bullets */\n    display: flex; /* Use flexbox for better alignment */\n}\n\nnav ul li {\n    margin-left: 20px; /* Add space between list items */\n}\n\nnav ul li:first-child {\n    margin-left: 0; /* Remove left margin for the first item */\n}\n\nnav ul li a {\n    padding: 12.5px 20px; /* Adjust padding for better spacing */\n    color: #fff; /* White text color for links */\n    text-decoration: none; /* Remove underline from links */\n    transition: background-color 0.3s; /* Smooth transition for background color on hover */\n}\n\nnav ul li a:hover {\n    background-color: #3e8e41; /* Change background color on hover */\n}\nWe can save this CSS code in a file named navbar.css. Once we have created this stylesheet, we can link it to our navbar.html file by adding the following line of code within the &lt;head&gt; section:\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='styles/navbar.css') }}\"&gt;\nThis will apply the styling rules from navbar.css to the navbar in our HTML file, ensuring the design enhancements we have specified are implemented.\n\nIncorporating the navigation bar into our templates\nAt this point, a question arises: how can we integrate the navigation bar we have just created into our templates? Jinja2 templates allow us to reuse parts of HTML code, similar to how we reuse functions in Python.\nBy applying this principle, we can not only add our navigation bar to different pages but also make our templates more elegant and maintainable. For example, both the homepage (index.html) and the top favorite websites page (popular.html) share several common elements. To avoid repetition and streamline our code, we can create a base template, which we’ll call base_template.html.\nThis base template will serve as a foundation for all our pages, allowing us to define shared components like the navigation bar, footer, or any other recurring elements in one place. Individual pages can then extend this base template, adding or overriding specific content as needed. This approach not only improves code organization but also makes updates easier—if you need to change the navigation bar, you only need to do it once in the base template, and the changes will be reflected across all pages that use it.\nIn our case, we could create the base_template.html using the following code:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/base_template.css')}}\"&gt;\n    {% block styles %}{% endblock %}\n&lt;/head&gt;\n&lt;body&gt;\n  {% include 'navbar.html' %}\n  &lt;div class=\"content\"&gt;\n    {% block content %}{% endblock %}\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nIn this template, we use several Jinja2 features, including blocks and includes, to structure our code effectively.\nBlocks allow child templates to insert or override specific content. They are defined using the {% block %} syntax and serve as content placeholders, allowing child templates to override or add content to these sections. For example, the &lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt; block allows child templates of base_template.html to define their own titles.\nAnother example is the {% block styles %}{% endblock %} block, which is used for adding custom styles. In this way, child templates can add their own CSS files within this block. Similarly, the {% block content %}{% endblock %} block acts as a placeholder for the main content of any page that extends this template.\nIncludes enable the reuse of common components, like a navigation bar, across multiple pages. Basically, they are used to insert content from one template into another. They are defined using the {% include %} statement. For example, in our template, we use {% include 'navbar.html' %} to insert the navigation bar from navbar.html. This promotes reusability and maintainability. If the navigation bar needs updating, you only need to modify the navbar.html file, and the changes will be reflected across all pages that extend this template or include the navigation bar from navbar.html.\nAdditionally, we have linked a CSS file named base_template.css to this template, in order to establish a consistent look and feel across all our pages. This file will include the shared styles that are common to all pages, ensuring uniformity in elements such as layout, typography, and color schemes throughout the site. Below, you can find the code for this CSS file:\nbody {\n    font-family: Arial, sans-serif;\n    margin: 0;\n    padding: 0;\n    height: 100vh;\n}\n\n.content {\n    display: flex;\n    align-items: center;\n    padding-top: 5vh;  /* Adds padding at the top to avoid overlap with the navbar */\n    height: 90.5vh;   /* Sets the height of the container to 90.5% of the viewport height, i.e., area remaining after navbar and top padding */\n    background-color: #f0f0f0;\n    flex-direction: column;\n    justify-content: center; /* Horizontally centers the content */\n}\nWe can now rewrite our two main templates—the homepage (index.html) and the popular URLs page (popular.html)—as children of the base template, meaning that these templates will inherit the structure and layout defined in base_template.html, simplifying their code.\nHere’s how the index.html template will look after using the base template:\n{% extends \"base_template.html\" %}\n\n{% block title %}Home{% endblock %}\n\n{% block content %}\n    &lt;div class=\"container\"&gt;\n         &lt;h1&gt;What is your favorite website?&lt;/h1&gt;\n          &lt;form method=\"post\"&gt;  &lt;!-- Specify method=\"post\" for form submission --&gt;\n              &lt;input type=\"text\" name=\"urlInput\" placeholder=\"Enter URL\" class=\"form-text-input\" required&gt;\n              &lt;br&gt;\n              &lt;button type=\"submit\" class=\"submit-btn\"&gt;Register URL&lt;/button&gt;\n          &lt;/form&gt;\n          {% if url %}\n            &lt;div id=\"confirmationContainer\"&gt;\n                &lt;p&gt;{{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n            &lt;/div&gt;\n          {% endif %}\n      &lt;/div&gt;\n{% endblock %}\n\n{% block styles %}\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/home.css')}}\"&gt;\n{% endblock %}\nIn this updated version of the index.html template, we have removed all the elements already included in base_template.html and added the {% extends \"base_template.html\" %} statement to indicate that this template inherits from the base template.\nThe {% block title %}Home{% endblock %} block sets the title for this page, which will appear in the browser tab.\nThe {% block content %}{% endblock %} section specifies the HTML content to be placed within the main content area defined by the base template. This includes the main elements for the homepage, such as the URL checking form.\nLastly, the {% block styles %}{% endblock %} block is used to include an additional CSS file, home.css, to apply specific styles to this page.\nWe then update home.css by removing any styles that are already covered in base_template.css. The updated home.css will look like this:\n.container {\n    text-align: center; /* Centers the text inside the container */\n    margin-bottom: 20px; /* Adds bottom margin to separate the form from the result */\n}\n\n.form-text-input {\n    width: 300px; /* Sets the width of the URL input field */\n    padding: 10px; /* Adds padding around the input field */\n    font-size: 16px; /* Sets the font size of the input field */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the input field */\n    margin-bottom: 20px; /* Adds bottom margin to separate the input field from the button */\n}\n\n.submit-btn {\n    background-color: #4CAF50; /* Sets the background color of the submit button */\n    color: white; /* Sets the text color of the submit button */\n    padding: 10px 20px; /* Adds padding around the button text */\n    font-size: 16px; /* Sets the font size of the button text */\n    border: none; /* Removes the border from the submit button */\n    border-radius: 5px; /* Applies rounded corners to the submit button */\n    cursor: pointer; /* Changes the cursor to a pointer when hovering over the button */\n    transition: background-color 0.3s; /* Adds a smooth transition to the button's background color */\n}\n\n.submit-btn:hover {\n    background-color: #3e8e41; /* Changes the button's background color when hovering over it */\n}\n\n#confirmationContainer {\n    padding: 20px; /* Adds padding around the content of the result container */\n    background-color: #f0eaea; /* Sets the background color of the result container */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the result container */\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Adds a shadow effect to the result container */\n    margin-top: 20px; /* Adds top margin to visually separate the confirmation container from the form */\n}\nSimilarly, for the top favorite websites page (popular.html and popular.css), we follow the same approach: we remove the elements already defined in the base template and extend it. You will find the code for these files in the “Final Code” section (link to the updated code for popular.html and popular.css).\nWith these changes, our application will look as shown in Figure 18, and as you can see, navigating between different pages is much easier and more convenient!\n\n\n\n\n\n\nFigure 18: Using the application with the new navigation bar"
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#final-code",
    "href": "posts/2024/connecting-db-app/index.html#final-code",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Final code",
    "text": "Final code\nYou can unfold the section below to see the complete, updated content for each file of our Flask application.\n\n\nUpdated file structure and files content\n\n\nFinal file structure\n- my_flask_app/                    (Main folder of the application)\n    |\n    |- app.py                      (Main file for the Flask application)\n    |\n    |- .env                        (File to store environmental variables)\n    |\n    |- templates/                  (Folder to store HTML templates)\n    |    |\n    |    |- base_template.html     (HTML template for the base template)\n    |    |\n    |    |- index.html             (HTML template for the main page)\n    |    |\n    |    |- navbar.html            (HTML template for the navigation bar)\n    |    |\n    |    |- popular.html           (HTML template for the top favorite websites page)\n    |\n    |- static/                     (Folder to store static files such as CSS, JavaScript, images, etc.)\n         |\n         |- styles                 (Folder to store style sheets, such as CSS files)\n            |\n            |- base_template.css   (CSS file to style the base template elements)\n            |\n            |- home.css            (CSS file to style the main page)\n            |\n            |- navbar.css          (CSS file to style the navigation)\n            |\n            |- popular.css         (CSS file to style the top favorite websites page)\n\n\nPython - app.py\n\nfrom flask import Flask, request, render_template, redirect, url_for\nimport os\nfrom dotenv import load_dotenv\nimport psycopg\nimport re \n\nload_dotenv()\n\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(e)\n\ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\ndef insert_url_into_database(url: str) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"INSERT INTO registered_urls (url) VALUES (%s)\", (url,))\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting URL into the database:\", e)\n    finally:\n        conn.close()\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        insert_url_into_database(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\ndef get_top_registered_urls() -&gt; List[Tuple[str, int]] | None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"\"\"\n            SELECT url, COUNT(*) as count\n            FROM registered_urls\n            GROUP BY url\n            ORDER BY count DESC\n            LIMIT 10;\n        \"\"\")\n        top_registered_urls = cur.fetchall()\n        return top_registered_urls\n    except Exception as e:\n        print(f\"Error retrieving the URLs: {e}\")\n    finally:\n        if conn:\n            conn.close()\n\napp = Flask(__name__)\n\n@app.cli.command(\"init-db-table\")\ndef create_table() -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        print(\"create_table function executed correctly\")\n    except Exception as e:\n        print(e)\n    finally:\n        conn.close()\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\n@app.route(\"/popular\", methods=['GET'])\ndef top_favorite_websites_page():\n    top_registered_urls = get_top_registered_urls()\n    return render_template('popular.html', top_registered_urls=top_registered_urls)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\n\nEnvironmental variables file - .env\n\nDB_USER = 'your_username'\nDB_PASSWORD = 'your_password'\nDB_HOST = 'database_host'\nDB_PORT = 'port_number'\nDB_NAME = 'database_name'\n\n\n\nBase Template\n\nHTML - base_template.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;{% block title %}{% endblock %}&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/base_template.css')}}\"&gt;\n    {% block styles %}{% endblock %}\n&lt;/head&gt;\n&lt;body&gt;\n  {% include 'navbar.html' %}\n  &lt;div class=\"content\"&gt;\n    {% block content %}{% endblock %}\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nCSS - base_template.css\nbody {\n    font-family: Arial, sans-serif;\n    margin: 0;\n    padding: 0;\n    height: 100vh;\n}\n\n.content {\n    display: flex;\n    align-items: center;\n    padding-top: 5vh;  /* Adds padding at the top to avoid overlap with the navbar */\n    height: 90.5vh;   /* Sets the height of the container to 90.5% of the viewport height, i.e., area remaining after navbar and top padding */\n    background-color: #f0f0f0;\n    flex-direction: column;\n    justify-content: center; /* Horizontally centers the content */\n}\n\n\n\nNavigation bar\n\nHTML - navbar.html\n&lt;nav&gt;\n  &lt;ul&gt;\n      &lt;li&gt;&lt;a href=\"/\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n      &lt;li&gt;&lt;a href=\"/popular\"&gt;Popular&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/navbar.css')}}\"&gt;\n\n\nCSS - navbar.css\n/* Styles for the navigation bar */\nnav {\n    background-color: #4CAF50; /* Light green background color */\n    color: #fff; /* White text color */\n    height: 4.5%; /* Height of the navigation bar */\n    display: flex; /* Use flexbox for layout */\n    justify-content: flex-end; /* Align items to the right edge */\n    align-items: center; /* Center items vertically */\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Add subtle shadow for a lifted effect */\n    padding-right: 2%; /* Add padding to the right side */\n}\n\nnav ul {\n    margin: 0; /* Remove default margin */\n    padding: 0; /* Remove default padding */\n    list-style-type: none; /* Remove default list bullets */\n    display: flex; /* Use flexbox for better alignment */\n}\n\nnav ul li {\n    margin-left: 20px; /* Add space between list items */\n}\n\nnav ul li:first-child {\n    margin-left: 0; /* Remove left margin for the first item */\n}\n\nnav ul li a {\n    padding: 12.5px 20px; /* Adjust padding for better spacing */\n    color: #fff; /* White text color for links */\n    text-decoration: none; /* Remove underline from links */\n    transition: background-color 0.3s; /* Smooth transition for background color on hover */\n}\n\nnav ul li a:hover {\n    background-color: #3e8e41; /* Change background color on hover */\n}\n\n\n\nHome page\n\nHTML - index.html\n{% extends \"base_template.html\" %}\n\n{% block title %}Home{% endblock %}\n\n{% block content %}\n    &lt;div class=\"container\"&gt;\n         &lt;h1&gt;What is your favorite website?&lt;/h1&gt;\n          &lt;form method=\"post\"&gt;  &lt;!-- Specify method=\"post\" for form submission --&gt;\n              &lt;input type=\"text\" name=\"urlInput\" placeholder=\"Enter URL\" class=\"form-text-input\" required&gt;\n              &lt;br&gt;\n              &lt;button type=\"submit\" class=\"submit-btn\"&gt;Register URL&lt;/button&gt;\n          &lt;/form&gt;\n          \n          {% if url %}\n            &lt;div id=\"confirmationContainer\"&gt;\n                &lt;p&gt;{{ url }}&lt;/p&gt;  &lt;!-- Display URL registration confirmation --&gt;\n            &lt;/div&gt;\n          {% endif %}\n      &lt;/div&gt;\n{% endblock %}\n\n{% block styles %}\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/home.css')}}\"&gt;\n{% endblock %}\n\n\nCSS - home.css\n.container {\n    text-align: center; /* Centers the text inside the container */\n    margin-bottom: 20px; /* Adds bottom margin to separate the form from the result */\n}\n\n.form-text-input {\n    width: 300px; /* Sets the width of the URL input field */\n    padding: 10px; /* Adds padding around the input field */\n    font-size: 16px; /* Sets the font size of the input field */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the input field */\n    margin-bottom: 20px; /* Adds bottom margin to separate the input field from the button */\n}\n\n.submit-btn {\n    background-color: #4CAF50; /* Sets the background color of the submit button */\n    color: white; /* Sets the text color of the submit button */\n    padding: 10px 20px; /* Adds padding around the button text */\n    font-size: 16px; /* Sets the font size of the button text */\n    border: none; /* Removes the border from the submit button */\n    border-radius: 5px; /* Applies rounded corners to the submit button */\n    cursor: pointer; /* Changes the cursor to a pointer when hovering over the button */\n    transition: background-color 0.3s; /* Adds a smooth transition to the button's background color */\n}\n\n.submit-btn:hover {\n    background-color: #3e8e41; /* Changes the button's background color when hovering over it */\n}\n\n#confirmationContainer {\n    padding: 20px; /* Adds padding around the content of the result container */\n    background-color: #f0eaea; /* Sets the background color of the result container */\n    border: 1px solid #ccc; /* Sets a 1px solid border with a light gray color */\n    border-radius: 5px; /* Applies rounded corners to the result container */\n    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Adds a shadow effect to the result container */\n    margin-top: 20px; /* Adds top margin to visually separate the confirmation container from the form */\n\n}\n\n\n\nTop favorite Websites pages\n\nHTML - popular.html\n{% extends \"base_template.html\" %}\n\n{% block title %}Popular Websites{% endblock %}\n\n{% block content %}\n  &lt;div class=\"table-container\"&gt;\n      &lt;h1&gt;Top Favorite Websites&lt;/h1&gt;\n      &lt;table&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n&lt;th&gt;URL&lt;/th&gt;\n&lt;th&gt;Tracking Count&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    {% for top_url in top_registered_urls %}\n&lt;tr&gt;\n  &lt;td&gt;{{ top_url[0] }}&lt;/td&gt;\n  &lt;td&gt;{{ top_url[1] }}&lt;/td&gt; \n&lt;/tr&gt;\n    {% endfor %}\n  &lt;/tbody&gt;\n&lt;/table&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n{% endblock %}\n\n{% block styles %}\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='/styles/popular.css')}}\"&gt;\n{% endblock %}\n\n\nCSS - popular.css\nbody {\n    font-family: Arial, sans-serif;\n    margin: 0;\n    padding: 0;\n    height: 100vh; /* Ensure the body takes up the full height of the viewport */\n}\n\n/* CSS styling for the table format */\n.table-container h1 {\n    text-align: center; /* Center-aligns the table header */\n}\n\n.table-container table {\n    width: 50vw; /* Set table width to 50% of the viewport width */\n    border-collapse: collapse; /* Collapse borders for a cleaner look */\n    margin: auto; /* Center the table horizontally */\n}\n\n.table-container th,\n.table-container td {\n    padding: 12px; /* Add padding for better readability */\n    border: 1px solid #ddd; /* Light gray border for table cells */\n    text-align: left; /* Left-align text in table cells */\n}\n\n.table-container th {\n    background-color: #4CAF50; /* Dark green background for header cells */\n    color: white; /* White text color for header cells */\n}\n\n.table-container tr:nth-child(even) {\n    background-color: #f2f2f2; /* Light gray background for even rows */\n}\n\n.table-container tr:hover {\n    background-color: #ddd; /* Change table row background color on hover for better visibility */\n}"
  },
  {
    "objectID": "posts/2024/connecting-db-app/index.html#summary",
    "href": "posts/2024/connecting-db-app/index.html#summary",
    "title": "Data-driven web applications basics: Connecting to a database",
    "section": "Summary",
    "text": "Summary\nIn this second post of the data-driven web applications basics, we have expanded our application by integrating a database to store user-registered URLs. This enhancement allows us to track and identify the most popular URLs. As a result, we have added a new page to our application that displays the top favorite websites—those that have been registered the most frequently—in a table format.\nWe have also covered several best practices for developing such applications. For instance, we emphasized the importance of validating user data before processing or storing it to ensure data integrity and security. We used .env files for storing connection and setup details rather than hardcoding them into the main code, which helps in managing configurations more securely and flexibly.\nAdditionally, we discussed the benefits of encapsulating functions to improve code organization and maintenance, employing design patterns to enhance code structure, and considering the overall UX to ensure a positive interaction with the application."
  },
  {
    "objectID": "posts/2024/intro-to-docker/index.html",
    "href": "posts/2024/intro-to-docker/index.html",
    "title": "Data-driven web applications basics: Containerizing our application",
    "section": "",
    "text": "In the previous posts of the data-driven web applications basics, we incrementally built a first and functional version of our application: a web application that allows users to submit their favorite websites and view the most popular sites based on these submissions.\nIn the first post, we focused on setting up the basic structure of our web application using Flask. This initial version of the application presented users with a one-page application containing a text input that allowed them to submit their favorite websites. However, at that stage, while we provided users with a confirmation message indicating their input had been received, we did not implement any mechanism to store the submitted data.\nFor this reason, in the second post, we addressed this limitation by integrating a database into our application. This upgrade allowed us to store user-submitted URLs persistently. As a result, we introduced a new page that displayed the most popular websites based on user submissions.\nWith these updates, our application is now functional, allowing users to register their favorite websites and view a ranking of the most popular sites based on user input. At this point, we may share the application with other developers for testing and further enhancement, or even consider deployment. This raises an important question: how can we ensure the application runs consistently across different environments?\nDeploying an application in various environments—such as development, testing, and production—often reveals issues related to differences in operating systems, dependencies, and configurations. These inconsistencies can be time-consuming because each environment may require separate installation and configuration of all necessary dependencies and requirements to ensure the application functions correctly. Moreover, these discrepancies may lead to bugs or performance issues that are difficult to diagnose and resolve."
  },
  {
    "objectID": "posts/2024/intro-to-docker/index.html#containerization",
    "href": "posts/2024/intro-to-docker/index.html#containerization",
    "title": "Data-driven web applications basics: Containerizing our application",
    "section": "Containerization",
    "text": "Containerization\nTo address these challenges, one effective solution is containerization. Containerization involves packaging an application along with all its necessary components and requirements into a single, self-contained unit called a container image.\nA container image is a standalone, executable package of software that includes everything needed to run an application, such as the application code, runtime environment, system tools, system libraries and settings. Because of that container images are highly portable, enabling developers to create applications that can be “built once and run anywhere.”\nWhen a container image is ran, it transforms into a running instance called a container. Containers run independently of other software on the host system by including all necessary dependencies, configurations, and libraries within themselves. This ensures that the application operates consistently across different environments.\nContainerization simplifies deployment across various environments—such as development, testing, and production—by eliminating the need to install dependencies or verify software compatibility. It ensures consistent application behavior regardless of the deployment environment, reducing the risk of discrepancies between different environments.\nAdditionally, containers enhance scalability, as they can be easily replicated and managed, facilitating efficient scaling in response to varying user demands. Containers are also instrumental in supporting microservices architectures. By isolating each service within its own container, developers can build, deploy, and maintain complex applications more effectively. This modularity allows for easier management of different components, enabling continuous integration and delivery while simplifying troubleshooting and updates.\nTherefore, containers provide a robust solution for modern software development and deployment, offering consistency, portability, scalability, and flexibility. These benefits contribute to faster delivery of software with fewer errors and greater reliability.\n\n\n\n\n\n\nContainers vs. Virtual Machines\n\n\n\nContainers may sound similar to virtual machines (VMs), but there are key differences between the two. While VMs virtualize at the hardware level, creating full instances of an operating system that run on top of a hypervisor, containers virtualize at the operating system level by sharing the host’s kernel. This means containers only package the application and its dependencies, without the need to include an entire OS. Because of this, containers are significantly lighter, faster to start and stop, and more efficient in terms of resource usage compared to VMs. These advantages allow for greater density—more containers can run on a single host compared to VMs—making containers a more agile and scalable solution for modern application deployment.\n\n\n\nDocker\nDocker has become the most widely adopted platform for creating, deploying, and managing containers thanks to its ease of use, extensive ecosystem, and strong community support.\nTo start creating and using Docker containers, installation of the the Docker Engine is required. Detailed installation instructions for various operating systems can be found at the Docker Engine Installation Guide.\nMoreover, Windows users also need to install the Windows Subsystem for Linux (WSL), which serves a compatibility layer allowing to run a Linux distribution directly on Windows. This is necessary because Docker relies on Linux-based technologies to function properly, and WSL provides the required Linux environment on Windows. You can install WSL by following the instructions here: Install Windows Subsystem for Linux. Furthermore, setting up WSL enables Docker to build and run Linux-based containers on a Windows machine as it provides a Linux kernel environment.\nOnce Docker is installed and configured, we are ready to start building and using Docker containers. This process generally involves three key stages: creating a Dockerfile, building a container image from this Dockerfile, and running that image as a container.\nThe first step in containerization with Docker is creating a Dockerfile. A Dockerfile is a plain text file that contains a series of instructions for Docker to follow in order to build a container image. This file outlines everything necessary to construct and run your application, including which files to include in the image, which libraries to install, the configuration settings to apply, and the commands needed to launch the application.\nOnce the Dockerfile is ready, we can use it to build a container image. When building an image, Docker reads the instructions in the Dockerfile and assembles the image. A container image is an executable package that includes everything needed to run our application.\nThese images can be run, becoming containers, i.e., an isolated environment that runs the application along with all its requirements. Figure 1 visually summarizes the containerization process that we just described.\n\n\n\n\n\n\nFigure 1: Docker containerization process\n\n\n\nWith this foundation in place, we can move on to the practical task of containerizing our application using Docker."
  },
  {
    "objectID": "posts/2024/intro-to-docker/index.html#creating-a-dockerfile",
    "href": "posts/2024/intro-to-docker/index.html#creating-a-dockerfile",
    "title": "Data-driven web applications basics: Containerizing our application",
    "section": "Creating a Dockerfile",
    "text": "Creating a Dockerfile\nAs we have just seen, the first step step to create a container image is to generate a Dockerfile. This file contains a set of instructions that define how the container image should be built and configured. It specifies the base image to use, the software and dependencies to install, and any additional configuration required for your application.\nIn essence, the Dockerfile serves as a blueprint for creating a container image, detailing a sequence of instructions that guide the Docker engine through the image-building process. Although there are numerous instructions available, some are fundamental to the process and are widely used in Dockerfiles. The following are among the most essential:\n\nFROM: Defines the base image that the container will be built on. Docker images are often created by extending an existing image that includes the necessary components for your software. For example, if we are containerizing a Python application, the container will need a working version of Python, so the application can run. In this case, we can start with a base image that already has Python installed and then add the application on top of it.\nWORKDIR: Sets the working directory inside the container where all subsequent commands will be executed. It ensures that any following instructions operate within this specified directory. If the specified working directory does not exist, it will be automatically created.\nCOPY: Specifies which files from your local system should be copied into the container’s working directory. This typically includes essential elements like application code, configuration files, images, and other assets necessary for the containerized application to function.\nRUN: Executes commands during the image build process. These commands might include installing software, setting up dependencies, or configuring the environment to prepare it for running the application.\nCMD: Defines the default command that will run when the container starts. This command could involve launching an application server, running a script, or starting a service. Unlike RUN, which is used during the image-building process, CMD determines what happens when the container image is executed.\nENV: Sets environment variables that will be available for all subsequent instructions. In addition, these environment variables will persist when a container is run from the resulting image.\nEXPOSE: Indicates the ports that the containerized application will use. This instruction does not automatically publish the ports to the host; it merely documents which ports the container is intended to listen on for connections.\n\nThese instructions are sufficient for creating our Dockerfile. We can now move forward by specifying each of these instructions for our application, completing the Dockerfile and enabling us to build the container image for our application.\n\n1. FROM\nWhen creating a container image, it’s typically more efficient to start with a pre-existing base image that already includes essential components, rather than building everything from scratch. A base image is a pre-built container image that comes with necessary software, libraries, and tools, providing a solid foundation for your application. The FROM instruction in your Dockerfile specifies which base image to use, establishing the starting point for a container image.\nDocker uses a layered architecture to build images. Each layer represents a set of changes or additions to the image and is built on top of the previous layer. By choosing a base image, you create the initial layer of your Docker image. Subsequent instructions in your Dockerfile—such as installing additional software, copying files, or setting environment variables—add new layers on top of this base image. In the next sections, we will take a closer look at how this layered architecture works.\n\n\n\n\n\n\nContainer isolation\n\n\n\nThe primary advantage of using containers is their ability to ensure consistent application performance across different systems. Containers achieve this by providing a fully isolated environment for your application, separate from the host system. This isolation guarantees that your application will run the same way on any device with a compatible kernel, since all the necessary components are contained within the container itself. However, this means that the container must incorporate all the components required for your application to function properly.\n\n\nBase images can be chosen from different sources, including images stored locally on your system from previous projects, as well as external repositories such as Docker Hub. Docker Hub is a container registry, hosting a vast collection of images contributed by numerous developers and organizations. It provides an extensive library of pre-built images that can be directly pulled and utilized.\nFor our specific needs, given that the application we have developed in earlier posts is written in Python, we can start by selecting a base image that already includes Python, which we will retrieve from Docker Hub.\nTo do so, we will navigate to Docker Hub, where we will search for “Python”. This search will return a list of container images that include Python. The top result is the official Python image. We can click on this image to access detailed information about it. Figure 2 illustrates how to navigate to this image on Docker Hub.\n\n\n\n\n\n\nFigure 2: Searching for the Python base image on Docker Hub\n\n\n\nNow that we are on the page for the official Python image, we can see several sections, such as “Simple Tags,” “Shared Tags,” and “Image Variants.” These sections indicate that there are different versions or variants of this Python image. Essentially, these variants include various versions of Python and may also come with additional tools or libraries.\nTo clarify further, let’s explore the “Simple Tags” section. As shown in Figure 3, all tags begin with a version number such as “3.13.0a5,” “3.13,” or “3.12.2.” This part of the tag specifies the version of Python included in that particular image variant.\nFollowing the version number, separated by a hyphen, you will find terms like “bookworm,” “slim-bookworm,” “bullseye,” or “windowsservercore.” This part of the tag indicates the base operating system that influences the image’s environment.\nFor instance, “bookworm” is based on the Debian Bookworm operating system. The “slim-bookworm” variant, while also based on Debian Bookworm, is a lighter version, which means it excludes unnecessary components found in the non-slim “bookworm” variant.\nHowever, it’s crucial to understand that the image itself does not include a complete operating system. Instead, the image incorporates libraries and dependencies from the specified base OS but does not contain its full kernel. The actual kernel used by the image is provided by the host system. Thus, while the image relies on components from the indicated base OS for its runtime environment, it operates using the host’s kernel for fundamental system functions.\n\n\n\n\n\n\nFigure 3: “Simple Tags” section for the Python base image\n\n\n\nTo select the appropriate variant of the Python image for our base image, we need to first determine the Python version that our application was developed with and is currently using. Remember that the purpose of using containers is to encapsulate the application along with all its dependencies, ensuring that it runs smoothly and consistently within its own environment. Therefore, to ensure that our application behaves as expected in the container, we must use the same Python version that we used during development.\nTo identify the Python version, open a terminal and execute the command python --version or python -V. This command will display the exact Python version installed on your system, which is crucial for maintaining consistency. Figure 4 shows the output of this command, showing the specific version of Python in use.\n\n\n\n\n\n\nFigure 4: Checking Python’s version\n\n\n\nAs shown by the output of the previous commands, in my case, the Python version in use is 3.12.4. Therefore, we will choose a base image that matches this Python version to ensure consistency. For our Flask application, we do not require any additional components from the base image, such as specific operating system variants like “bookworm” or “bullseye.” These variants typically include extra libraries and tools that are unnecessary for a simple Flask app.\nIncluding extra components, such as OS-specific libraries and tools, can substantially increase the size of the resulting image, resulting in a bulkier and less efficient deployment. Since our application only requires Python and no additional OS-specific elements, we should opt for a base image that excludes these unnecessary extras. Additionally, minimizing the inclusion of redundant components helps to reduce build time, and lower potential security vulnerabilities.\nIn the Dockerfile, we will specify the base image to include Python 3.12.4. To achieve this, we will use the image name from Docker Hub, which is python, followed by the tag for the desired version, 3.12.4. Tags are denoted using a colon (:), so the base image we will use is python:3.12.4. Therefore, our Dockerfile will start with the following instruction:\nFROM python:3.12.4\nThis FROM instruction tells Docker to first look for the python:3.12.4 image on our local machine. If the image is not found locally, Docker will search Docker Hub, pull the image, and use it as the base for our containerized application.\n\n\n2. WORKDIR\nAfter defining the base image in our Dockerfile, the next step is to set the working directory using the WORKDIR instruction. The working directory establishes the default location where subsequent instructions, such as RUN, CMD, and COPY, will be executed.\nIf the directory specified by the WORKDIR instruction does not already exist, Docker will automatically create it.\nIn our example, we will set the working directory to /app using the WORKDIR /app instruction. This means that all subsequent instructions will be run from within the /app directory. As a result, our Dockerfile will include the following lines:\nFROM python:3.12.4\nWORKDIR /app\n\n\n3. COPY\nHaving defined the working directory, we can proceed to copy all the required files needed to run our application. Specifically, we need to copy the folder containing our Flask application and all its associated files into this directory.\nHowever, copying the folder alone won’t make the application functional right away. This is because while our container will have Python installed, it won’t include the additional libraries required by our application. For instance, our Flask application depends on libraries such as Flask itself.\nTo address this, we first need to create a requirements file, i.e. a file that specifies the libraries that must be installed for our application to run, allowing us to install these dependencies within the container.\n\nCreating the Requirements File\nIn Python, a requirements file is a simple text file that lists the names and versions of all the libraries needed for an application or program, each on a separate line. The format for each line is library_name==library_version. The standard name for this file is requirements.txt, though we can technically use any name. Following this convention is recommended because it simplifies understanding and collaboration.\nWe can generate a requirements file in different ways. For instance, we can create it manually, or we can use tools that automate its creation. For instance, a common approach is to use the pipreqs library. This library checks the imports of a project and based on that automatically creates a requirements file.\nTo use pipreqs, we first need to install it via pip with the following command:\npip install pipreqs\nOnce we have installed pipreqs, we can navigate to our application’s directory and run the pipreqs command. This command scans the entire directory to identify all the libraries that our application uses, excluding those included by default in Python. It then generates a requirements.txt file in the same directory, listing these libraries along with their respective versions. Figure 5 illustrates the output of running this command.\n\n\n\n\n\n\nFigure 5: Generating requirements.txt using pipreqs\n\n\n\n\n\n\n\n\n\nAn alternative approach to generate requirements files\n\n\n\n\n\nAn alternative approach to using pipreqs for generating a requirements.txt file involves using a Python virtual environment along with the pip freeze command.\n\nPython virtual environments\nA Python virtual environment is an isolated environment in which you can install Python packages independently from the global Python installation. It allows you to manage dependencies on a per-project basis, ensuring that each project has its own set of packages and versions without affecting other projects or the system-wide Python installation. This is particularly useful for avoiding conflicts between package versions and for maintaining consistent environments across different development setups.\nYou can create a virtual environment using the venv module by running the the following command on a terminal:\npython -m venv myenv\nWhen you run this command, it will create a new directory named myenv (or the name you chose) in the current directory. This new directory will contain a copy of the Python interpreter and a local site-packages directory where you can install packages. It also includes scripts for activating the environment, allowing you to use isolated dependencies within that environment.\nAfter creating the virtual environment, you activate it:\nOn Windows:\nmyenv\\Scripts\\activate\nOn macOS and Linux:\nsource myenv/bin/activate\nActivating the virtual environment ensures that any packages you install or any Python commands you run use the Python interpreter and libraries from within this environment, rather than the global Python installation. This helps maintain dependencies specific to your project and prevents potential conflicts.\nOnce the environment is activated, you should install the libraries required for the current project. You can do this using pip:\n\npip install somepackage\n\n\n\nUsing pip freeze\nOnce you have activated your virtual environment and installed the necessary packages for your project, you can use the pip freeze command to generate a requirements.txt file.\nThe pip freeze command outputs a list of all installed packages in the current environment along with their versions in the format package==version. This list represents the exact state of the environment and is useful for recreating the environment elsewhere. To save this list to a requirements.txt file, you would run the following command on the terminal you have activated your virtual environment:\npip freeze &gt; requirements.txt\nThis command captures the list of all installed packages and their versions in the current environment and saves it to requirements.txt in the directory where you run the command.\n\n\n\n\nNow, in the application folder, we can see a new file named requirements.txt, as shown in Figure 6.\n\n\n\n\n\n\nFigure 6: After executing pipreqs a requirements file is automatically generated\n\n\n\nIf we open the requirements.txt file, we will see that it contains the following contents:\nFlask==3.0.2\npython-dotenv==1.0.1\npsycopg==3.2.1\nWe now have all the necessary files to containerize our application: the application code, along with the requirements file, which specifies the the Python libraries not bundled with the base Python installation required for our application.\nHowever, before we proceed with specifying the COPY instructions in the Dockerfile to copy these files into the container image, it’s important to recall how Docker builds images. Docker images are constructed using a layered structure, where each instruction in the Dockerfile creates a new layer on top of the previous ones. This structure allows Docker to rebuild only the layers that have changed and any subsequent layers to these, while reusing previous layers. This optimization is intended to speed up the rebuild process.\nGiven this, it is advantageous to separate the application files from the requirements file. The application code is likely to change frequently, leading to multiple image rebuilds. In contrast, the required libraries typically remain stable and do not change as often. By separating these components, we avoid reinstalling the libraries during each rebuild in which our code has changed, as the layer containing the libraries remains unchanged.\nTo achieve this, we will first make a slight adjustment to our file structure. Specifically, we will move the application files into a folder named src, while leaving the Dockerfile and the requirements file outside of this folder. This separation helps us to clearly differentiate between the application code and the files used for containerization. The updated file structure will look as follows:\n- my_flask_app/                        (Main folder of the application)\n    |\n    |- Dockerfile                      (File specifying how to build the Docker image for our application)\n    |\n    |- requirements.txt                (Python requirements file)\n    |\n    |- src/                            (Folder containing the application code)\n        |\n        |- app.py                      (Main file for the Flask application)\n        |\n        |- .env                        (File to store environmental variables)\n        |\n        |- templates/                  (Folder to store HTML templates)\n        |    |\n        |    |- base_template.html     (HTML template for the base template)\n        |    |\n        |    |- index.html             (HTML template for the main page)\n        |    |\n        |    |- navbar.html            (HTML template for the navigation bar)\n        |    |\n        |    |- popular.html           (HTML template for the top favorite websites page)\n        |\n        |- static/                     (Folder to store static files such as CSS, JavaScript, images, etc.)\n             |\n             |- styles                 (Folder to store style sheets, such as CSS files)\n                |\n                |- base_template.css   (CSS file to style the base template elements)\n                |\n                |- home.css            (CSS file to style the main page)\n                |\n                |- navbar.css          (CSS file to style the navigation)\n                |\n                |- popular.css         (CSS file to style the top favorite websites page)\nWith our file structure adjusted, we can proceed to copy these files into the container.\nTo achieve this, we use the COPY instruction in the Dockerfile. This instruction allows us to specify the source path and filename relative to the Dockerfile’s location, as well as the destination path within the container. In our setup, we will copy the requirements.txt file and the src directory into the /app directory inside the container, which serves as our working directory. As a result, our Dockerfile will look like this:\nFROM python:3.12.4\nWORKDIR /app\nCOPY requirements.txt /app\nCOPY src /app\n\n\n\n\n\n\nWhy the sequence of instructions matters in a Dockerfile\n\n\n\nWhen working with Dockerfiles, the order of the instructions plays an important role. In this post, we present the various blocks of instructions in a somewhat organized manner, but it’s important not to be misled by this arrangement. The sequence we use here is designed to help us define instructions in a more structured way during this practical example. However, the actual order of instructions in a Dockerfile doesn’t necessarily follow this same sequence, and instructions may be repeated at different points in the file. This means that a Dockerfile doesn’t always have a strict linear structure—instructions can be interleaved as needed.\nThe order of instructions in a Dockerfile plays significant importance due to the way Docker images are constructed. Docker images are built in layers, with each instruction in the Dockerfile typically corresponding to a new layer. When the docker build command is executed, Docker attempts to reuse layers from previous builds to optimize the process.\nIf a layer has changed since the last build, that layer and all subsequent layers must be rebuilt. This is why the order of instructions matters: to minimize unnecessary rebuilding and redundancy, it’s beneficial to arrange the Dockerfile in a way that optimizes layer caching. For instance, placing the instructions for downloading and installing dependencies before copying the source code allows Docker to reuse the “dependencies” layer from the cache, even if changes are made to the source code later. For this reason, it is important to keep this in mind when creating Dockerfiles, as the proper ordering of instructions can significantly reduce build times in subsequent builds.\nFigure 7 visually illustrates the concept of Docker image layering, helping to clarify how Docker builds images and why the order of instructions in a Dockerfile is so important.\n\n\n\n\n\n\nFigure 7: Visual overview of layered container image building\n\n\n\n\n\n\n\n\n4. RUN\nIn the previous step, we created a requirements file for our application. This file lists all the libraries that our application needs to run correctly. During the Docker image build process, both the requirements file and the application’s code are copied into the Docker image. The purpose of the requirements file is to ensure that all necessary libraries are installed in the Docker image, so the application can function as intended.\nTo install these libraries, we can use the RUN instruction in the Dockerfile. This instruction allows us to execute commands during the build process of the image. Specifically, we run the command pip install -r requirements.txt to install all the dependencies specified in the requirements file.\nProper placement of this RUN instruction is essential due to Docker’s layered image structure. To optimize the build process, we should place the dependency installation steps before adding the application code. By placing the dependency installation steps before adding the application code, we ensure that Docker can cache the layer containing the installed libraries. As a result, if the application code changes but the dependencies remain the same, Docker can reuse the cached layer for the dependencies, which speeds up subsequent builds by avoiding redundant installations.\nWith these adjustments, our Dockerfile will now look like this:\nFROM python:3.12.4\nWORKDIR /app\nCOPY requirements.txt /app\nRUN pip install -r requirements.txt\nCOPY src /app\n\n\n5. CMD\nWe have now defined all the necessary instructions in the Dockerfile to set up our application. This includes specifying the required dependencies such as Python and the necessary libraries, how to install them, and outlining how to copy the application files into the Docker image. Our Dockerfile is almost complete; however, we still need to specify how to run the application when the container starts.\nPreviously, we have run our Flask application using the flask run command. We will use the same command to start the application within the Docker container. However, because a Docker container operates in an isolated environment with its own network configuration, we must specify additional parameters to ensure the application is accessible from outside the container. Specifically, we need to set the host and port parameters in the flask run command:\n\n-h 0.0.0.0: By default, Flask binds to localhost, which restricts access to only within the container. Setting the host to 0.0.0.0 instructs Flask to listen on all network interfaces within the container. This change makes the application accessible from outside the container, allowing it to be reached from any IP address that can connect to the Docker container.\n-p 5000: This parameter specifies the port number on which Flask will listen for incoming connections. Port 5000 is the default for Flask applications, but this can be adjusted if necessary. It ensures that Flask is listening on the appropriate port inside the container.\n\nTherefore, the command to run the Flask application with the required parameters will be:\n\nflask run -h 0.0.0.0 -p 5000\n\nWe can execute this command every time the container is started by using the CMD instruction in our Dockerfile. Remember that the CMD instruction differs from the RUN instruction. The RUN instruction is used during the build process of the container image, whereas CMD specifies the command to run when the container is started.\nTo ensure clarity and proper execution within the container, it is a best practice to use the JSON array format for the CMD instruction in the Dockerfile. This format avoids potential issues with shell parsing and ensures that each part of the command is handled correctly. In this format, the command would be specified as: [\"flask\", \"run\", \"-h\", \"0.0.0.0\", \"-p\", \"5000\"].\nHere is how the Dockerfile should look with the CMD instruction included:\nFROM python:3.12.4\nWORKDIR /app\nCOPY requirements.txt /app\nRUN pip install -r requirements.txt\nCOPY src /app\nCMD [\"flask\", \"run\", \"-h\", \"0.0.0.0\", \"-p\", \"5000\"]\n\n\n6. ENV\nWhen we use the flask run command, we are assuming that Flask will correctly identify the main application file. However, within the container environment, this may not always be clear or correctly interpreted.\nTo resolve this issue, it’s important to explicitly specify the main Flask application file by defining an environment variable within the Docker container. In our case, the main file is app.py. We can achieve this by using the ENV instruction in the Dockerfile, which allows us to define environment variables in the format key=value. Specifically, we need to set the FLASK_APP environment variable to app.py. This can be done with the following instruction:\nENV FLASK_APP=app.py\nIn addition, we need to set this instruction before the CMD instruction in our Dockerfile to ensure that the FLASK_APP variable is available when the flask run command is executed.\nWith this addition, our updated Dockerfile will look like this:\nFROM python:3.12.4\nWORKDIR /app\nCOPY requirements.txt /app\nRUN pip install -r requirements.txt\nCOPY src /app\nENV FLASK_APP=app.py\nCMD [\"flask\", \"run\", \"-h\", \"0.0.0.0\", \"-p\", \"5000\"]\n\n\n7. EXPOSE\nTo ensure that our Flask application running inside the Docker container can be accessed from outside the container, we need to make its network port available. Since our Flask application listens on port 5000, it is crucial to expose this port in the Docker container configuration. Exposing the port allows external applications and services to connect to our Flask application without encountering connectivity issues.\nIn Docker, the EXPOSE instruction is used to declare which ports the container will listen on at runtimeSince our Flask application listens on port 5000, we need to specify this port using the EXPOSE instruction.\nBy doing so, our updated Dockerfile will look as follows:\nFROM python:3.12.4\nWORKDIR /app\nCOPY requirements.txt /app\nRUN pip install -r requirements.txt\nCOPY src /app\nENV FLASK_APP=app.py\nCMD [\"flask\", \"run\", \"-h\", \"0.0.0.0\", \"-p\", \"5000\"]\nEXPOSE 5000\n\n\nFinished Dockerfile\nAfter completing these steps, we have finalized our Dockerfile, which now defines everything needed to build a Docker image for our application. This file specifies the process for setting up the base environment, managing dependencies, and configuring the application’s runtime behavior.\nTo quickly recap, we have specified that the Dockerfile starts with the official Python 3.12.4 base image. We then create a working directory inside the container and copy the requirements.txt file into this directory. This file specifies all the required Python libraries, which we use to install the libraries during the image build process.\nNext, we copy the application files into the container and set the FLASK_APP environment variable to specify the main file of our Flask application. We then use the CMD instruction to define the command that will run when the container starts and we expose port 5000 to allow external access to our Flask application.\nHere is the completed Dockerfile:\n# Use the Python 3.12.4 image as the base\nFROM python:3.12.4\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the requirements file and install the dependencies\nCOPY requirements.txt /app\nRUN pip install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY src /app\n\n# Set the environment variable to specify the Flask application file\nENV FLASK_APP=app.py\n\n# Run the Flask application with the host set to 0.0.0.0 to allow external access\nCMD [\"flask\", \"run\", \"-h\", \"0.0.0.0\", \"-p\", \"5000\"]\n\n# Expose port 5000 for external access\nEXPOSE 5000"
  },
  {
    "objectID": "posts/2024/intro-to-docker/index.html#building-our-container-image",
    "href": "posts/2024/intro-to-docker/index.html#building-our-container-image",
    "title": "Data-driven web applications basics: Containerizing our application",
    "section": "Building our container image",
    "text": "Building our container image\nAfter specifying all the necessary instructions in the Dockerfile to set up our container image, the next step is to build the image. Building the image instructs the Docker engine to process the Dockerfile, executing all the specified instructions to create a self-contained package that includes all dependencies and application code. This package can then be deployed and run consistently across various environments.\nTo build the image, we first need to open a terminal or console and navigate to the directory where our Dockerfile is located. In this case, the directory is my_flask_app. Once we are in the correct directory, we can execute the build command to create the container image based on the Dockerfile’s instructions. Specifically, we use the following command:\ndocker build -t my_flask_app .\nThis command uses the Docker command-line interface to build an image. The -t flag tags the image with the name my_flask_app, making it easier to later refer to this image. The final dot (.) specifies the build context, which tells Docker to use the current directory to locate the Dockerfile and any associated files necessary for the build process.\nUpon execution of this command, Docker will read and process the Dockerfile, following each instruction specified within it. This process results in the creation of a container image, as illustrated in Figure 8.\n\n\n\n\n\n\nFigure 8: Building our container image\n\n\n\n\n\n\n\n\n\nEnsure the Docker engine is running before using Docker commands\n\n\n\nBefore executing Docker commands, including those for building images, verify that the Docker engine is running. On Windows, Docker Desktop usually starts automatically after installation. You can check its status by looking for the Docker icon in your system tray. If the icon is not present, manually start Docker Desktop by searching for “Docker Desktop” in the Start menu and launching it from there.\nOn Linux, you can check if Docker is running by using the command sudo systemctl status docker. This will provide the current status of the Docker service. If Docker is not running, start it with sudo systemctl start docker and configure it to start automatically at boot with sudo systemctl enable docker. Additionally, prepend sudo to Docker commands to ensure they execute with the necessary administrative privileges."
  },
  {
    "objectID": "posts/2024/intro-to-docker/index.html#running-our-container",
    "href": "posts/2024/intro-to-docker/index.html#running-our-container",
    "title": "Data-driven web applications basics: Containerizing our application",
    "section": "Running our container",
    "text": "Running our container\nAfter successfully creating the container image, the next step is to run it. To do this, open a terminal and execute the following command:\ndocker run -p 5000:5000 my_flask_app\nThis command uses the Docker command-line interface to start a new container instance based on the image named my_flask_app. The -p 5000:5000 flag is included to map port 5000 on the host machine to port 5000 in the container. Without this flag, the container’s internal port may remain inaccessible from outside the container, meaning we may not be able to interact with the application through our web browser or any other external client.\nOnce executed, we will receive a confirmation message indicating that the container is up and running. The message will also provide the address through which we can access the containerized Flask application, as shown in Figure 9.\n\n\n\n\n\n\nFigure 9: Running our container\n\n\n\nIn the image above, we can observe that the container outputs three different messages about its running state:\n\nRunning on all addresses (0.0.0.0): This message signifies that the Flask application is configured to listen for requests on all network interfaces within the container. The address 0.0.0.0 is a special placeholder that tells the application to accept connections from any IP address available within the container’s network environment. This configuration is specified by the CMD instruction in our Dockerfile.\nRunning on http://127.0.0.1:5000: This line indicates that the Flask application is accessible via the loopback address 127.0.0.1 on port 5000 within the container. The loopback address is specific to the container itself, meaning this address can only be reached from within the container and is not accessible from outside the container.\nRunning on http://172.17.0.2:5000: This message provides another address, 172.17.0.2, where the Flask application is accessible. This IP address is assigned to the container by Docker and allows access to the application from other containers on the same Docker network or from the host machine.\n\nTherefore, we can access our containerized application by navigating to http://172.17.0.2:5000 in our web browser. This will allow us to interact with the application running inside the container, as illustrated in Figure 10.\n\n\n\n\n\n\nFigure 10: Containerized application running at http://172.17.0.2:5000"
  },
  {
    "objectID": "posts/2024/intro-to-docker/index.html#summary",
    "href": "posts/2024/intro-to-docker/index.html#summary",
    "title": "Data-driven web applications basics: Containerizing our application",
    "section": "Summary",
    "text": "Summary\nIn this post, we walked through the process of creating a Dockerfile to build a container image for our application. After creating the Dockerfile, we built the container image and successfully ran it, confirming that our application functioned correctly within the container.\nDuring this process, we saw that the Dockerfile’s instructions must be carefully considered—not only for their inclusion but also for their order. Docker images are constructed in layers, with each layer representing a step in the build process. When we rebuild an image, only the layers that have changed and those built after them are re-executed. This layer-based architecture allows Docker to optimize the build process, significantly speeding up image reconstruction."
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\n\n\n\nFigure 1: S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-bit-of-history",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "",
    "text": "The decade of the 2000s may have been a pretty good decade in many aspects, movies such as the lord of the rings or harry potter were released; music albums such as The Strokes’ “Is this It” or Bob Dylan’s “Modern Times” were released. Nevertheless, this decade was not a good one in economic terms. This decade started with the dotcom crash, which was triggered by the rise and fall of technology stocks. And, this was not the only remarkable economic event of this decade, when the economy seemed to have recovered from this crisis, another crisis broke out, the financial crisis of 2007-2008, triggered by the collapse of the housing market in the U.S. All these economic turbulences can be clearly seen in the evolution of the S&P500 between 2000 and 2010 as shown in Figure 1.\n\n\n\n\n\n\nFigure 1: S&P500 (2000 - 2010)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-time-of-low-interest-rates",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A time of low interest rates",
    "text": "A time of low interest rates\nMoreover, these economic disturbances led governments to act severely. Keynesian policies for the activation of the economy began to play a fundamental role. To encourage consumption, interest rates were lowered to near historic lows. Figure 2 displays the US 10 year note bond yield between 1920 and 2020. In this figure, we can see how the dotcom crash pushed down this yield. However, after that it started recovering until July 2007, when the financial crisis started, after that, that yield continued a downward trend.\n\n\n\n\n\n\nFigure 2: US 10 year note bond yield"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#stocks-as-the-only-attractive-investment-vehicle",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Stocks as the only attractive investment vehicle",
    "text": "Stocks as the only attractive investment vehicle\nSuch a long period of low interest rates inflated stock prices. Figure 3, shows how cyclically adjusted S&P 500 price-to-earnings ratio has been rising during the last decade, being quite high in comparison to other periods of time (specially before the dot-com bubble). Stock prices started trading at a premium, because there were no attractive alternative investments, this channelled much of the liquidity into equities (bonds with almost no interest or even negative interests were not an attractive investment anymore). Moreover, this low interest rate setting has prompted greater investor leverage, due to its low cost. Hence, low interest rates justify high stock prices, since stocks are highly attractive relative to bonds and debt is stimulated due to its reduced cost.\n\n\n\n\n\n\nFigure 3: Cyclically adjusted S&P 500 price-to-earnings rations"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#an-unexpected-event-a-global-pandemic",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "An unexpected event: A global pandemic",
    "text": "An unexpected event: A global pandemic\n2020 was not a good year, this year will always be remembered as the year of the COVID. COVID brought many changes in our lives, which undoubtedly had an impact on the economy. As a result, governments continued to pursue stimulative policies and interest rates remained at very low levels.\nOne of the best illustrations of those stimulative policies is the amount of dollars printed in 2020: 21% of the United States dollar was printed in 2020, as shown in Figure 4. This large injection was used for both direct and indirect assistance in the COVID situation. This, at the same time, indirectly channelled part of this aid to the financial markets, increasing their value.\n\n\n\n\n\n\nFigure 4: Annual money stock growth (trillions of USD)"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#a-double-edged-sword",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "A double-edged sword",
    "text": "A double-edged sword\nInjecting money to the economy is always something controversial. Through the law of supply and demand it is easy to infer that a considerable increase in supply (without a similar increase in demand) will reduce the price of a good. In money, when this happens, we say that the money loses value, i.e. one monetary unit can acquire fewer products. In other words, this is what we call inflation. Even Warren Buffet has shown concern for inflation in the past month:\n\n“We are seeing very substantial inflation […] We are raising prices. People are raising prices to us and it’s being accepted.”\n\nThis raise on prices can be already tracked on several indices such as Bloomberg’s agriculture index, shown in Figure 5 and also in the annual single-family home price, as shown in Figure 6.\n\n\n\n\n\n\nFigure 5: Bloomberg agriculture index\n\n\n\n\n\n\n\n\n\nFigure 6: Annual single-family home price\n\n\n\nThe appearance of inflation means that interest rates should rise. Something that was already pointed by Janet Yellen at the start of this month:\n\n“It may be that interest rates will have to rise somewhat to make sure that our economy doesn’t overheat”\n\nBut what would happen if inflation persists and interest rates have to be risen? Remember that I previously said that stocks are trading at a premium due to the lack of attractive alternative investments. This would no longer be true and this premium would no longer be a thing. In addition, an increase in interest rates would reduce the attractiveness of leverage, thereby encouraging investor’s deleverage. And, thus, we should expect a decrease in the stock price."
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#the-market-can-not-be-timed",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "The market can not be timed",
    "text": "The market can not be timed\nEven with all these indicators, it is difficult to say whether this will happen in the short to medium term. There are many factors which could deter inflation away and interest rates low. As an example, Berkshire Hathaway has been stacking cash during the last years, as shown in Figure 7, playing a slightly more defensive position. This may be due the fact that they already saw that low interest rates during the last decade were driving the stock market at high prices. However, interest rates are still low and during that time the stock market has continued rising.\n\n\n\n\n\n\nFigure 7: Berkshire’s cash holdings"
  },
  {
    "objectID": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "href": "posts/2021/why-potential-inflation-could-lead-to-a-financial-crisis/index.html#summary",
    "title": "Why potential inflation could lead to a financial crisis?",
    "section": "Summary",
    "text": "Summary\n\nThe first decade of the 2000s was characterized by two economic crisis, which defined an economy with very low interest rates\nLow interest rates increased stock market prices, since there were no attractive alternative investments and leverage was cheap. Thus, stock markets traded at a premium.\nThe Covid crisis led to a massive injection of money into the economy.\nThis money injection is a double-edged sword which may have brought an exuberance illusion awakening a ghost that has been dormant in recent years, inflation.\nThe emergence of inflation would imply an increase in interest rates.\nAn increase in interest rates would mean that the premium paid for stocks would be lost.\nDespite all these facts, timing the market is no easy task. And thus, other factors could keep inflation away and interest rates away, prolonging this situation."
  },
  {
    "objectID": "posts/2025/improving-architecture/index.html",
    "href": "posts/2025/improving-architecture/index.html",
    "title": "Data-driven web applications basics: Improving the architecture",
    "section": "",
    "text": "In the previous posts of our series on building data-driven web applications, we developed a web application that allows users to submit their favorite websites and view a ranking of the most popular sites based on those submissions. We began by setting up the basic structure of the app using Flask, a lightweight web framework, and then integrated a PostgreSQL database to store user submissions, ensuring data persistence. Next, to leverage the stored data, we created a new page, the Top Favorite Websites page, where users could view the most popular websites based on the number of submissions. Finally, we containerized the application using Docker to streamline deployment across different environments and facilitate scalability.\nNow that the app is fully functional, we may start to think about deploying it for real-world usage. However, before moving forward, we need to evaluate whether there are any potential bottlenecks or areas of the application that might cause issues under real-world conditions. To do so, let’s take a moment to refresh our memory on how our application operates by reviewing what each page of the application does.\nThe Home page serves as the data submission component of the application. Here, users interact with the system by submitting their favorite website URLs. The page contains a simple form where users can input the URL of a website they want to submit. Upon entering a URL and clicking the Register URL button, the backend performs input validation to ensure that the submitted URL is properly formatted and valid. This step is crucial to prevent invalid data from being stored in the database. If the validation is successful, the URL is then stored in a PostgreSQL database. Once the URL has been successfully added to the database, the user receives a confirmation message indicating that their submission was successful.\nIn contrast, the Top Favorite Websites page serves as the data presentation component of the application. This page provides a user-friendly interface where users can view the ten most popular websites based on the submissions from the Home page. When a user accesses this page, the backend queries the database to count how many times each website has been submitted. These counts are then sorted in descending order, with the most frequently submitted websites appearing at the top of the list. The data is dynamically processed and used to render a table on the page, displaying the ten most popular websites.\nAll this information is graphically summarized in Figure 1.\nUpon examining the current architecture, a critical issue emerges: the application design places a significant and potentially unsustainable load on the database during periods of high user activity. This problem arises from the way data is handled in both the Home page and the Top Favorite Websites page. While the initial implementation worked well during development, it poses significant scalability challenges as the user base and data volume grow. Let’s delve deeper into the specific issues, analyzing each page to understand why the current design struggles under increasing demand."
  },
  {
    "objectID": "posts/2025/improving-architecture/index.html#implementing-these-changes-into-our-application",
    "href": "posts/2025/improving-architecture/index.html#implementing-these-changes-into-our-application",
    "title": "Data-driven web applications basics: Improving the architecture",
    "section": "Implementing these changes into our application",
    "text": "Implementing these changes into our application\nHaving identified the key areas for improvement and outlined the necessary changes, we are now ready to implement these optimizations. To achieve this, we will need to set up Redis, integrate Celery for task scheduling, and create the necessary background tasks to handle the periodic processes. Additionally, we’ll update the backend logic to interact with Redis instead of directly querying the PostgreSQL database. Let’s walk through each of these steps in detail.\n\nSetting up Redis\nThe first step in implementing these changes is to set up Redis, which will serve two critical roles: as a cache for the Top Favorite Websites table and as a queue for managing URL submissions from the Home page.\nOne of the simplest and most efficient ways to use Redis is through its official container images. Essentially, Redis offers two official Docker images:\n\nredis/redis-stack-server: A lightweight image that includes only the Redis Stack server. This image is ideal for production deployments.\nredis/redis-stack: This image includes both the Redis Stack server and RedisInsight, making it perfect for local development as you can leverage the integrated RedisInsight tool to visualize and manage your data.\n\nRedis provides detailed documentation on how to download and use these images, which can be found at the following link: Redis Docker Installation Guide.\nIn our case, we’ll go with the redis/redis-stack image to take advantage of RedisInsight, which will make it easier to debug and monitor the data flow during the development process.\n\n\n\n\n\n\nRedisInsight\n\n\n\n\n\nRedisInsight is a graphical user interface (GUI) tool for Redis that enables you to visualize, interact with, and manage data stored in a Redis database. It provides an intuitive way to explore datasets, execute queries, inspect keys, monitor performance, and perform various data management tasks.\n\n\n\nTo download and run the Redis container, we will execute the following command in the terminal:\ndocker run --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\nThis command instructs Docker to create and start a container using the redis/redis-stack:latest image. The --name redis-stack flag assigns the container a recognizable name (redis-stack) for easy management. The -p flags map ports between the container and the host machine, with port 6379 used by Redis for database connections and port 8001 used by RedisInsight, a web-based UI for managing and visualizing Redis data.\nWhen you execute this command, Docker will first check if the redis/redis-stack:latest image is available locally. If it isn’t found, Docker will proceed to download the image from the Docker registry, as shown in Figure 7.\nIt’s important to note that the Docker engine must be running on your system for this command to execute properly.\n\n\n\n\n\n\nFigure 7: Downloading and running the redis-stack image\n\n\n\nAfter the image is successfully downloaded, Docker will automatically start the container. At this point, Redis will be up and running, accessible through port 6379, and RedisInsight will be available through port 8001, ready to provide a graphical interface for managing Redis.\n\n\n\n\n\n\nManaging Docker containers\n\n\n\n\n\nTo view information about all the containers on your system, regardless of their current state (whether they are running or not), you can use the following command in the terminal:\ndocker ps -a\nThis command lists all containers on your system and provides details such as their names, the images they are based on, their creation time, and their current status.\n\n\n\n\n\n\nFigure 8: docker ps -a output\n\n\n\nFigure 8 shows an example of the output you’ll see after running this command. As you can see, the output of this command includes several columns with key information about each container:\n\nCONTAINER ID: A unique identifier for the container, usually shown as a short alphanumeric string (e.g., 72cb722bfa5d).\nIMAGE: The Docker image that the container is based on, such as redis/redis-stack:latest.\nCOMMAND: This column shows the command that is being executed inside the container once it starts running. For example, for a Flask application, this could be flask run.\nCREATED: The timestamp indicating when the container was created.\nSTATUS: The current state of the container, such as running, exited, or paused.\nPORTS: The ports exposed by the container and the corresponding ports on the host machine. For example, in the case of the redis-stack container, you can see 0.0.0.0:6379-&gt;6379/tcp. This means that port 6379 inside the container is mapped to port 6379 on the host machine, allowing external access to the container’s Redis service through that port.\nNAMES: The names assigned to the containers. If you didn’t specify a name when creating the container, Docker will automatically assign a random name (e.g., gracious_tesla). If you did specify a name, it will appear here (e.g., redis-stack).\n\nWith the information provided by docker ps -a, you can manage the state of your containers using the following commands:\n\nStart a Container: To start a stopped container, you can use the docker start command followed by the container name or ID. For example, to start the redis-stack container, you would run the following command:\ndocker start redis-stack\nStop a Container: To stop a running container, use the docker stop command. For example, to stop the redis-stack container, you would run the following command:\ndocker stop redis-stack\n\n\n\n\n\nAccessing RedisInsight\nWith Redis and RedisInsight running, we can now access RedisInsight’s graphical interface by navigating to http://localhost:8001/.\nUpon visiting this URL, the first thing we will need to do is accept the terms of service and configure the privacy settings for RedisInsight, as shown in Figure 9.\n\n\n\n\n\n\nFigure 9: Accepting RedisInsight terms and configuring privacy settings\n\n\n\nOnce we have accepted the RedisInsight terms of service, we will be directed to the main page where we can explore our Redis database, as well as create new keys, as shown in Figure 10.\n\n\n\n\n\n\nFigure 10: RedisInsight main page\n\n\n\nOn this page, we can see two main panels. The left panel displays all the keys in our Redis database. Since we haven’t created any keys yet, it’s currently empty. The right panel will allow us to explore the details of any key we click on, displaying the contents stored in that key.\n\n\nConnecting to Redis\nNow that Redis is up and running, it’s time to integrate it with our application. Similar to how we connected to our PostgreSQL database earlier in the project, we need to configure our application to interact with Redis.\nTo ensure that connection details are managed securely and are not hardcoded into our application, we’ll store them in the .env file. To do so, we will open the .env file and add the following lines to specify the Redis connection details:\nREDIS_HOST='localhost'\nREDIS_PORT='6379'  \nNext, we can establish a connection to Redis by using the redis Python library, which provides a simple and efficient interface to interact with the Redis database.\nIf the redis library isn’t already installed, you can add it by running the following command:\npip install redis\nWith the redis library installed and the environment variables already placed into the .env file, we can now import the redis library, retrieve the connection details from the .env file, and establish a connection to the Redis server with the following code:\nimport os\nfrom dotenv import load_dotenv\nimport redis\n\nload_dotenv()\n\n# Fetch Redis connection details from environment variables\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\n\n# Establish connection to Redis\nredis_conn = redis.Redis(host=redis_host, port=redis_port)\nThe redis.Redis() class is a central part of the Redis Python library, acting as an interface to interact with the Redis server. When you create an instance of this class, it establishes a connection to the specified Redis host and port. Allowing to execute Redis commands to manage data within the database.\nTo further organize the logic of connecting to Redis, we can create a custom RedisClient class. This class will handle the connection setup and provide a structured way to interact with Redis.\nclass RedisClient:\n    def __init__(self, host: str, port: int):\n        self.redis_host = host\n        self.redis_port = port\n        self.connection = self._connect()\n\n    def _connect(self) -&gt; redis.Redis:\n        try:\n            # Create a connection to the Redis server\n            return redis.Redis(host=self.redis_host, port=self.redis_port)\n        except redis.ConnectionError as e:\n            print(f\"Failed to connect to Redis: {e}\")\n            raise\nNow that we’ve set up our RedisClient class, we can easily create an instance of it by passing in the connection details that we retrieved from the environment variables. This will initialize the connection to Redis, allowing us to interact with the Redis server.\nTo create the client, we can simply do:\nredis_client = RedisClient(redis_host, int(redis_port))\nWith redis_client now initialized, we can use redis_client.connection to perform any Redis operations, such as setting or getting data.\n\n\n\n\n\n\nLogging\n\n\n\nLogging is the process of recording information about events or actions that occur during the execution of a program. It gives developers and system administrators insights into the application’s performance and behavior, helping them monitor its status, detect issues, and troubleshoot problems efficiently.\nFor example, in our case, logging allows us to track key details such as when a task is triggered, how many URLs are processed, how long the process takes, and whether any errors occur during execution.\n\nLogging in Python\nPython provides a built-in logging library called logging, which allows us to log messages with varying levels of severity. By default, these messages are displayed in the terminal, but they can also be saved to log files for later analysis. This flexibility allows for detailed tracking, whether it’s for real-time monitoring or for post-execution reviews.\nThe different severity levels help categorize messages according to their importance, allowing us to focus on critical information while filtering out less relevant details. The five standard logging levels are:\n\nDEBUG: Used for detailed, verbose messages that provide insights into the internal state of the program. This level is primarily useful during development and debugging.\nINFO: Used for general messages that indicate the normal operation of the application, such as logging when a task starts or finishes.\nWARNING: Used for potential issues that are not errors but may require attention. These are situations where something could go wrong in the future if left unaddressed.\nERROR: Used for errors that occur during execution, signaling that something has gone wrong, but the program can still continue running.\nCRITICAL: Used for severe issues, indicating that the application may crash or become non-functional without immediate attention.\n\n\nConfiguring the Logging System\nThe Python logging library gives us a lot of control over how log messages are captured and handled. Using the basicConfig method, we can configure important aspects of the logging system, such as which messages should be recorded, their format, and where they should be saved. Unlike print statements, which are primarily for quick debugging and lack flexibility, logging allows for structured and customizable output that can be filtered and redirected as needed.\nFor example, if we want to capture messages of severity INFO and above (which includes INFO, WARNING, ERROR, and CRITICAL), we can set the logging level by using this method as follows:\nlogging.basicConfig(level=logging.INFO)\nOnce we’ve incorporate the logging library, we can enhance the visibility and traceability of our code’s behavior. For instance, in the RedisClient class we just created, we can replace the print statement with a logging call to capture errors in a more structured way. Here’s the improved class with logging:\nimport logging\n\nclass RedisClient:\n    def __init__(self, host: str, port: int):\n        self.redis_host = host\n        self.redis_port = port\n        self.connection = self._connect()\n\n    def _connect(self) -&gt; redis.Redis:\n        \"\"\"Establish a connection to Redis.\"\"\"\n        try:\n            # Create a connection to the Redis server\n            return redis.Redis(host=self.redis_host, port=self.redis_port)\n        except redis.ConnectionError as e:\n            logging.error(f\"Failed to connect to Redis: {e}\")\n            raise\n\n\n\n\n\n\n\nSetting up Celery\nWith Redis successfully set up, the next step is to configure Celery, which will manage our background tasks. Celery will periodically check the Redis queue, process any URLs found, and transfer them to the PostgreSQL database. It will also precompute and cache the top ten most submitted URLs from the PostgreSQL database into Redis.\nTo get started with Celery, we first need to install the Celery library, which we can do by running the following command in a terminal:\npip install celery\nIn addition to Celery, we need a message broker to handle task messaging. Since Redis is already set up as our queue, we will configure Celery to use Redis as its message broker, taking advantage of our current setup and avoiding the need for an additional service.\nNext, we need to set up Celery to run in the background by creating a new file named celery_app.py. In this file, we will define the Celery application, and specify Redis as the broker:\nfrom celery import Celery\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Fetch Redis connection details from environment variables\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\n\n# Define the Celery application and configure it to use Redis as the broker\napp_celery = Celery(broker=f\"redis://{redis_host}:{redis_port}/0\")\nAfter setting up Celery, we can proceed to define the tasks that need to be executed.\n\n\nImproving the Home page\nWith Redis and Celery already set up, we’re now ready to enhance the URL submission on the Home page. As we discussed earlier, we’re moving from directly inserting URLs into the database to enqueuing them in Redis for deferred processing. To implement this change, we’ll begin by replacing the direct database insertion with a Redis queue. Then, we’ll create a Celery task to process and transfer these enqueued URLs into our PostgreSQL database.\n\nReplacing direct database insertion with a Redis queue\nIn the current implementation, each time a user submits a URL, the backend first validates its format to ensure it’s correct. After validation, the URL is immediately inserted into PostgreSQL using the insert_url_into_database function. Below is the original implementation for reference:\ndef insert_url_into_database(url: str) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"INSERT INTO registered_urls (url) VALUES (%s)\", (url,))\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting URL into the database:\", e)\n    finally:\n        conn.close()\nAs previously discussed, the goal is to replace the direct insertion of URLs into PostgreSQL with a Redis queue. To achieve this, we will completely rewrite this function to enqueue the submitted URL into Redis instead of inserting it directly into the PostgreSQL database.\nTo implement this, we’ll use Redis’ RPUSH command. The RPUSH command inserts the specified values at the tail of a list stored at a given key. If the key doesn’t already exist, Redis will create it as an empty list before performing the push operation. In Python, we can execute this command using the rpush method, which is part of the Redis connection object. This connection object can be set up as described in the Connecting to Redis section. The rpush method takes two arguments: the key and the values we want to add to the list. In this case, we’ll name the key to store the urls as url_queue.\nHere’s the updated function that enqueues submitted URLs into the Redis queue:\ndef enqueue_url_to_redis(url: str) -&gt; None:\n    try:\n        # Push the URL to a Redis queue (list)\n        redis_client.connection.rpush(\"url_queue\", url)\n        logging.debug(f\"URL enqueued to Redis: {url}\")\n    except Exception as e:\n        logging.error(\"Error sending URL to Redis queue:\", e)\nNote that we replaced the print statement with a logging call to capture errors in a more structured manner. In addition, we added logging for successful URL enqueueing to Redis, providing better visibility for debugging and tracking the function’s behavior.\nOnce we’ve replaced the insert_url_into_database function with enqueue_url_to_redis, the next step is to update the process_url function. Previously, this function called insert_url_into_database to directly store URLs in PostgreSQL. Now, we’ll modify it to use enqueue_url_to_redis instead:\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        enqueue_url_to_redis(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\nAfter implementing these changes, it’s time to test everything. To accomplish this, we’ll start the application by opening a terminal in the directory where the app.py file is located and running the command flask run. Once the application is running, we can submit some URLs, such as www.google.com and www.facebook.com. Then, using RedisInsight, we’ll verify if the URL has been successfully added to the Redis queue. Specifically, we’ll verify that the url_queue key has been created and that it contains the submitted URL as shown in Figure 11.\n\n\n\n\n\n\nFigure 11: Verifying submitted URLs in RedisInsight\n\n\n\n\n\nImplementing a Celery task for Redis queue processing\nNow that we’ve replaced direct database insertion with a Redis queue, the next step is to create a Celery task that will interact with Redis to process the enqueued URLs. This task will connect to the Redis database, check the queue for any submitted URLs, and, if any are found, batch them and insert them into the PostgreSQL database.\nTo get started, we’ll create a new file named celery_app.py. This file will serve as the central place for all the code related to our Celery tasks.\nOnce the file is created, we can begin implementing our task, which we’ll divide into two main phases: First, we’ll use our Redis connection to check the queue for any enqueued URLs and retrieve them if available. Then, we’ll proceed to batch the retrieved URLs and insert them into the PostgreSQL database. This approach ensures that each part of the task is clear and manageable\n\nExtracting enqueued URLs\nBefore we begin extracting URLs from the Redis queue, we first need to establish a connection to Redis, just as we did for our Flask application. The same steps outlined in the Connecting to Redis section can be followed to set up the Redis connection.\nOnce the connection is in place, we can move on to the process of extracting URLs from the Redis queue. To do so, we will create a function designed to retrieve a specific number of values at a time. We’ll introduce a parameter called n to define the maximum number of values to retrieve.\nIt is important to note that when we say “extracting” URLs from the queue, we don’t just mean retrieving the values; we also need to ensure that they are removed from the queue after extraction. This is crucial to prevent the same URLs from being reprocessed in future tasks. Unfortunately, Redis doesn’t provide a built-in function to both extract and remove multiple items from the queue in a single operation.\nTo work around this, we can combine two Redis commands: LRANGE and LTRIM.\n\nLRANGE: This command allows us to retrieve a range of elements from the Redis list (queue). To use it, we need to specify three parameters:\n\nThe key of the list.\nThe starting index from which to begin extraction.\nThe ending index where extraction should stop.\n\nRedis lists maintain the order of elements, with items added according to the command used. In our case, we add URLs using RPUSH, which places new elements at the end of the list. As a result, the element at index 0 is always the oldest. To retrieve the n oldest URLs, we extract elements starting from index 0 up to n-1.\nIf the queue was populated using LPUSH, however, the item at index 0 would be the most recent, and we would extract accordingly.\nLTRIM: This command allows us to retrieve a range of elements from the Redis list (queue), so that it will contain only the specified range of elements. To use it, we need to specify three parameters:\n\nThe key of the list.\nThe starting index from which to begin extraction.\nThe ending index where extraction should stop.\n\nAfter extracting the processed items with LRANGE, we will use LTRIM to remove them from the list. Therefore, we will trim the list to include the elements starting from index n (the first unprocessed item) to -1 (the last item in the list).\n\nIn Python, we can execute these commands, just as we did with RPUSH, using a Redis connection and the corresponding methods: lrange and ltrim.\nIn a multi-worker environment, there is a risk of concurrency issues when executing LRANGE and LTRIM as separate commands. Other workers might modify the queue in between these operations—either adding new URLs or removing existing ones—leading to race conditions, incorrect deletions, or data inconsistencies.\nTo prevent such issues, Redis provides pipelining, which allows multiple commands to be executed in a single batch. This ensures that commands like LRANGE and LTRIM are processed together, minimizing the risk of interference from other workers.\nIn Python, we can implement pipelining using the pipeline method of a Redis connection. This method creates a pipeline object that allows us to queue multiple Redis commands and execute them all at once.\n\n\n\n\n\n\nUsing Redis pipelines in python\n\n\n\n\n\nTo start using a pipeline, we first need to create a pipeline object using the pipeline method from the Redis client. This object will queue the commands to be executed later.\nredis_conn = redis.Redis(host=redis_host, port=redis_port)\n\npipeline = redis_conn.pipeline()\nOnce the pipeline is created, we can add Redis commands to it. This is done by calling the appropriate Redis command method on the pipeline object, just like we would with a regular Redis command. For example, to add the LRANGE and LPUSH commands:\npipeline.lrange(\"url_queue\", 0, n-1)\n        \npipeline.ltrim(\"url_queue\", n, -1)\nOnce all the commands have been added to the pipeline, we can execute them by calling the execute method of the pipeline.\npipeline.execute()\n\n\n\nThe result returned by the pipeline execution will be a list where:\n\nThe first element is a list of extracted URLs (returned by LRANGE).\nThe second element is a boolean indicating whether LTRIM executed successfully.\n\nUsing this result, we can verify if the queue was properly trimmed by checking the second element. If the trim operation was successful, it confirms that all the extracted elements were removed from the queue. In that case, we can proceed to return the list of extracted URLs (the first element). Since Redis stores strings as bytes, each URL needs to be decoded from its byte format to a regular string using the .decode method. If the trim operation was unsuccessful, we will return an empty list instead.\nWith all this in mind, the implementation of the dequeue_urls_from_redis function will look as follows:\ndef dequeue_urls_from_redis(n: int) -&gt; List[str]:\n    try:\n        # Start a Redis pipeline to batch operations\n        url_dequeue_pipeline = redis_client.connection.pipeline()\n        \n        # Extract the specified range of values from the queue\n        url_dequeue_pipeline.lrange(\"url_queue\", 0, n-1)\n        \n        # Trim the queue to remove the processed values\n        url_dequeue_pipeline.ltrim(\"url_queue\", n, -1)\n        \n        # Execute the batch operations\n        urls = url_dequeue_pipeline.execute()\n        \n        # If queue was trimmed, decode the extracted URLs from bytes to strings\n        if urls[1]:\n            registered_urls = [url.decode() for url in urls[0]]\n        else:\n            registered_urls = []\n        \n        return registered_urls\n    \n    except Exception as e:\n        logging.error(f\"Failed to dequeue URLs from Redis: {e}\")\n\n\nBatching and storing dequeued URLs to PostgreSQL\nOnce we’ve connected to Redis and retrieved the URLs from the queue, the next step is to insert them into our PostgreSQL database. Specifically, we need to store these URLs in the registered_urls table.\nWe can reuse the insert_url_into_database function we previously defined in our app.py:\ndef insert_url_into_database(url: str) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"INSERT INTO registered_urls (url) VALUES (%s)\", (url,))\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting URL into the database:\", e)\n    finally:\n        conn.close()\nWhile this function works for inserting a single URL, it becomes inefficient when we need to process a batch of URLs. The reason for this inefficiency is that the function inserts each URL individually, which leads to multiple database interactions, increasing overhead.\nSince we are using a queue system to avoid overwhelming the PostgreSQL database with excessive requests, inserting each URL one by one would go against this principle by adding unnecessary load to the database.\nTo address this, we need to refactor the function to support batch insertions. Instead of inserting one URL at a time, we will modify the function to accept a list of URLs. We will also switch from using execute to executemany, which allows us to execute a single query that inserts all URLs in one operation. This significantly reduces the load on the database. Additionally, we will use a list comprehension to format the URLs into a structure suitable for executemany. Finally, we will replace the print statement for errors with proper logging and we will add a log message to confirm how many URLs have been successfully inserted into the database.\nHere is the updated version of the function:\nfrom typing import List\n\ndef insert_urls_to_database(urls: List[str]) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.executemany(\"INSERT INTO registered_urls (url) VALUES (%s)\", [(url,) for url in urls])\n        conn.commit()\n        logging.info(f\"Inserted {len(urls)} URLs into the database.\")\n    except Exception as e:\n        logging.error(\"Error inserting into the database:\", e)\n    finally:\n        conn.close()\nIt’s important to note that we also need to include the connect_to_database function along with the necessary PostgreSQL connection details into our celery_app.py, as these are required for the insert_urls_to_database function.\n\n\nUsing these functions to implement a Celery task for dequeuing and batching submitted URLs\nAfter having created all the functions to perform our task, we can proceed to create our Celery task. Specifically, our Celery task will do the following:\n\nExtract up to 150 URLs from the Redis queue using dequeue_urls_from_redis. We limit the batch size to 150 URLs for several important reasons. If the queue contains thousands of URLs and we try to process them all at once, it could overwhelm the system. This would result in high memory usage, slower processing speeds, and excessive strain on both Redis and the database. By processing the URLs in smaller batches, we can reduce the load on these systems, helping to maintain more efficient use of resources and improve overall performance.\nCheck if any URLs were retrieved.\nIf URLs are available, insert them into the PostgreSQL database using insert_urls_to_database.\n\nTo implement this, we will define a function that encapsulates these steps, which we will name process_queued_urls. However, simply defining this function is not enough for Celery to recognize and manage it as a task.\nIn Celery, tasks are registered using the @app_celery.task decorator. Here, app_celery is our Celery application instance. By applying this decorator to our function, we are telling Celery that this function should be executed asynchronously as part of a distributed task queue.\nBelow is our complete Celery task definition:\n@app_celery.task\ndef process_queued_urls() -&gt; None:\n    logging.info(\"Started processing queued URLs.\")\n    dequeued_urls = dequeue_urls_from_redis(150)\n    \n    if dequeued_urls:\n        insert_urls_to_database(dequeued_urls)\n    logging.info(\"Finished processing queued URLs.\")\nIn addition, to improve the observability, we have added log messages to track the status of the task.\nNow that we’ve defined the task function, we need to configure it to run at regular intervals. To achieve this, we will use Celery beat, which is a scheduler. Celery Beat allows us to schedule periodic tasks, so we can automate the process of running the process_queued_urls function at specific times.\nThe scheduling of tasks is configured through the beat_schedule setting. This is a dictionary within the Celery configuration (app_celery.conf.beat_schedule) where we define tasks and their corresponding execution intervals. Each entry in this dictionary represents one task, and for each task, we need to specify its:\n\nTask: The task that Celery should execute periodically. It’s important to specify the full path to the task, which includes both the module (or file) name and the function name. This ensures Celery can locate and execute the task correctly.\nSchedule: The interval at which the task should run, specified in seconds.\n\nFor our example, the task we want to run periodically is process_queued_urls, and we want it to execute every 5 minutes. Since this task is defined in the celery_app module, we specify the full path as celery_app.process_queued_urls. The schedule value will be set to 300 seconds (since 5 minutes = 300 seconds). With this in mind, we can configure the beat_schedule in the following way:\napp_celery.conf.beat_schedule = {\n    'dequeue-urls-every-5-mins': {\n        'task': 'celery_app.process_queued_urls',\n        'schedule': 300.0,\n    },\n}\nBelow, you can expand the code to see how celery_app.py looks up to this point.\n\n\ncelery_app.py up to this point\nfrom celery import Celery\nimport os\nimport redis\nfrom dotenv import load_dotenv\nfrom typing import List\nimport psycopg\nimport logging\n\nload_dotenv()\n\n# Fetch Redis connection details from environment variables\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\n\nclass RedisClient:\n    def __init__(self, host: str, port: int):\n        self.redis_host = host\n        self.redis_port = port\n        self.connection = self._connect()\n\n    def _connect(self) -&gt; redis.Redis:\n        \"\"\"Establish a connection to Redis.\"\"\"\n        try:\n            # Create a connection to the Redis server\n            return redis.Redis(host=self.redis_host, port=self.redis_port)\n        except redis.ConnectionError as e:\n            logging.error(f\"Failed to connect to Redis: {e}\")\n            raise\n          \nredis_client = RedisClient(redis_host, int(redis_port))\n\n# Fetch PostgreSQL connection details from environment variables\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(e)\n\n# Define the Celery application and configure it to use Redis as the broker\napp_celery = Celery(broker=f\"redis://{redis_host}:{redis_port}/0\")\n\napp_celery.conf.beat_schedule = {\n    'dequeue-urls-every-5-mins': {\n        'task': 'celery_app.process_queued_urls',\n        'schedule': 300.0,\n    },\n}\n\ndef dequeue_urls_from_redis(n: int) -&gt; List[str]:\n    try:\n        # Start a Redis pipeline to batch operations\n        url_dequeue_pipeline = redis_client.connection.pipeline()\n        \n        # Extract the specified range of values from the queue\n        url_dequeue_pipeline.lrange(\"url_queue\", 0, n-1)\n        \n        # Trim the queue to remove the processed values\n        url_dequeue_pipeline.ltrim(\"url_queue\", n, -1)\n        \n        # Execute the batch operations\n        urls = url_dequeue_pipeline.execute()\n        \n        # If queue was trimmed, decode the extracted URLs from bytes to strings\n        if urls[1]:\n            registered_urls = [url.decode() for url in urls[0]]\n        else:\n            registered_urls = []\n        \n        return registered_urls\n    \n    except Exception as e:\n        # Handle potential errors\n        logging.error(f\"Failed to dequeue URLs from Redis: {e}\")\n        \ndef insert_urls_to_database(urls: List[str]) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.executemany(\"INSERT INTO registered_urls (url) VALUES (%s)\", [(url,) for url in urls])\n        conn.commit()\n    except Exception as e:\n        print(\"Error inserting into the database:\", e)\n    finally:\n        conn.close()\n        \n@app_celery.task\ndef process_queued_urls() -&gt; None:\n    logging.info('Started processing queued URLs.')\n    \n    dequeued_urls = dequeue_urls_from_redis(150)\n    \n    if dequeued_urls:\n        logging.info(f'Retrieved {len(dequeued_urls)} URLs from Redis.')\n        insert_urls_to_database(dequeued_urls)\n        logging.info(f'Inserted dequeued URLs into the PostgreSQL database.')\n    else:\n        logging.info('No URLs retrieved from Redis.')\n    \n    logging.info('Finished processing queued URLs.')\n\n\n\n\nExecuting Celery tasks\nNow that we have set up Celery and created our task, we can move forward with executing it to be sure all works as expected.\nCelery tasks are executed by a worker, a process that listens for incoming tasks and runs them when triggered. In our case, the worker will listen for the process_queued_urls task.\nTo start the Celery worker, we need to run the following command in a terminal from the directory where celery_app.py is located:\ncelery -A celery_app worker --loglevel=info\nThis command is made up of several parts that work together. First, celery is the command-line tool used to interact with Celery. The -A celery_app flag specifies which application to use, in this case, the celery_app module where we’ve defined our Celery app. By adding worker, we indicate that we want to start the worker process, which listens for and executes tasks when they’re triggered. The --loglevel=info flag sets the logging level to “info,” providing detailed output in the terminal to track the worker’s activities.\nIn addition to the Celery worker, we also need to start Celery Beat as we’ve configured a periodic task process_queued_urls to run every 5 minutes (300 seconds) in the beat_schedule of celery_app.py. Celery Beat will ensure that the periodic task is executed according to the schedule.\nThis is done by running the following command in a separate terminal window from the same directory:\ncelery -A celery_app beat --loglevel=info\nLike the worker command, this also uses -A celery_app to specify the application, but here we use beat instead of worker to indicate that we want to start the Celery Beat process. This will manage our periodic tasks. The --loglevel=info flag again ensures that we see detailed logging information in the terminal, so we can monitor what’s happening.\nFigure 12 shows the execution of both the Celery worker and Celery Beat. The left terminal window shows the output of the command celery -A celery_app worker --loglevel=info, indicating that the Celery worker is running and ready to process tasks. The right terminal window displays the output of the command celery -A celery_app beat --loglevel=info, showing that Celery Beat is active and scheduling the periodic task process_queued_urls according to the defined schedule.\n\n\n\n\n\n\nFigure 12: Starting Celery worker and scheduler for task execution\n\n\n\nOnce both the Celery worker and Celery Beat are running, the process_queued_urls task will be executed every 5 minutes as configured, and the worker will process any incoming tasks.\nIn Figure 13, we can see a side-by-side view of the graphical interfaces for both our PostgreSQL database (on the left) and our Redis database (on the right). Initially, the registered_urls table in PostgreSQL contains three URLs: www.google.com, www.facebook.com, and www.google.com. At the same time, in Redis, the queue with the key url_queue holds two URLs: www.google.com, and www.facebook.com.\nWhen the process_queued_urls task is triggered, as shown in the red-highlighted section of the terminal, the task processes the URLs in Redis. After this, if we refresh the Redis database, the url_queue key disappears because all the URLs in the queue have been dequeued and inserted into the registered_urls table in PostgreSQL. After refreshing the PostgreSQL database, we now see that it contains five URLs instead of the original three, including the URLs that were previously in the Redis queue.\n\n\n\n\n\n\nFigure 13: Watching Redis Queue Empty as URLs Are Moved to PostgreSQL\n\n\n\n\n\n\n\nImproving the Top Favorite Websites page\nHaving implemented the improvements for the Home page, we now proceed to optimize the Top Favorite Websites page. As previously discussed, the goal is to move away from querying the PostgreSQL database in real time and instead precompute and cache the results for the top ten favorite websites.\nTo implement this change, we will begin by creating a Celery task that will run periodically to compute and cache the top ten favorite websites. Once this caching system is in place, we will update the Flask application to fetch the data from Redis, bypassing the need to query PostgreSQL for each user request.\n\nCreating a Celery task to cache the top ten favorite websites\nThe first step in this process is to set up the cache. We’ll create a Celery task that will execute every thirty minutes. This task will perform two main actions. First, it will connect to the PostgreSQL database to count the occurrences of each submitted URL, sort these counts in descending order, and identify the top ten most frequent URLs. Next, the task will store these results in Redis to ensure fast and efficient access for future requests.\n\nExtracting the top ten favorite websites from our PostgreSQL\nCurrently, the process of extracting the top ten favorite websites from the PostgreSQL database is already implemented in our Flask application through the get_top_registered_urls function. This function connects to the database and queries it to retrieve the top favorite websites.\nTo recap, below is the existing implementation:\ndef get_top_registered_urls() -&gt; List[Tuple[str, int]] | None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"\"\"\n            SELECT url, COUNT(*) as count\n            FROM registered_urls\n            GROUP BY url\n            ORDER BY count DESC\n            LIMIT 10;\n        \"\"\")\n        top_registered_urls = cur.fetchall()\n        return top_registered_urls\n    except Exception as e:\n        print(f\"Error retrieving the URLs: {e}\")\n    finally:\n        if conn:\n            conn.close()\nSince we are transitioning to a background caching strategy, we will move this function from the Flask application (app.py) to the Celery application (celery_app.py).\n\n\nCaching the top ten favorite websites into Redis\nAfter extracting the top ten favorite websites from PostgreSQL, we need to store them in our Redis cache. To achieve this, we will create a function called save_popular_to_redis, which will take the output of get_top_registered_urls as its only argument, and it store it in Redis.\nSince get_top_registered_urls returns a list of tuples containing URLs and their respective counts (e.g., [('www.google.com', 4), ('www.facebook.com', 4), ('www.test.com', 1)]), we first need to convert this structure into a dictionary.\nBefore storing this data in Redis, we must first convert it into a dictionary. This is necessary because Redis supports different data structures, and for our use case, a hash (or hashmap) is the most appropriate. Hashes in Redis allow us to store multiple field-value pairs under a single Redis key, making it easy to retrieve and manipulate individual entries. In this case we will store the URLs and counts as field-value pairs\nTo store this dictionary in Redis, we will use the HSET command, which sets multiple fields and values in a hash stored at a specific key. If the specified key does not exist, Redis will automatically create a new hash. If the key already exists, the HSET command will update the fields within the hash, leaving any fields that are not included in the update unchanged.\nTo perform this operation, we will reuse the Redis connection we created earlier. We will call the hset method with two parameters: the key name (in this case, \"top_favorite_websites\") and the dictionary containing the URLs and their corresponding counts.\nWhen using the HSET command to store the data, it is important to understand its behavior: HSET updates only the fields that are included in the data provided. If any field in the hash already exists but is not included in the update, it will not be modified and will remain in the hash. This means that fields which are not updated will persist, even if they no longer need to be in the hash.\nHowever, in our case, we do not want to retain any previous data that’s not part of the updated dictionary of URLs and counts. To ensure that only the current top ten favorite websites are stored, we must delete the existing Redis key (if it exists) before calling HSET. This can be done using the delete method of our Redis connection, which will remove the old data and ensure that only the new data is stored.\nBelow is the function implementation:\nfrom typing import List, Tuple\n\ndef save_popular_to_redis(registered_urls: List[Tuple[str, int]]):\n    try:\n        registered_urls_dict = dict(registered_urls)\n        \n        redis_client.connection.delete(\"top_favorite_websites\")\n        \n        redis_client.connection.hset(\"top_favorite_websites\", mapping=registered_urls_dict)\n        \n        logging.info(\"Successfully cached top favorite websites in Redis.\")\n    except Exception as e:\n        logging.error(f\"Error when caching top favorite websites: {e}\")\n\n\nUsing these functions to implement a Celery task for caching top favorite websites\nOnce we have implemented the necessary functions, we can proceed with creating a Celery task to automate the caching of the top favorite websites. The Celery task will perform the following steps:\n\nRetrieve the top ten most popular websites from the PostgreSQL database using the get_top_registered_urls function.\nStore the retrieved websites in Redis using the save_popular_to_redis function.\n\nTo achieve this, we define a Celery task named cache_top_favorite_websites, which encapsulates these steps:\n@app_celery.task\ndef cache_top_favorite_websites() -&gt; None:\n    logging.info(\"Started caching top favorite websites.\")\n\n    top_urls = get_top_registered_urls()\n    \n    if top_urls:\n        logging.info(f\"Retrieved top URLs from PostgreSQL.\")\n        \n        save_popular_to_redis(top_urls)\n        logging.info(\"Successfully cached top favorite websites.\")\n    else:\n        logging.info(\"No URLs retrieved from PostgreSQL, skipping caching.\")\n\n    logging.info(\"Finished caching process.\")\nJust like the previous task we created, we want this task to periodically, i.e., every 30 minutes. To accomplish this, we need to add it to the Celery beat schedule so that it executes every 30 minutes (1800 seconds).\nHere’s the updated beat schedule configuration:\napp_celery.conf.beat_schedule = {\n    'cache-top-favorite-websites-every-30-mins': {\n        'task': 'celery_app.cache_top_favorite_websites',\n        'schedule': 1800.0,  # 30 minutes\n    },\n    'dequeue-urls-every-5-mins': {\n        'task': 'celery_app.process_queued_urls',\n        'schedule': 300.0,  # 5 minutes\n    },\n}\nTo see how all these pieces fit together, you can unfold the code below for the full celery_app.py file.\n\n\nfull celery_app.py\nfrom celery import Celery\nimport os\nimport redis\nfrom dotenv import load_dotenv\nfrom typing import List, Tuple\nimport psycopg\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\nload_dotenv()\n\n# Fetch Redis connection details from environment variables\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\n\nclass RedisClient:\n    def __init__(self, host: str, port: int):\n        self.redis_host = host\n        self.redis_port = port\n        self.connection = self._connect()\n\n    def _connect(self) -&gt; redis.Redis:\n        \"\"\"Establish a connection to Redis.\"\"\"\n        try:\n            # Create a connection to the Redis server\n            return redis.Redis(host=self.redis_host, port=self.redis_port)\n        except redis.ConnectionError as e:\n            logging.error(f\"Failed to connect to Redis: {e}\")\n            raise\n          \nredis_client = RedisClient(redis_host, int(redis_port))\n\n# Fetch PostgreSQL connection details from environment variables\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        logging.error(f\"Database connection failed: {e}\")\n\n# Define the Celery application and configure it to use Redis as the broker\napp_celery = Celery(broker=f\"redis://{redis_host}:{redis_port}/0\")\n\napp_celery.conf.beat_schedule = {\n    'cache-top-favorite-websites-every-30-mins': {\n        'task': 'celery_app.cache_top_favorite_websites',\n        'schedule': 1800.0,  # 30 minutes\n    },\n    'dequeue-urls-every-5-mins': {\n        'task': 'celery_app.process_queued_urls',\n        'schedule': 300.0,  # 5 minutes\n    },\n}\n\ndef dequeue_urls_from_redis(n: int) -&gt; List[str]:\n    try:\n        # Start a Redis pipeline to batch operations\n        url_dequeue_pipeline = redis_client.connection.pipeline()\n        \n        # Extract the specified range of values from the queue\n        url_dequeue_pipeline.lrange(\"url_queue\", 0, n-1)\n        \n        # Trim the queue to remove the processed values\n        url_dequeue_pipeline.ltrim(\"url_queue\", n, -1)\n        \n        # Execute the batch operations\n        urls = url_dequeue_pipeline.execute()\n        \n        # If queue was trimmed, decode the extracted URLs from bytes to strings\n        if urls[1]:\n            registered_urls = [url.decode() for url in urls[0]]\n        else:\n            registered_urls = []\n        \n        return registered_urls\n    \n    except Exception as e:\n        # Handle potential errors\n        logging.error(f\"Failed to dequeue URLs from Redis: {e}\")\n        \ndef insert_urls_to_database(urls: List[str]) -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.executemany(\"INSERT INTO registered_urls (url) VALUES (%s)\", [(url,) for url in urls])\n        conn.commit()\n        logging.info(f\"Inserted {len(urls)} URLs into the database.\")\n    except Exception as e:\n        logging.error(f\"Error inserting into the database: {e}\")\n    finally:\n        conn.close()\n    \ndef get_top_registered_urls() -&gt; List[Tuple[str, int]] | None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"\"\"\n            SELECT url, COUNT(*) as count\n            FROM registered_urls\n            GROUP BY url\n            ORDER BY count DESC\n            LIMIT 10;\n        \"\"\")\n        top_registered_urls = cur.fetchall()\n        return top_registered_urls\n    except Exception as e:\n        print(f\"Error retrieving the URLs: {e}\")\n    finally:\n        if conn:\n            conn.close()\n            \ndef save_popular_to_redis(registered_urls: List[Tuple[str, int]]):\n    try:\n        registered_urls_dict = dict(registered_urls)\n        \n        redis_client.connection.delete(\"top_favorite_websites\")\n        \n        redis_client.connection.hset(\"top_favorite_websites\", mapping=registered_urls_dict)\n        \n        logging.info(\"Successfully cached top favorite websites in Redis.\")\n    except Exception as e:\n        logging.error(f\"Error when caching top favorite websites: {e}\")\n        \n@app_celery.task\ndef process_queued_urls() -&gt; None:\n    logging.info('Started processing queued URLs.')\n    \n    dequeued_urls = dequeue_urls_from_redis(150)\n    \n    if dequeued_urls:\n        logging.info(f'Retrieved {len(dequeued_urls)} URLs from Redis.')\n        insert_urls_to_database(dequeued_urls)\n        logging.info(f'Inserted dequeued URLs into the PostgreSQL database.')\n    else:\n        logging.info('No URLs retrieved from Redis.')\n    \n    logging.info('Finished processing queued URLs.')\n    \n@app_celery.task\ndef cache_top_favorite_websites() -&gt; None:\n    logging.info(\"Started caching top favorite websites.\")\n\n    top_urls = get_top_registered_urls()\n    \n    if top_urls:\n        logging.info(f\"Retrieved top URLs from PostgreSQL.\")\n        \n        save_popular_to_redis(top_urls)\n        logging.info(\"Successfully cached top favorite websites.\")\n    else:\n        logging.info(\"No URLs retrieved from PostgreSQL, skipping caching.\")\n\n    logging.info(\"Finished caching process.\")\n\n\nWe are now ready to test the newly implemented task. To do this, we need to restart both the Celery worker and Celery Beat. If they are still running from the previous session, we should stop them first. Afterward, we can restart them by executing the following commands in two separate terminal windows. In the first terminal, navigate to the directory where celery_app.py is located and run celery -A celery_app worker --loglevel=info. Then, in the second terminal, run celery -A celery_app beat --loglevel=info. This will restart both components and prepare them for testing.\nAfter 30 minutes, once the cache_top_favorite_websites task has finished running, we can use RedisInsight to check the Redis database. As shown in Figure 14, we will find a new key named top_favorite_websites. Clicking on this key will display its contents in the right panel of RedisInsight. Specifically, it will show a Hash, which contains the data for the top favorite websites. In this case, we will see two URLs: www.google.com with a count of 3, and www.facebook.com with a count of 2, as we have registered only two unique URLs.\n\n\n\n\n\n\nFigure 14: Viewing the top_favorite_websites Key in RedisInsight\n\n\n\n\n\n\nReplacing direct database queries with Redis cache retrieval\nNow, we have a cached version of our popular URLs table. Therefore, we need to modify our code in app.py so that it no longer retrieves the popular URLs table from our PostgreSQL database, but instead fetches it from our Redis cache.\nwe’ll create a function called retrieve_favorite_websites_from_redis. This function will be responsible for retrieving all the key-value pairs stored in the Hash under the top_favorite_websites key in Redis. We can easily achieve this by using Redis’s HGETALL command, which fetches all key-value pairs from a specific hash in Redis. We only need to provide the key (in this case, top_favorite_websites) where the hash is stored, and Redis will return the entire hash.\nIn Python, we can execute this by calling the hgetall method on a Redis connection object. This will return the key-value pairs in the form of a Python dictionary. Since we originally represented the table as a list of tuples, we will then convert this dictionary into a list of tuples for easier handling in our code.\nBelow is the resulting code to implement this:\ndef retrieve_favorite_websites_from_redis() -&gt; List[Tuple[str, int]] | None:\n    try:\n        favorite_websites = redis_client.connection.hgetall(\"top_favorite_websites\")\n\n        favorite_websites = [(url.decode(), int(count)) for url, count in favorite_websites.items()]\n\n        return favorite_websites\n    except Exception as e:\n        logging.error(f\"Error retrieving favorite websites from Redis: {e}\")\nNow, we just need to call this function each time a user visits the Top Favorite Websites page, instead of calling get_top_registered_urls as we did before.\nAfter applying these changes, you can see how the app.py file would look by unfolding the code below.\n\n\nupdated app.py\nfrom flask import Flask, request, render_template, redirect, url_for\nimport os\nfrom dotenv import load_dotenv\nimport psycopg\nimport re \nimport redis\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\nload_dotenv()\n\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\n\nclass RedisClient:\n    def __init__(self, host: str, port: int):\n        self.redis_host = host\n        self.redis_port = port\n        self.connection = self._connect()\n\n    def _connect(self) -&gt; redis.Redis:\n        \"\"\"Establish a connection to Redis.\"\"\"\n        try:\n            # Create a connection to the Redis server\n            return redis.Redis(host=self.redis_host, port=self.redis_port)\n        except redis.ConnectionError as e:\n            logging.error(f\"Failed to connect to Redis: {e}\")\n            raise\n          \nredis_client = RedisClient(redis_host, int(redis_port))\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        logging.error(f\"Database connection failed: {e}\")\n\ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\ndef enqueue_url_to_redis(url: str) -&gt; None:\n    try:\n        # Push the URL to a Redis queue (list)\n        redis_client.connection.lpush(\"url_queue\", url)\n        logging.debug(f\"URL enqueued to Redis: {url}\")\n    except Exception as e:\n        logging.error(f\"Error sending URL to Redis queue: {e}\")\n\ndef process_url(url: str) -&gt; str:\n    if is_valid_url(url):\n        enqueue_url_to_redis(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\ndef retrieve_favorite_websites_from_redis() -&gt; List[Tuple[str, int]] | None:\n    try:\n        favorite_websites = redis_client.connection.hgetall(\"top_favorite_websites\")\n\n        favorite_websites = [(url.decode(), int(count)) for url, count in favorite_websites.items()]\n\n        return favorite_websites\n    except Exception as e:\n        logging.error(f\"Error retrieving favorite websites from Redis: {e}\")\n\napp = Flask(__name__)\n\n@app.cli.command(\"init-db-table\")\ndef create_table() -&gt; None:\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        logging.info(\"Database table initialized successfully.\")\n    except Exception as e:\n        logging.error(f\"Error initializing database table: {e}\")\n    finally:\n        conn.close()\n\n@app.route(\"/\", methods=['GET', 'POST'])\ndef home():\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\n@app.route(\"/display_url/&lt;path:url&gt;\", methods=['GET', 'POST'])\ndef display_url(url: str | None = None):\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\n@app.route(\"/popular\", methods=['GET'])\ndef top_favorite_websites_page():\n    top_registered_urls = retrieve_favorite_websites_from_redis()\n    return render_template('popular.html', top_registered_urls=top_registered_urls)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n\nAfter implementing this change in our application, we can verify that the Top Favorite Websites table is correctly retrieved from the Redis cache by rerunning the application. To do this, we will proceed to open a terminal window in the directory where our app.py is located and run the flask run command.\nThis will start our Flask application. Once the application is running, we will navigate to the Top Favorite Websites page. As shown in Figure 15, we should see the Top Favorite Websites table displayed correctly. This confirms that the data is now being successfully retrieved from the Redis cache.\n\n\n\n\n\n\nFigure 15"
  },
  {
    "objectID": "posts/2025/improving-architecture/index.html#organizing-our-codebase",
    "href": "posts/2025/improving-architecture/index.html#organizing-our-codebase",
    "title": "Data-driven web applications basics: Improving the architecture",
    "section": "Organizing our codebase",
    "text": "Organizing our codebase\nAs applications evolve, they become more complex, and managing the various components within them can become increasingly difficult. In the case of our Celery and Flask applications, both have been structured as single files that contain all their relevant functionality. The Celery file, for example, manages configurations, Redis interactions, database operations, and task logic, while the Flask file handles routes, data validation, database operations, and application logic.\nWhile this monolithic structure can work initially, it quickly leads to challenges as the application grows. The addition of more tasks, routes, configurations, and utility functions results in a cluttered codebase that becomes harder to navigate, debug, and extend. To ensure that our application remains scalable, maintainable, and easy to debug, it’s essential to refactor the code.\nRefactoring involves breaking the application down into smaller, modular components, each with a clear and distinct responsibility. This approach allows us to better organize the code, reduce duplication, and simplify future updates and troubleshooting.\n\nCentralizing shared logic\nA crucial step in this refactoring process is identifying shared functionality between the Celery and Flask applications. Currently, both applications contain redundant pieces of logic that perform similar tasks. This duplication creates inefficiencies and increases the risk of inconsistencies. For instance, if a change is made in one part of the code, we may forget to update the other, leading to errors.\nTo address the issue of redundant code, we can centralize shared functionality into a single, dedicated location. The first step in refactoring our codebase is to identify and extract common elements that are used across both the Celery and Flask applications. Upon reviewing the code, we can see several shared components. For instance, the .env file for environment variables, and the connection methods for PostgreSQL and Redis databases. Instead of maintaining these functions separately in each application, we will consolidate them into a new folder, which we will call shared.\nThis shared folder will house all the common logic, making the code more maintainable and reducing duplication. We will start by moving the .env file into this folder and create two new Python files to manage the database connections: db.py for PostgreSQL and redis.py for Redis. These files will contain the necessary functions for connecting to the respective databases, ensuring each file is responsible for a specific service, which keeps our code modular and organized.\nBelow is the code for the db.py file, which will handle the connection to the PostgreSQL database. It loads the necessary environment variables and provides a function for connecting to the database.\nimport os\nimport psycopg\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndb_user = os.getenv('DB_USER')\ndb_password = os.getenv('DB_PASSWORD')\ndb_host = os.getenv('DB_HOST')\ndb_port = os.getenv('DB_PORT')\ndb_name = os.getenv('DB_NAME')\n\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(f\"Database connection error: {e}\")\n\n\n\n\n\n\nDocumenting code\n\n\n\n\n\nAs we enhance our codebase, one important aspect to prioritize alongside refactoring and improving code structure is documentation. Effective documentation is not limited to writing comments; it involves providing clear, accessible, and useful information that allows anyone interacting with the code, including our future selves, to understand it and use it effectively.\nGood documentation should clearly explain what each function does, how it should be used, and what input it expects, so others can easily follow and utilize the code without needing to decipher its purpose.\n\nAdding documentation in Python\nIn Python, the standard approach for documenting functions is by using docstrings—short for “documentation strings”—which are placed right after the function definition. Docstrings provide detailed information about a function’s behavior, inputs, outputs, and exceptions, making it easier to understand and maintain the code.\nA docstring in Python is typically encapsulated within triple single quotes ('''docstring''') or double quotes (\"\"\"docstring\"\"\"), and it can span multiple lines if necessary. The key here is to strike a balance between brevity and thoroughness. A well-written docstring should be informative, yet concise enough to be practical.\nThere are different conventions for writing docstrings in Python. In our case, we’ll use the Google Style docstring format. You can find more information about the Google Style guide here.\nIn essence, Google Style docstrings contain the following section:\n\nSummary Line: A short, one-liner describing what the function does. This should be concise and in the imperative mood (e.g., “Fetch data from a database”).\nExtended Description (optional): A more detailed explanation of what the function does, how it works, and any additional context that might be helpful.\nArgs Section: A list of all the parameters the function takes. For each parameter, you provide:\n\nThe name of the parameter.\nA description of what the parameter is and its type (e.g., str, int).\n\nReturns Section: This describes what the function returns and the type of the return value. If the function doesn’t return anything (i.e., returns None), this section can be omitted.\nRaises Section: This section lists any exceptions the function might raise and under what conditions. It’s important to document any errors that could occur, especially if they’re not immediately obvious from the function’s code.\n\nLet’s now look at an example of how to document a function using the Google Style docstring format by adding documentation to the connect_to_database function using the Google Style docstring format:\ndef connect_to_database() -&gt; psycopg.Connection | None:\n    \"\"\"\n    Establishes a connection to the PostgreSQL database.\n\n    This function uses the psycopg library to connect to a\n    PostgreSQL database using the credentials from the env file.\n\n    Returns:\n        psycopg.Connection: A connection object to the PostgreSQL database.\n\n    Raises:\n        Exception: If there is an error during the connection attempt.\n    \"\"\"\n    try:\n        conn = psycopg.connect(\n            dbname=db_name,\n            user=db_user,\n            password=db_password,\n            host=db_host,\n            port=db_port\n        )\n        return conn\n    except Exception as e:\n        print(f\"Database connection error: {e}\")\nFrom now on, we will include proper documentation to all the refactored functions following the Google Style docstring format.\n\n\n\n\nSimilarly, the redis.py file in the shared folder will manage the Redis connection logic. It loads the Redis environment variables and provides a class to establish and manage the Redis connection.\nimport os\nimport redis\nimport logging\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nredis_host = os.getenv('REDIS_HOST')\nredis_port = os.getenv('REDIS_PORT')\n\nclass RedisClient:\n    \"\"\"\n    A Redis client for connecting to a Redis server.\n\n    This class manages the connection to a Redis server using the \n    credentials loaded from environment variables.\n    \"\"\"\n    def __init__(self, host: str, port: int):\n      \"\"\"\n        Initializes the RedisClient and establishes a connection.\n\n        Args:\n            host (str): The hostname or IP address of the Redis server.\n            port (int): The port number of the Redis server.\n\n        Raises:\n            redis.ConnectionError: If the connection to the Redis server fails.\n        \"\"\"\n        self.redis_host = host\n        self.redis_port = port\n        self.connection = self._connect()\n\n    def _connect(self) -&gt; redis.Redis:\n        \"\"\"\n        Establishes a connection to the Redis server.\n\n        This method tries to connect to the Redis server using \n        the provided host and port. \n        If the connection fails, it logs an error and raises\n        an exception.\n\n        Returns:\n            redis.Redis: A Redis connection object.\n\n        Raises:\n            redis.ConnectionError: If the connection to the Redis server fails.\n        \"\"\"\n        try:\n            # Create a connection to the Redis server\n            return redis.Redis(host=self.redis_host, port=self.redis_port)\n        except redis.ConnectionError as e:\n            logging.error(f\"Failed to connect to Redis: {e}\")\n            raise\n          \nredis_client = RedisClient(redis_host, int(redis_port))\nTo make the shared folder easier to work with, we’ll add an __init__.py file inside it. This is necessary because Python treats any folder without an __init__.py file as a regular directory, meaning we can’t import it directly. Without this file, we would have to reference the folder with a dot (.shared) when importing, which is less intuitive.\nIn its simplest form, the __init__.py file can be empty. However, it can also include setup code or define what gets imported when the whole package is imported. Since we don’t need any extra behavior, we’ll keep it empty.\nBy adding the __init__.py file, we turn the shared folder into a Python package. This allows us to import components from it directly, like this:\nfrom shared.db import connect_to_database\nfrom shared.redis import RedisClient\nAfter doing so, we no longer need to define PostgreSQL and Redis connection details separately in both the Flask and Celery applications. Instead, we can simply import them from shared.\nTo further improve our codebase, we’ll introduce a clearer separation of concerns by organizing the project into distinct folders. Alongside the shared folder, we will create two additional folders:\n\nflask_app for all Flask-related code, including routes, models, and views.\ncelery_app for Celery-related code, including background tasks, caching, and task queue management.\n\nAfter this reorganization, our project structure will look like this:\nfavurls/\n|- shared/                 (Common utilities used by both Flask and Celery)\n|   │\n|   │- __init__.py\n|   |\n│   │- db.py               (PostgreSQL database connection)\n|   |\n|   │- redis.py            (Redis connection)\n|   |\n│   |- .env                (File to store environmental variables)\n│\n│- flask_app/\n|   |\n|   │- (Flask-related code: routes, models, views)\n|\n|- celery_app/\n|   |\n│   |- (Celery-related code: caching, task queue)\n\n\nOrganizing Celery code\nNext, we’ll refactor the Celery-related code. Instead of having all Celery-related logic in one file, we will distribute responsibilities across four new files in the celery_app folder:\n\ncelery_app.py: This file will exclusively handle Celery configuration, including setting up the Celery instance, defining the task broker, and configuring periodic tasks.\ndb.py: This file will contain functions related to interacting with the PostgreSQL database.\nredis.py: This file will manage all Redis-related interactions, such as managing the queue.\ntasks.py: This file will be dedicated to defining the Celery tasks themselves.\n\nIf we take a closer look at this structure, we can observe that the db.py and redis.py files are primarily focused on utility functions. These functions serve the purpose of supporting the Celery tasks in interacting with PostgreSQL and Redis. As such, to improve organization and modularity, we can group these utility files into a dedicated utils folder within the celery_app directory. This will better reflect their role as utility modules and make the structure more intuitive.\nTo make importing from the utils folder easier, we’ll add an __init__.py file inside it. Just like with the shared folder, this file is necessary because it tells Python to treat the utils folder as a package.\nAfter these changes, the folder structure for our Celery application will look as follows:\ncelery_app/\n|\n│- celery_config.py    (Celery configuration)\n|\n│- /utils\n│   │\n│   │- __init__.py   \n│   │\n│   │- db.py           (PostgreSQL utility functions)\n│   │\n│   │- redis.py        (Redis utility functions)\n|\n│- tasks.py            (Celery task logic)\n\nCelery configuration: celery_app.py\nLet’s begin by creating the celery_config.py file, where we will move the Celery configuration. This file will only include the Celery app definition, the beat schedule configuration, and the necessary Celery-related imports. In order to import the celery broker details we can do so from the redis module in the shared folder we previously we created\nThe resulting celery_config.py file will look like this:\nfrom celery import Celery\nfrom shared.redis import redis_host, redis_port\n\n# Create the Celery instance\napp_celery = Celery(broker=f\"redis://{redis_host}:{redis_port}/0\")\n\n# Optional: You can put other configuration here if necessary\napp_celery.conf.beat_schedule = {\n    'cache-top-favorite-websites-every-30-mins': {\n        'task': 'celery_app.tasks.cache_top_favorite_websites',\n        'schedule': 30.0,  # 30 minutes\n    },\n    'dequeue-urls-every-5-mins': {\n        'task': 'celery_app.tasks.process_queued_urls',\n        'schedule': 30.0,  # 5 minutes\n    },\n}\nNote that we now refer to tasks using their full names, like celery_app.tasks.cache_top_favorite_websites, instead of just calling the function directly by its name. This is because Celery needs a clear way to find and run the task. Since we’ve split the Celery logic into separate files, we need to tell Celery where the task is located by mentioning the file (or module) it’s in (celery_app.tasks), followed by the task’s name. This way, Celery can correctly identify and execute the task.\n\n\nDatabase functions: db.py\nNext, we will create the db.py file inside the utils folder. We will move here all the functions related to interacting with the PostgreSQL database. Since these functions need to connect to the PostgreSQL database, we’ll import the connection logic from the shared module to keep things organized.\nHere is how the db.py file will look:\nfrom shared.db import connect_to_database\nimport logging\nfrom typing import List\n\ndef insert_urls_to_database(urls: List[str]) -&gt; None:\n    \"\"\"\n    Inserts a list of URLs into the registered_urls table in the database.\n\n    This function establishes a connection to the database, \n    inserts the provided list of URLs into the `registered_urls`\n    table, and commits the transaction.\n\n    Args:\n        urls (List[str]): A list of URLs to be inserted into the database.\n\n    Raises:\n        Exception: If an error occurs while inserting data into the database.\n    \"\"\"\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.executemany(\"INSERT INTO registered_urls (url) VALUES (%s)\", [(url,) for url in urls])\n        conn.commit()\n        logging.info(f\"Inserted {len(urls)} URLs into the database.\")\n    except Exception as e:\n        logging.error(f\"Error inserting into the database: {e}\")\n    finally:\n        conn.close()\n        \ndef get_top_registered_urls() -&gt; List[Tuple[str, int]] | None:\n    \"\"\"\n    Retrieves the top 10 most frequently registered URLs from the database.\n\n    This function queries the `registered_urls` table to count \n    the occurrences of each URL and returns the top 10 URLs with\n    the highest count.\n\n    Returns:\n        List[Tuple[str, int]]: A list of tuples where each tuple \n                              contains a URL and the count of its\n                              occurrences in the database.\n\n    Raises:\n        Exception: If an error occurs while retrieving data from the database.\n    \"\"\"\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n        cur.execute(\"\"\"\n            SELECT url, COUNT(*) as count\n            FROM registered_urls\n            GROUP BY url\n            ORDER BY count DESC\n            LIMIT 10;\n        \"\"\")\n        top_registered_urls = cur.fetchall()\n        return top_registered_urls\n    except Exception as e:\n        print(f\"Error retrieving the URLs: {e}\")\n    finally:\n        if conn:\n            conn.close()\n\n\nRedis functions: redis.py\nSimilarly, we’ll create the redis.py file inside the utils folder to manage Redis-specific interactions. We will move the functions for dequeuing URLs from the Redis queue and caching popular URLs in Redis into this file. Just like with the database functions, we’ll import the Redis connection logic from the shared module.\nHere’s how the redis.py file will be structured:\nfrom shared.redis import redis_client\nfrom typing import List, Tuple\nimport logging\n\ndef dequeue_urls_from_redis(n: int) -&gt; List[str]:\n    \"\"\"\n    Retrieves a specified number of URLs from the Redis queue and removes them.\n\n    This function extracts a range of URLs from the \"url_queue\"\n    list in Redis, trims the queue to remove the processed URLs,\n    and returns the decoded list of URLs.\n\n    Args:\n        n (int): The maximum number of URLs to dequeue from the Redis queue.\n\n    Returns:\n        List[str]: A list of URLs dequeued from the Redis queue.\n\n    Raises:\n        Exception: If there is an error during the Redis operations.\n    \"\"\"\n    try:\n        # Start a Redis pipeline to batch operations\n        url_dequeue_pipeline = redis_client.connection.pipeline()\n        \n        # Extract the specified range of values from the queue\n        url_dequeue_pipeline.lrange(\"url_queue\", 0, n-1)\n        \n        # Trim the queue to remove the processed values\n        url_dequeue_pipeline.ltrim(\"url_queue\", n, -1)\n        \n        # Execute the batch operations\n        urls = url_dequeue_pipeline.execute()\n        \n        # If queue was trimmed, decode the extracted URLs from bytes to strings\n        if urls[1]:\n            registered_urls = [url.decode() for url in urls[0]]\n        else:\n            registered_urls = []\n        \n        return registered_urls\n    \n    except Exception as e:\n        # Handle potential errors\n        logging.error(f\"Failed to dequeue URLs from Redis: {e}\")\n        \ndef save_popular_to_redis(registered_urls: List[Tuple[str, int]]):\n    try:\n        registered_urls_dict = dict(registered_urls)\n        \n        redis_client.connection.delete(\"top_favorite_websites\")\n        \n        redis_client.connection.hset(\"top_favorite_websites\", mapping=registered_urls_dict)\n        \n        logging.info(\"Successfully cached top favorite websites in Redis.\")\n    except Exception as e:\n        logging.error(f\"Error when caching top favorite websites: {e}\")\n\n\nCelery task functions: tasks.py\nFinally, we’ll create the tasks.py file, where we’ll move the Celery task functions. We will also import all the necessary functions from the utils folder that the tasks need to operate.\nThe tasks.py file will look like this:\nfrom .celery_config import app_celery\nfrom utils.redis import dequeue_urls_from_redis, save_popular_to_redis\nfrom utils.db import insert_urls_to_database, get_top_registered_urls\nimport logging\n\n@app_celery.task\ndef process_queued_urls() -&gt; None:\n    logging.info('Started processing queued URLs.')\n    \n    dequeued_urls = dequeue_urls_from_redis(150)\n    \n    if dequeued_urls:\n        logging.info(f'Retrieved {len(dequeued_urls)} URLs from Redis.')\n        insert_urls_to_database(dequeued_urls)\n        logging.info(f'Inserted dequeued URLs into the PostgreSQL database.')\n    else:\n        logging.info('No URLs retrieved from Redis.')\n    \n    logging.info('Finished processing queued URLs.')\n    \n@app_celery.task\ndef cache_top_favorite_websites() -&gt; None:\n    logging.info(\"Started caching top favorite websites.\")\n\n    top_urls = get_top_registered_urls()\n    \n    if top_urls:\n        logging.info(f\"Retrieved top URLs from PostgreSQL.\")\n        \n        save_popular_to_redis(top_urls)\n        logging.info(\"Successfully cached top favorite websites.\")\n    else:\n        logging.info(\"No URLs retrieved from PostgreSQL, skipping caching.\")\n\n    logging.info(\"Finished caching process.\")\n\n\nExecuting the refactored Celery application\nNow that we have refactored our Celery application, we can proceed to execute it. It’s likely that we would want to run the Celery application from the top-level folder of our project, instead of navigating into the celery_app folder every time. To make this possible, we need to ensure that the celery_app package is properly set up for execution from the root directory.\nTo achieve this, we need to add an __init__.py file in the celery_app folder. This __init__.py file is crucial because it turns the celery_app folder into a Python package, making it accessible for import. Additionally, in this case, this file won’t be empty. It needs to import the Celery application instance from celery_app.py. This allows us to directly execute Celery commands from the root folder, such as running the Celery worker or scheduling tasks with Celery Beat.\nHere’s what the __init__.py file should contain:\nfrom .celery_config import app_celery\nfrom .tasks import *\nWith the __init__.py in place, we can now execute the Celery worker from the root folder (favurls) using:\ncelery -A celery_app worker --loglevel=info\nAnd the Celery Beat scheduler in another terminal instance with:\ncelery -A celery_app beat --loglevel=info\nAn important step after creating this __init__.py file is updating our imports in tasks.py. Instead of importing utility functions directly from utils, it is better to import them from celery_app.utils. This is especially important as we continue refactoring and expanding our codebase, as we might introduce additional utils packages in other parts of the project.\nUsing a generic utils namespace across different parts of the project could lead to confusion or errors during refactoring, making it unclear which utils module a function belongs to.\nAdditionally, it is recommended to import all modules within celery_app, including configurations like celery_config, using the celery_app namespace. This ensures that the correct configurations are loaded from the intended package and prevents potential conflicts with similarly named modules elsewhere in the project.\nWith this change, our imports in tasks.py would now look like:\nfrom celery_app.celery_config import app_celery\nfrom celery_app.utils.redis import dequeue_urls_from_redis, save_popular_to_redis\nfrom celery_app.utils.db import insert_urls_to_database, get_top_registered_urls\nimport logging\n\n\n\nOrganizing Flask code\nNow, let’s focus on organizing our Flask application. Previously, all our Flask code was contained within a single folder named my_flask_app, where everything was bundled into a single file: app.py. As we just did with our Celery code, we will now distribute responsibilities across multiple files inside a new folder named flask_app.\nWe will create the following files, each serving a specific purpose:\n\nvalidators.py: In this file we will place the function to validate URLs.\nredis.py: This file will manage all Redis-related interactions, such as enqueuing URLs into Redis or retrieving the cached top favorite websites.\nprocessors.py: This file will house the core logic for processing URLs, specifically the process_urls function.\ncli.py: This file will define our command-line utility for initializing the PostgreSQL database.\nroutes.py: This file will define all Flask routes, handling incoming requests and responses.\nflask_config.py: This file will initialize our Flask application, register the routes from routes.py and the CLI command from cli.py into the Flask CLI.\n\nAs we did with the Celery code, we will further improve organization by grouping related files into dedicated folders. We will create a utils folder to store utility-related files, such as redis.py for interacting with Redis and validators.py for URL validation. Additionally, we will introduce a services folder, where we will place processors.py, which contains the business logic for handling URLs. Both folders will include an __init__.py file to indicate that they are Python packages, allowing for cleaner imports and better modularity.\nFurthermore, we will add an __init__.py file in the flask_app folder itself. This will allow the flask_app folder to be recognized as a Python package, enabling us to import components from it and maintaining clarity on where each component resides.\nAfter these changes, our flask_app folder will be structured as follows:\nflask_app/\n│- flask_config.py     (Flask application setup and route registration)\n│ \n│- cli.py              (Database initialization command)\n│ \n│- routes.py           (Route handling and request processing)\n│ \n│- /utils     \n│   │\n│   │- __init__.py\n│   │\n│   │- redis.py        (Functions for interacting with Redis)\n│   │\n│   │- validators.py   (URL validation functions)\n│   \n│- /services\n│   │\n│   │- __init__.py\n│   │\n│   │- processors.py   (Handles URL processing logic)\n│\n│- __init__.py\nWith this new structure in place, we can begin moving our code into these modular components.\n\nURL validation: validators.py\nWe’ll start by moving the is_valid_url function, which checks whether a URL follows the correct format, to the validators.py file. This will help centralize validation logic.\nHere’s how the validators.py file will be structured:\nimport re \n\ndef is_valid_url(url: str) -&gt; bool:\n    pattern = r\"^(https?:\\/\\/)?(www\\.)?[a-zA-Z0-9]+\\.[a-zA-Z]+$\"\n    return bool(re.fullmatch(pattern, url))\n\n\nRedis utility functions: redis.py\nNext, we’ll transfer all Redis-related interactions to the redis.py file inside the utils folder, which will include all the functions interacting with Redis, i.e., enqueue_url_to_redis and retrieve_favorite_websites_from_redis. We will also import the Redis connection logic from the shared module.\nThe redis.py file will look like this:\nfrom shared.redis import redis_client\nimport logging\n\ndef retrieve_favorite_websites_from_redis() -&gt; List[Tuple[str, int]] | None:\n   \"\"\"\n    Retrieves the top favorite websites and their counts from Redis.\n\n    This function fetches all the entries from the Redis hash\n    \"top_favorite_websites\", decodes the URLs and their counts\n    from bytes, and returns a list of tuples containing the \n    URLs and their corresponding counts.\n\n    Returns:\n        List[Tuple[str, int]]: A list of tuples where each tuple\n        contains a URL and its associated count.\n\n    Raises:\n        Exception: If there is an error retrieving data from Redis.\n    \"\"\"\n    try:\n        favorite_websites = redis_client.connection.hgetall(\"top_favorite_websites\")\n\n        favorite_websites = [(url.decode(), int(count)) for url, count in favorite_websites.items()]\n\n        return favorite_websites\n    except Exception as e:\n        logging.error(f\"Error retrieving favorite websites from Redis: {e}\")\n        \ndef enqueue_url_to_redis(url: str) -&gt; None:\n  \n    \"\"\"\n    Adds a URL to the Redis queue.\n\n    This function pushes the specified URL onto the Redis list \"url_queue\".\n\n    Args:\n        url (str): The URL to be enqueued to Redis.\n\n    Raises:\n        Exception: If there is an error sending the URL to the Redis queue.\n    \"\"\"\n    try:\n        # Push the URL to a Redis queue (list)\n        redis_client.connection.lpush(\"url_queue\", url)\n        logging.debug(f\"URL enqueued to Redis: {url}\")\n    except Exception as e:\n        logging.error(f\"Error sending URL to Redis queue: {e}\")\n\n\nURL processsing logic: processors.py\nIn the processors.py file, we will move all the code that handles URL processing, which in our case refers to the process_url function. To achieve this, we first need to import a couple of functions from other parts of the application. The first is the is_valid_url function, which is found in the validators.py file within the utils directory. The second import is enqueue_url_to_redis from the redis.py file, also located within the utils directory.\nThe processors.py file should now look like this:\nfrom flask_app.utils.validators import is_valid_url\nfrom flask_app.utils.redis import enqueue_url_to_redis\n\ndef process_url(url: str) -&gt; str:\n    \"\"\"\n    Validates and processes the provided URL.\n\n    This function checks if the provided URL is valid. If valid,\n    it enqueues the URL to the Redis queue and returns a success\n    message. If the URL is invalid, it returns an error message\n    indicating the issue.\n\n    Args:\n        url (str): The URL to be validated and processed.\n\n    Returns:\n        str: A confirmation message indicating whether \n            the URL was successfully registered or if\n            the URL format was invalid.\n\n    Raises:\n        None\n    \"\"\"\n    if is_valid_url(url):\n        enqueue_url_to_redis(url)\n        confirmation_message = \"You have successfully registered the following URL: \" + url\n    else:\n        confirmation_message = \"The URL you entered is not valid. Please check the format and try again.\"\n    return confirmation_message\n\n\nCommand-line interface (CLI) utility: cli.py\nNext, we’ll centralize all command-line interface (CLI) functions into cli.py.\nCurrently, we only have one CLI function, create_table, which is responsible for setting up the database schema. Since this function interacts with the PostgreSQL database, we will also import the connect_to_database function from the shared module to handle the database connection.\nWith these changes, our cli.py file will be structured as follows:\nfrom shared.db import connect_to_database\nimport logging\n\ndef create_table() -&gt; None:\n  \"\"\"\n    Create the 'registered_urls' table in the database if\n    it does not already exist.\n\n    This function establishes a connection to the database,\n    creates a table named 'registered_urls' with columns \n    'id' (as a SERIAL PRIMARY KEY) and 'url' (as TEXT), \n    and commits the changes. If the table already exists,\n    no changes are made.\n\n    Raises:\n        psycopg.DatabaseError: If an error occurs during\n                               the table creation process.\n    \"\"\"\n    try:\n        conn = connect_to_database()\n        cur = conn.cursor()\n\n        cur.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS registered_urls (\n                id SERIAL PRIMARY KEY,\n                url TEXT\n            )\n        \"\"\")\n\n        conn.commit()\n        logging.info(\"Database table initialized successfully.\")\n    except Exception as e:\n        logging.error(f\"Error initializing database table: {e}\")\n    finally:\n        conn.close()\n\n\nRoute Handling: routes.py\nSimilarly, we will centralize all route-related functions into the routes.py file. To achieve this, we will need to import several functions that our route functions depend on, such as retrieve_favorite_websites_from_redis from redis.py and process_url from processors.py.\nOnce the necessary imports are in place, we can move all route-related functions into this file.\nHere’s how the routes.py file will be structured:\nfrom flask_app.utils.redis import retrieve_favorite_websites_from_redis\nfrom flask import request, render_template, redirect, url_for\nfrom flask_app.services.processors import process_url \n\ndef home():\n  \"\"\"\n    Render the home page and handle form submissions.\n\n    If the request method is 'POST', it processes the\n    submitted URL and redirects to the 'display_url' \n    route with a confirmation message. Otherwise, it \n    renders the home page template.\n\n    Returns:\n        flask.Response: A rendered template for GET\n                        requests or a redirect for \n                        POST requests.\n    \"\"\"\n    if request.method == 'POST':\n        url = request.form.get('urlInput')\n        confirmation_message = process_url(url)\n        return redirect(url_for('display_url', url=confirmation_message))\n    else:\n        return render_template('index.html')\n\ndef display_url(url: str | None = None):\n  \"\"\"\n    Display a processed URL or redirect to the home\n    page if no URL is provided.\n\n    If a URL is given, it renders the index page with\n    the URL. If the request method is 'POST', it \n    processes a new URL and redirects to the same \n    route with the updated confirmation message. \n    If no URL is provided, it redirects to the home page.\n\n    Args:\n        url (str, optional): The processed URL to be \n                             displayed. Defaults to None.\n\n    Returns:\n        flask.Response: A rendered template for GET \n                        requests or a redirect for \n                        POST requests.\n    \"\"\"\n    if url:\n        if request.method == 'POST':\n            url2 = request.form.get('urlInput')\n            confirmation_message = process_url(url2)\n            return redirect(url_for('display_url', url=confirmation_message))\n        else:\n            return render_template('index.html', url=url)\n    else:\n        return redirect(url_for('home'))\n\ndef top_favorite_websites_page():\n  \"\"\"\n    Render the top favorite websites page with the\n    most popular registered URLs.\n\n    Retrieves the top registered URLs from Redis\n    and displays them on the 'popular.html' template.\n\n    Returns:\n        flask.Response: A rendered template with the\n                        top registered URLs.\n    \"\"\"\n    top_registered_urls = retrieve_favorite_websites_from_redis()\n    return render_template('popular.html', top_registered_urls=top_registered_urls)\nNote that here, we avoid importing utils directly and instead specify the full module path, just as we did in our Celery tasks.py file. This ensures that we clearly reference the correct utilities for each part of the application. As we now have both celery_app.utils and flask_app.utils, failing to specify the correct module could lead to conflicts, incorrect imports, or confusion when maintaining the code.\n\n\nFlask app configuration: flask_config.py\nFinally, we will move the creation and configuration of the Flask app into the flask_config.py file, centralizing all configurations related to the application’s instance and routes.\nTo achieve this, we will first import all the necessary components for the Flask app, such as the route functions (home, display_url, and tracked_page) that we previously moved into the routes.py file, and then import the create_table function from cli.py to associate it with the Flask CLI commands.\nWith these components, the flask_config.py file will look as follows:\nfrom flask import Flask\nfrom flask_app.routes import home, display_url, tracked_page\nfrom flask_app.cli import create_table\n\napp = Flask(__name__)\n\napp.add_url_rule(\"/\", view_func=home)\napp.add_url_rule(\"/display_url/&lt;path:url&gt;\", view_func=display_url)\napp.add_url_rule(\"/popular\", view_func=tracked_page)\n\napp.cli.add_command(\"init_db_table\", create_table)\n\n\nExecuting the refactored Flask application\nNow that we have refactored the code for our Flask application, we can proceed to run the application from the root directory, just like we did with Celery.\nTo make this possible, we need to ensure that Flask is properly configured for execution. One way to achieve this is by adding a __init__.py file in the flask_app folder. This file will help convert the folder into a Python package, allowing us to import and run the Flask application easily.\nThe __init__.py file should import the app instance from the flask_config.py file so that we can run it directly.\nHere’s what the __init__.py file should contain:\nfrom .flask_config import app\nAfter having created our __init__.py file, we can run the Flask application from the root folder (favurls) by using the following command:\nflask run\nHowever, after executing this command, you may encounter an error message that states that Flask could not locate the application, as showin in Figure 16. This happens because, by default, Flask looks for a file named app.py or wsgi.py in the directory where the flask run command is executed. Since we don’t have either of these files in the root directory, Flask fails to identify the application and throws an error.\n\n\n\n\n\n\nFigure 16: Flask error for missing application file\n\n\n\nUnlike Celery, where adding an __init__.py file in the package allows us to run Celery commands from the root directory, Flask requires a specific entry point to identify and run the application.\nTo fix this issue, we need to create an entry point for Flask in the root directory. This involves creating a new file in the root directory, which we will call app.py, and in which we define the Flask application to serve as the entry point for our Flask application.\nHowever, before creating app.py, we will make some changes to our flask_config.py file. Right now, our flask_config.py file creates and configures the Flask application immediately when the module is loaded. While this works fine for simple setups, it can lead to unintended side effects when modules are imported elsewhere. To prevent this and gain better control over the app’s initialization, we will wrap the entire configuration into a function called create_app. This change means that the app isn’t created until we explicitly call create_app, allowing us to avoid the pitfalls of premature instantiation.\nAfter implementing these changes, the flask_config.py file will look like this:\nfrom flask import Flask\nfrom flask_app.routes import home, display_url, top_favorite_websites_page\nfrom flask_app.cli import create_table\n\ndef create_app() -&gt; Flask:\n    \"\"\"\n    Initialize and configure the Flask application.\n\n    This function creates an instance of the Flask application,\n    registers URL routes, and adds CLI commands.\n\n    Returns:\n        Flask: The configured Flask application instance.\n    \"\"\"\n    app = Flask(__name__)\n    \n    # Register URL routes\n    app.add_url_rule(\"/\", view_func=home)\n    app.add_url_rule(\"/display_url/&lt;path:url&gt;\", view_func=display_url)\n    app.add_url_rule(\"/popular\", view_func=top_favorite_websites_page)\n    \n    # Register CLI commands\n    app.cli.add_command(\"init_db_table\", create_table)\n    \n    return app\nFollowing these changes, we also need to update the __init__.py file in the flask_app folder. Instead of importing a pre-configured app instance, we now import the create_app function. The updated __init__.py will now look like this:\nfrom .flask_config import create_app \nNow that we have updated the flask_config.py and __init__.py files, we can now create the app.py file in the root folder to serve as the explicit entry point for our Flask application. In this file, we will import the create_app function from our flask_app package and use it to instantiate our application.\nHere’s the code for app.py:\nfrom flask_app import create_app \n\napp = create_app()  # Create an instance of the Flask app\n\nif __name__ == \"__main__\":\n    app.run() \nWith all the changes in place, we can now successfully run the Flask application. Simply execute flask run from the terminal in the root folder, and the Flask server will start without any errors, making the application accessible."
  },
  {
    "objectID": "posts/2025/improving-architecture/index.html#containerizing-the-refactored-application",
    "href": "posts/2025/improving-architecture/index.html#containerizing-the-refactored-application",
    "title": "Data-driven web applications basics: Improving the architecture",
    "section": "Containerizing the refactored application",
    "text": "Containerizing the refactored application\nIn the previous post on data-driven web applications, we discussed the benefits of containerization and successfully containerized our Flask application. Since then, we have modified our application to improve performance and scalability. Instead of directly inserting submitted URLs into PostgreSQL, we now enqueue them in Redis for more efficient processing. Additionally, we cache the most popular websites in Redis rather than recomputing them for each user query. To support these changes, we have introduced a Celery service to handle background tasks such as processing submitted URLs and managing the cache. This means we now need to containerize both the Flask and Celery applications while ensuring they interact seamlessly. To achieve this, we will containerize them separately, allowing us to scale each service independently based on its specific workload.\n\nContainerizing the Flask application\nLet’s start by containerizing the Flask application, building upon the work we did in the previous post while taking into account the recent changes.\nThe first task is to update the requirements.txt file to include the new Redis library. There are a couple of ways to do this, such as using pipreqs to automatically generate the list of dependencies or manually adding the required packages. Since Redis is now part of our application stack, we need to ensure it’s included in our dependencies.\nHere is an updated version of the requirements.txt file:\nFlask==3.0.2\npython-dotenv==1.0.1\npsycopg==3.2.1\nredis==5.2.1\nNext, we will create a Dockerfile specifically for the Flask application. Since we have two components—Flask and Celery—it’s better to name the Dockerfiles differently for clarity. We will call this file Dockerfile_flask_app. This file will be placed in the root folder of the project (i.e., favurls), which will allow us to easily differentiate it from the Celery Dockerfile.\nIn this file we only need to change very few parts regarding the Dockerfile we already created in the previous post. Specifically, we need to copy the shared folder (which contains functions to connect to Redis and PostgreSQL), the flask_app folder (where the main logic resides), and the app.py entry point.\nHere is the updated Dockerfile_flask_app:\n# Use the Python 3.12.4 image as the base\nFROM python:3.12.4\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the requirements file and install the dependencies\nCOPY flask_app/requirements.txt /app\nRUN pip install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY shared /app/shared\nCOPY flask_app /app/flask_app\nCOPY app.py /app\n\n# Set the environment variable to specify the Flask application file\nENV FLASK_APP=app.py\n\n# Run the Flask application with the host set to 0.0.0.0 to allow external access\nCMD [\"flask\", \"run\", \"-h\", \"0.0.0.0\", \"-p\", \"5000\"]\n\n# Expose port 5000 for external access\nEXPOSE 5000\nAfter updating the Dockerfile_flask_app, we can build the Docker image. Since we are using a custom name for the Dockerfile, we need to specify it using the -f flag. Additionally, we will tag the image with an easy to idenfy name using the -t flag, such as favurls_flask_app.\nHere’s the command to build the image:\ndocker build -f Dockerfile_flask_app -t favurls_flask_app .\nOnce the image is built, we can run the container. However, since we need to connect the Flask application to the Redis service running locally, we must configure the container to use the local Redis instance. This can be done by specifying the --network=\"host\" flag to ensure that the container shares the host network and can access Redis. In addition, we set the REDIS_HOST environment variable to point to the local Redis instance.\nOnce the image is built, we can run the container. However, containers are, by default, isolated from the host machine’s network. This means that without any additional configuration, the container won’t be able to access services running on the host, such as the local Redis instance. To enable the container to connect to Redis, we need to configure it to share the host’s network. This is done by using the --network=\"host\" flag when running the container.\nHere’s the command to run the Flask container:\ndocker run --rm --network=\"host\" -e REDIS_HOST=127.0.0.1 -p 5000:5000 --name favurls_flask_app_container favurls_flask_app\nNote that in this command, we set the REDIS_HOST environment variable to point to the local Redis instance. While this is already defined in the .env file, we explicitly set it in the container configuration as a convenient way to adjust it if needed, making it easier to update or change the Redis host without modifying the .env file directly.\n\n\nContainerizing the Celery application\nNow that we have successfully containerized the Flask application, we can proceed to containerize the Celery application.\nAs with Flask, the first step will be to create a requirements.txt file for the Celery app, which we will place in the celery_app folder. This file will list the necessary dependencies to ensure that Celery works seamlessly with Redis and PostgreSQL:\ncelery==5.4.0\npython-dotenv==1.0.1\npsycopg==3.2.1\nredis==5.2.1\nNow, we’ll create a separate Dockerfile for the Celery application. To maintain clarity and avoid confusion with the Flask Dockerfile, we’ll name this file Dockerfile_celery_app, which we will also place in the root folder (favurls)\nThis Dockerfile will be quite similar to the one we created for the Flask application, but with a few important differences. First, we only need to copy the shared and celery_app folders, as those contain the necessary code for Celery. Additionally, since Celery doesn’t need to expose any ports (it’s not handling HTTP requests like Flask), we won’t need to include an EXPOSE instruction. Finally, we’ll adjust the CMD to run both the Celery worker and Celery beat services, which are required for processing background tasks and scheduling periodic tasks, respectively.\nHere’s how the Dockerfile_celery_app will look like:\n# Use the Python 3.12.4 image as the base\nFROM python:3.12.4\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the requirements file and install the dependencies\nCOPY celery_app/requirements.txt /app\nRUN pip install -r requirements.txt\n\n# Copy the rest of the application code\nCOPY shared /app/shared\nCOPY celery_app /app/celery_app\n\n# Run Celery worker and beat in the background\nCMD [\"sh\", \"-c\", \"celery -A celery_app worker --loglevel=info & celery -A celery_app beat --loglevel=info\"]\nAfter creating this Dockerfile_celery_app, we can build the image for the Celery application. We use the same process as before, specifying the custom Dockerfile and tagging the image with a recognizable name:\ndocker build -f Dockerfile_celery_app -t favurls_celery_app .\nOnce the image is built, we can run the Celery container. As with the Flask application, the Celery app needs to connect to Redis running locally, so --network=\"host\" flag in order to access Redis running locally. However in this case we do not need to expose any port, so we do not need to set the -p\ndocker run --rm --network=\"host\" -e REDIS_HOST=127.0.0.1 --name favurls_celery_app_container favurls_celery_app\nWith all these changes, our final file structure for our codebase will look like this:\nfavurls/\n|\n|-  app.py                 (Entrypoint for Flask application)\n|\n|-  Dockerfile_celery_app  (Dockerfile for Celery application)\n|\n|-  Dockerfile_flask_app  (Dockerfile for Flask application)\n|\n|- shared/                 (Common utilities used by both Flask and Celery)\n|   │\n|   │- __init__.py\n|   |\n│   │- db.py               (PostgreSQL database connection)\n|   |\n|   │- redis.py            (Redis connection)\n|   |\n│   |- .env                (File to store environmental variables)\n│\n│- flask_app/\n│   │\n│   │- __init__.py\n│   │\n│   │- requirements.txt\n│   │\n|   │- flask_config.py     (Flask application setup and route registration)\n|   │ \n|   │- cli.py              (Database initialization command)\n|   │ \n|   │- routes.py           (Route handling and request processing)\n|   │ \n|   │- /utils   \n|   │   │\n|   │   │- __init__.py\n|   │   │\n|   │   │- redis.py        (Functions for interacting with Redis)\n│   │   │\n|   │   │- validators.py   (URL validation functions)\n│   \n│- /services\n│   │\n│   │- __init__.py\n│   │\n│   │- processors.py   (Handles URL processing logic)\n|\n|- celery_app/\n│   │\n│   │- __init__.py\n│   │\n│   │- requirements.txt\n|   │ \n|   │- celery_config.py    (Celery configuration)\n|   │\n|   │- /utils\n│   |   │\n│   |   │- __init__.py   \n│   |   │\n│   |   │- db.py           (PostgreSQL utility functions)\n│   |   │\n│   |   │- redis.py        (Redis utility functions)\n|   │\n|   │- tasks.py            (Celery task logic)\n|   │"
  },
  {
    "objectID": "posts/2025/improving-architecture/index.html#summary",
    "href": "posts/2025/improving-architecture/index.html#summary",
    "title": "Data-driven web applications basics: Improving the architecture",
    "section": "Summary",
    "text": "Summary\nIn this post, we built on our previous work by refining our application to better handle real-world traffic and improve performance. We began by revisiting our application’s core functionality—a system where users can submit their favorite website URLs via the Home page, with the data stored in a PostgreSQL database—and the subsequent display of the top favorite websites on a dedicated page. Although the app performed well during development, as we started considering production, we identified potential scalability issues that could overwhelm the database.\nTo address these challenges, we introduced a more sophisticated data handling strategy. Instead of inserting each URL submission directly into PostgreSQL, we now enqueue submissions in Redis, an in-memory key-value store known for its speed. This adjustment allows us to batch multiple insert operations, which are processed periodically by a Celery worker every five minutes, significantly reducing the load on the database during peak times. At the same time, we addressed the inefficiency of querying the database for the top favorite websites with every request by precomputing these results every thirty minutes and caching them in Redis, ensuring users receive information quickly without repeatedly straining the system.\nBeyond performance improvements, we also restructured our codebase for better organization and long-term maintainability. We refactored our previously monolithic Flask and Celery applications into smaller, modular components, centralizing shared functionality—such as database and Redis connection logic—in a dedicated shared folder with files like db.py and redis.py. Additionally, we adopted comprehensive documentation practices using Google Style docstrings, providing clear explanations of function behaviors, expected inputs, and outputs.\nDespite all these changes, it is worth mentioning that our application is not yet perfect. We’ve kept the scope of these changes manageable for illustrative purposes, focusing on providing a good example of architectural improvements that could already significantly enhance the application. However, additional improvements could further elevate the app. For instance, implementing user identification would help prevent duplicate URL submissions and improve data integrity. Another enhancement could involve optimizing the cache update process: instead of fully rebuilding the cache every thirty minutes, we could register URL submissions before they’re added to the database and only update the cache with new URLs since the last update. This would help maintain cache freshness while minimizing overhead."
  }
]